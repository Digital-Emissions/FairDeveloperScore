#!/usr/bin/env python3
# ----------------------------------------------------------------------
# run_torque.py
#
# Heuristic "Torque Clustering" for git-commit datasets.
#
# Given the CSV extracted from GitHub API, assign each commit to a logical batch.
# A new batch starts when **either** of these is true:
#   •  Author changes
#   •  TORQUE = α·Δt (seconds) + β·ΔLOC exceeds the threshold GAP
#   •  Commit is a merge commit (optional, enabled by break_on_merge)
#
# Dependencies: pandas (pip install pandas)
# ----------------------------------------------------------------------

import pandas as pd
from pathlib import Path

# ==============================================================================
# CONFIGURATION - Modify these settings as needed
# ==============================================================================

# Input CSV file path (generated by acquire_pretrained_data.py)
INPUT_CSV = "data/github_commit_data_test/linux_kernel_commits.csv"

# Output CSV file path (will contain batch_id column)
OUTPUT_CSV = "data/github_commit_data_test/linux_kernel_commits_clustered.csv"

# TORQUE clustering parameters
ALPHA = 0.00001      # Time-weight α (seconds coefficient) - much lower for collaborative batches
BETA = 0.1           # LOC-weight β (lines of code coefficient) - lower weight on LOC
GAP = 7200.0         # Torque threshold for starting new batch (2 hours = 7200 seconds)
BREAK_ON_MERGE = True    # Start new batch on merge commits
BREAK_ON_AUTHOR = False  # Don't break batches just on author change - allow collaboration!

# ==============================================================================


def load_commits_data(csv_path: str):
    """
    Load commits data from CSV file using pandas
    
    Args:
        csv_path: Path to the CSV file
        
    Returns:
        pd.DataFrame: DataFrame containing commits data
    """
    try:
        # Read the CSV file
        df = pd.read_csv(csv_path)
        
        print(f"Successfully loaded {len(df)} commits from {csv_path}")
        print(f"Data shape: {df.shape}")
        print(f"Columns: {list(df.columns)}")
        
        # Show basic statistics
        print(f"\nData overview:")
        print(f"Unique authors: {df['author_email'].nunique()}")
        print(f"Date range: {pd.to_datetime(df['commit_ts_utc'], unit='s').min()} to {pd.to_datetime(df['commit_ts_utc'], unit='s').max()}")
        print(f"Total insertions: {df['insertions'].sum()}")
        print(f"Total deletions: {df['deletions'].sum()}")
        print(f"Merge commits: {df['is_merge'].sum()}")
        
        return df
        
    except FileNotFoundError:
        print(f"Error: Could not find CSV file at {csv_path}")
        return None
    except Exception as e:
        print(f"Error loading CSV file: {e}")
        return None


def torque_cluster(df: pd.DataFrame, α: float, β: float,
                   gap: float, break_on_merge: bool, break_on_author: bool = False) -> pd.Series:
    """
    Apply TORQUE clustering algorithm to assign batch IDs.
    
    Args:
        df: DataFrame with commit data
        α: Time-weight coefficient (alpha)
        β: LOC-weight coefficient (beta)
        gap: Torque threshold for starting new batch
        break_on_merge: Whether to start new batch on merge commits
        
    Returns:
        pd.Series: Series containing batch_id for each commit
    """
    print(f"\n--- Running TORQUE Clustering ---")
    print(f"Parameters: α={α}, β={β}, gap={gap}, break_on_merge={break_on_merge}")
    
    # Ensure chronological order
    df = df.sort_values("commit_ts_utc").reset_index(drop=True)

    batch_ids = []
    current_batch = 0
    prev_row = None

    for idx, row in df.iterrows():
        if prev_row is None:
            batch_ids.append(current_batch)
            prev_row = row
            continue

        # Author change starts new batch (only if break_on_author is True)
        author_changed = row["author_email"] != prev_row["author_email"] if break_on_author else False

        # Δt may be blank (NaN/empty string) for first commit; coerce
        try:
            Δt = float(row["dt_prev_commit_sec"]) if row["dt_prev_commit_sec"] != "" else float("inf")
        except (ValueError, TypeError):
            Δt = float("inf")

        # Lines changed this commit
        Δloc = (row.get("insertions", 0) or 0) + (row.get("deletions", 0) or 0)

        torque = α * Δt + β * Δloc
        torque_break = torque > gap

        merge_break = bool(row["is_merge"]) if break_on_merge else False

        # Debug output for first few decisions
        if idx < 5:
            print(f"Commit {idx}: Δt={Δt:.1f}s, Δloc={Δloc}, torque={torque:.1f}, "
                  f"author_changed={author_changed}, torque_break={torque_break}, merge_break={merge_break}")

        if author_changed or torque_break or merge_break:
            current_batch += 1

        batch_ids.append(current_batch)
        prev_row = row

    return pd.Series(batch_ids, name="batch_id")


def analyze_clustering_results(df: pd.DataFrame):
    """
    Analyze and display clustering results statistics.
    
    Args:
        df: DataFrame with batch_id column added
    """
    print(f"\n--- Clustering Results Analysis ---")
    
    total_batches = df['batch_id'].nunique()
    print(f"Total batches created: {total_batches}")
    print(f"Average commits per batch: {len(df) / total_batches:.2f}")
    
    # Batch size distribution
    batch_sizes = df['batch_id'].value_counts().sort_index()
    print(f"Batch size statistics:")
    print(f"  Min batch size: {batch_sizes.min()}")
    print(f"  Max batch size: {batch_sizes.max()}")
    print(f"  Median batch size: {batch_sizes.median()}")
    
    # Author distribution per batch
    authors_per_batch = df.groupby('batch_id')['author_email'].nunique()
    print(f"Authors per batch:")
    print(f"  Batches with 1 author: {(authors_per_batch == 1).sum()}")
    print(f"  Batches with >1 author: {(authors_per_batch > 1).sum()}")
    
    # Show some example batches
    print(f"\nExample batches:")
    for batch_id in sorted(df['batch_id'].unique())[:3]:
        batch_commits = df[df['batch_id'] == batch_id]
        authors = batch_commits['author_email'].unique()
        print(f"  Batch {batch_id}: {len(batch_commits)} commits, authors: {authors[:2]}{'...' if len(authors) > 2 else ''}")


def main():
    """
    Main function to load data, run TORQUE clustering, and save results.
    """
    # Get the project root directory
    project_root = Path(__file__).parent.parent.parent
    input_path = project_root / INPUT_CSV
    output_path = project_root / OUTPUT_CSV
    
    print("Loading commits data...")
    df = load_commits_data(input_path)
    
    if df is None:
        return
    
    # Run TORQUE clustering
    df["batch_id"] = torque_cluster(df,
                                    α=ALPHA,
                                    β=BETA,
                                    gap=GAP,
                                    break_on_merge=BREAK_ON_MERGE,
                                    break_on_author=BREAK_ON_AUTHOR)
    
    # Analyze results
    analyze_clustering_results(df)
    
    # Save clustered data
    output_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_path, index=False)
    print(f"\n[+] Clustered CSV written to {output_path}")
    
    # Save clustering summary
    summary_path = output_path.parent / "clustering_summary.txt"
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write(f"TORQUE Clustering Summary\n")
        f.write(f"========================\n\n")
        f.write(f"Parameters:\n")
        f.write(f"  alpha: {ALPHA}\n")
        f.write(f"  beta: {BETA}\n")
        f.write(f"  Gap threshold: {GAP}\n")
        f.write(f"  Break on merge: {BREAK_ON_MERGE}\n")
        f.write(f"  Break on author: {BREAK_ON_AUTHOR}\n\n")
        f.write(f"Results:\n")
        f.write(f"  Total commits: {len(df)}\n")
        f.write(f"  Total batches: {df['batch_id'].nunique()}\n")
        f.write(f"  Average commits per batch: {len(df) / df['batch_id'].nunique():.2f}\n")
    
    print(f"[+] Clustering summary written to {summary_path}")


if __name__ == "__main__":
    main()

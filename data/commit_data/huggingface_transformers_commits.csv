repo,sha,author,date,message,additions,deletions,total_changes
huggingface/transformers,5c30f7e390429904ecf0749c2e9fd9a3f29cc714,Parag Ekbote,2025-07-11T18:23:08Z,Update Model Card for Encoder Decoder Model (#39272)  * update model card.  * add back the model contributors for mamba and mamba2.  * update the model card.  * Apply suggestions from code review  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Apply suggestions from code review  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * update batches with correct alignment.  * update examples and remove quantization example.  * update the examples.  * Apply suggestions from code review  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * update example.  * correct the example.  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>,90,84,174
huggingface/transformers,0d7efe3e4b7e057d105d90862cffc4f8cc125d1a,Xiang Chendong,2025-07-11T17:59:41Z,fix gpt2 usage doc (#39351)  fix typo of gpt2 doc usage,1,1,2
huggingface/transformers,a646fd55fdd97427ad33c4ee17d41758b7cafa99,Muhammad Shaheer Malik,2025-07-11T17:59:09Z,"Updated CamemBERT model card to new standardized format (#39227)  * Updated CamemBERT model card to new standardized format  * Applied review suggestions for CamemBERT: restored API refs, added examples, badges, and attribution  * Updated CamemBERT usage examples, quantization, badges, and format  * Updated CamemBERT badges  * Fixed CLI Section",88,33,121
huggingface/transformers,af74ec65a7d5a1fbe220164f0c3ece601c091114,eromomon,2025-07-11T17:58:26Z,Update Readme to Run Multiple Choice Script from Example Directory (#39323)  * Update Readme to run in current place  * Update Readme files to execute PyTorch examples from their respective folders,11,11,22
huggingface/transformers,70e57e4710d8a617a6f0ea73183d9bc4c91063c9,Julien Denize,2025-07-11T16:26:58Z,Add mistral common support (#38906)  * wip: correct docstrings  * Add mistral-common support.  * quality  * wip: add requested methods  * wip: fix tests  * wip: add internally some methods not being supported in mistral-common  * wip  * wip: add opencv dependency and update test list  * wip: add mistral-common to testing dependencies  * wip: revert some test changes  * wip: ci  * wip: ci  * clean  * check  * check  * check  * wip: add hf image format to apply_chat_template and return pixel_values  * wip: make mistral-common non-installed safe  * wip: clean zip  * fix: from_pretrained  * fix: path and base64  * fix: path and import root  * wip: add docs  * clean  * clean  * revert  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com> Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>,3573,8,3581
huggingface/transformers,665418dacc199d0c0825cdb0442de46b04e40e15,Jay Zhuang,2025-07-11T14:59:51Z,"Remove device check in HQQ quantizer (#39299)  * Remove device check in HQQ quantizer  Fix https://github.com/huggingface/transformers/issues/38439  * Apply style fixes  ---------  Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",1,4,5
huggingface/transformers,601bea2c4efd58440f9a7399b0dfae164703338b,Manuel de Prada Corral,2025-07-11T14:36:10Z,Verbose error in fix mode for utils/check_docstrings.py (#38915)  * fix ast deprecations for python 3.14: replace node.n by node.value and use `ast.Constant`  More verbose exceptions in `fix_docstring` on docstring formatting issues.,19,4,23
huggingface/transformers,24f771a043871a109d8a969bf92730746e085f3b,Yih-Dar,2025-07-11T14:30:56Z,fix failing `test_sdpa_can_dispatch_on_flash` (#39259)  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,14,7,21
huggingface/transformers,ee74397d207cb6e5e698a558c2952f54935bd942,Arthur,2025-07-11T13:54:25Z,update cb TP (#39361)  * update cb TP  * safety,12,1,13
huggingface/transformers,9bc675b3b6e6faddb90619a0f1fdc43cce943190,Cyril Vallez,2025-07-11T13:34:01Z,Fix link for testpypi (#39360)  fix link,1,1,2
huggingface/transformers,bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29,Shuming Hu,2025-07-11T09:07:32Z,PerceptionLM (#37878)  * plm template  * A working plm with fixed image features  * hacked processor  * First version that reproduced PLM output using PE from timm.  * Simplify and fix tie_word_embeddings  * Use PIL resize. Simplify converstion.  * First version that works with video input.  * simplifed image preprocessing (not batched)  * Minor fixes after rebasing on main.  * Video processor based on new API.  * Revert to use _preprocess for image processor.  * refactor with modular  * fix tie_word_embedding  * Testing with timm PE  * check in missed converstion from modular to model.py  * First working version of PLM with Eva PE. PLM-1B and 3B outputs are exactly the same as before. PLM-8B output has some differences.  * address review comments  * Fixed batching if video and image examples mixed.  * Simplify PE configuration.  * Enable AutoModel for PerceptionEncoder.  * Update PE config style.  * update all headers  * Minor fixes.  * Move lm_head to PerceptionLMForConditionalGeneration. Fix vit_G model specification.  * Fix for testing_modeling_perception_lm.py  * Image processing refactoring to use more common parts.  * Fix processor test.  * update tests to use model from hub  * More test fixes.  * integration test GT update after rebasing; probably due to video preprocessing  * update test media path to hub  * Stop tracking local scripts  * address some review comments  * refactor image processing.  * small fixes  * update documentation and minor fixes  * remove scripts  * Minor fix for CI  * Fix image processing  * CI and doc fix  * CI formatting fix  * ruff fix  * ruff formatting  * ran utils/sort_auto_mappings.py  * update docstring  * more docstring udpates  * add vision_input_type default fallback for image processing  * more verbose variable naming  * test update  * Remove PE and PEConfig use AutoModel(TimmWrapper) instead  * Minor cleanup.  * Minor Fix: remove any ref to PE. Ruff format and check.  * fix docstring  * Fix modular/model consistency.Improvex docstringfor  .  * Fix PerceptionLMForConditionalGenerationModelTest  * ruff fix  * fix for check_repo  * minor formatting  * dummy size arg to fix for processor test.  * Update docstring for PerceptionLMConfig  * Minor fixes from review feedback.  * Revert some minor changes per reviewer feedback.  * update base_model_prefix  * address reviewer feedback  * fix comment in modeling file  * address reviewer feedback  * ruff format  * Pre-merge test update.  * reapply modular and fix checkpoint name  * processor test path  * use modular a bit more  * remove dead code  * add token decorator  ---------  Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co> Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>,3262,1,3263
huggingface/transformers,4b47b2b8ea9252e36669397243b770505f953d69,Giuseppe Coccia,2025-07-10T22:34:10Z,Updated Switch Transformers model card with standardized format (Issue #36979) (#39305)  * Updated Switch Transformers model card with standardized format (Issue #36979)  * Apply reviewer suggestions to the new standardised Switch Transformer's model card  * Update switch_transformers.md  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>,73,18,91
huggingface/transformers,fe1a5b73e62c54adafcedb47ac2430e058dc4312,Pavel Iakubovskii,2025-07-10T18:07:59Z,[modular] speedup check_modular_conversion with multiprocessing (#37456)  * Change topological sort to return level-based output (lists of lists)  * Update main for modular converter  * Update test  * update check_modular_conversion  * Update gitignore  * Fix missing conversion for glm4  * Update  * Fix error msg  * Fixup  * fix docstring  * update docs  * Add comment  * delete qwen3_moe,127,47,174
huggingface/transformers,571a8c21313ef734e77cf9874ea0334d25bd7ff5,Cyril Vallez,2025-07-10T16:53:40Z,Add a default value for `position_ids` in masking_utils (#39310)  * set default  * Update masking_utils.py  * add small test,11,6,17
huggingface/transformers,bdc8028cb3efe6e33982eb8d297ffbb695606e84,Kyle Sayers,2025-07-10T16:33:30Z,"[Core] [Offloading] Enable saving offloaded models with multiple shared tensor groups (#39263)  * fix counting meta tensors, fix onloading meta tensors  Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>  * remove unrelated fix  Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>  * add test  Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>  ---------  Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>",39,18,57
huggingface/transformers,df49b399dc02a2375f0e9bd0e74c544247ab3976,Joao Gante,2025-07-10T15:40:08Z,[tests] tag serve tests as slow  (#39343)  * maybe they need more cpu resources?  * add todo,2,0,2
huggingface/transformers,36e80a18da4cde6efe2d9d43875b7cd2d50f18c7,Paul Pak,2025-07-10T15:27:55Z,[modeling][lfm2] LFM2: Remove deprecated seen_tokens (#39342)  * [modeling][lfm2] remove deprecated seen_tokens  * [modular][lfm2] remove deprecated seen_tokens from modular file,0,8,8
huggingface/transformers,9682d07f92bffcc2d091a32cfbb3692884e7cacd,Paul Pak,2025-07-10T14:07:33Z,LFM2 (#39340)  * [modeling][lfm2] LFM2 model on 4.53.0 interface  * [configuration] hook in LFM2 keys  * [modeling][lfm2] update modeling interface for 4.53.1  * [modeling][lfm2] apply mask to hidden conv states  * [misc] ruff format/lint  * [modeling][lfm2] minor: NotImplemented legacy cache conversion  * Create lfm2.md  * create nice modular  * style  * Update modeling_auto.py  * clean and start adding tests  * style  * Update test_modeling_lfm2.py  * Update __init__.py  * small test model size  * config  * small fix  * fix  * remove useless config attrs -> block_dim and conv_dim are hiden_size  * fix prepare inputs  * fix config  * test  * typo  * skip tests accordingly  * config docstrings  * add doc to .md  * skip config docstring check  ---------  Co-authored-by: Maxime Labonne <81252890+mlabonne@users.noreply.github.com> Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>,1645,6,1651
huggingface/transformers,38c3931362e4d6548c586d217452a19b279d9bbe,Joao Gante,2025-07-10T13:41:38Z,[server] add tests and fix passing a custom `generation_config` (#39230)  * add tests; fix passing a custom generation_config  * tool integration test  * add install step  * add accelerate as dep to serving  * add todo,320,53,373
huggingface/transformers,6b09c8eab05820d480f4da97d23456428d410082,edwko,2025-07-10T10:36:58Z,Handle DAC conversion when using weight_norm with newer PyTorch versions (#36393)  * Update convert_dac_checkpoint.py  * Update convert_dac_checkpoint.py  * Apply style fixes  ---------  Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com> Co-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>,34,1,35
huggingface/transformers,92043bde294da59207816c40c7601fbc49e10d8d,Yih-Dar,2025-07-10T09:51:55Z,fix `phi3` tests (#39312)  fix  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,12,2,14
huggingface/transformers,520b9dcb42cef21662c304583368ff6645116a45,Kingsley,2025-07-10T08:44:28Z,fix Glm4v batch videos forward (#39172)  * changes for video  * update modular  * change get_video_features  * update video token replacement  * update modular  * add test and fix typo  * lint  * fix order  * lint  * fix  * remove dependency  * lint  * lint  * remove todo  * resize video for test  * lint..  * fix test  * new a processor for video_test  * fix test,127,23,150
huggingface/transformers,bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2,Raushan Turganbay,2025-07-10T05:18:44Z,Delete deprecated stuff (#38838)  * delete deprecated stuff  * fix copies  * remove unused tests  * fix modernbert and fuyu  * Update src/transformers/cache_utils.py  Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>  * bye bye `seen_tokens`  * address comments  * update typings  * ecnoder decoder models follow same pattern as whisper  * fix copies  * why is it set to False?  * fix switch transformers  * fix encoder decoder models shared weight  * fix copies and RAG  * remove `next_cache`  * fix gptj/git  * fix copies  * fix copies  * style...  * another forgotten docsrting  ---------  Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>,914,2164,3078
huggingface/transformers,c6ee0b1da8ff57102548430e18480fa78a106022,Yoni Gozlan,2025-07-09T21:46:22Z,Fix broken SAM after #39120 (#39289)  fix,2,2,4
huggingface/transformers,aff7df8436dde04762170d3d0fbe906c7216d6f2,jiqing-feng,2025-07-09T21:14:45Z,enable static cache on TP model (#39164)  * enable static cache on TP model  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * check tp size before init kv cache  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * fix docstring  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * add tp tests  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * fix comment  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * fix other cache head size  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  ---------  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>,84,1,85
huggingface/transformers,2ef59646b8466a6e47cbf42754637a9f4a82484f,ℍ𝕠𝕝𝕝𝕠𝕨 𝕄𝕒𝕟,2025-07-09T21:12:39Z,"Fix `max_length_q` and `max_length_k` types to `flash_attn_varlen_func` (#37206)  Also add notes asking users to set `TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` or call `torch._dynamo.config.capture_scalar_outputs = True`, as currently this will cause a graph break.  Signed-off-by: Hollow Man <hollowman@opensuse.org>",9,1,10
huggingface/transformers,2d600a4363b401f155fe6336994b50b2047982e8,Avihu Dekel,2025-07-09T21:09:50Z,"Granite speech speedups (#39197)  * ensure the query is updated during training  avoid unused parameters that DDP does not like  * avoid a crash when `kwargs` contain `padding=True`  trainers often pass this argument automatically  * minor  * Remove mel_spec lazy init, and rename to mel_filters. this ensures save_pretrained will not crash when saving the processor during training https://github.com/huggingface/transformers/blob/d5d007a1a0f0c11a726a54c8f00bd71825f84d02/src/transformers/feature_extraction_utils.py#L595  * minor - most feature extractors has a `sampling_rate` property  * speedup relative position embeddings  * fix several issues in model saving/loading: - avoid modifying `self._hf_peft_config_loaded` when saving - adapter_config automatically points to the original base model - a finetuned version should point to the model save dir. - fixing model weights names, that are changed by adding an adapter.  * minor  * minor  * minor  * fixing a crash without peft active  * add todo to replace einsum  * granite speech speedups: 1. register attention_dist to avoid cpu-to-gpu transfer every layer. 2. pad_sequence is much faster than per-sample-padding + concat. 3. avoid returning audio back to cpu when using a compute device.  * support audio.shape=(1,L)",5,11,16
huggingface/transformers,5111c8ea2f3eb918fc090f7dd4393d4204940e10,Tom Aarsen,2025-07-09T19:06:46Z,Fix typo: langauge -> language (#39317),1,1,2
huggingface/transformers,2781ad092dad77ff554cb70ec130b97e44cfba78,Priya aka Priyamvadha Balakrishnan,2025-07-09T18:32:40Z,docs: update LLaVA-NeXT model card (#38894)  * docs: update LLaVA-NeXT model card  * Update docs/source/en/model_doc/llava_next.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/llava_next.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/llava_next.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/llava_next.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/llava_next.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/llava_next.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/llava_next.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/llava_next.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * [docs] Updated llava_next model card  * Update docs/source/en/model_doc/llava_next.md remove image sources  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * [fix] Change Flash Attention to SDPA badge  * [doc] fixed quantization example  * docs: updated contribution details and badges  * Update llava_next.md  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>,125,234,359
huggingface/transformers,16dd7f48d00fbb2c1991ad90634b14856133f2d3,Yih-Dar,2025-07-09T17:36:48Z,skip files in `src/` for doctest (for now) (#39316)  fix  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,7,0,7
huggingface/transformers,d61c0d087cedbfdbbee8c75b210d5837c35addb8,Eman Risha,2025-07-09T17:23:03Z,"Updated the Model docs - for the MARIAN model (#39138)  * Update marian.md  This update improves the Marian model card to follow the Hugging Face standardized model card format. The changes include:  - Added a clear description of MarianMT, its architecture, and how it differs from other models. - Provided usage examples for Pipeline and AutoModel. - Added a quantization example for optimizing model inference. - Included instructions and examples for multilingual translation with language codes. - Added an Attention Mask Visualizer example. - Added a Resources section with relevant links to papers, the Marian framework, language codes, tokenizer guides, and quantization documentation. - Fixed formatting issues in the code blocks for correct rendering.  This update improves the readability, usability, and consistency of the Marian model documentation for users.  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update marian.md  * Update marian.md  * Update marian.md  * Update marian.md  * Update docs/source/en/model_doc/marian.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update marian.md  * Update marian.md  * Update marian.md  * Update marian.md  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",99,120,219
huggingface/transformers,161cf3415ed5b0caee38ce42d8805744d13c1b50,Yih-Dar,2025-07-09T17:07:44Z,add `stevhliu` to the list in `self-comment-ci.yml` (#39315)  add  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,1,1,2
huggingface/transformers,3be10c6d19fafc55d96d52e8bf30715058373543,Cyril Vallez,2025-07-09T16:40:37Z,Fix consistency and a few docstrings warnings (#39314)  * Update modeling_deepseek_v2.py  * fix docstrings  * fix  * fix,43,6,49
huggingface/transformers,4652677c89ecb664ee06bf141d1b7b648798e122,MaCAT,2025-07-09T16:29:51Z,🌐 [i18n-KO] Translated quark.md to Korean (#39268)  * initial translation  * removed english parts  * maintain consistency  * Update docs/source/ko/quantization/quark.md  Co-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>  * Update docs/source/ko/quantization/quark.md  Co-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>  * Update docs/source/ko/quantization/quark.md  Co-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>  * Update docs/source/ko/quantization/quark.md  Co-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>  * add toctree  * fixed indentation  ---------  Co-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>,87,0,87
huggingface/transformers,c98090420431e49d5cab8f41b1cb5426a8b87e5a,Vladislav Bronzov,2025-07-09T15:04:28Z,"Add DeepSeek V2 Model into Transformers (#36400)  * add initial structure  * doc fixes, add model base logic  * update init files  * some fixes to config and modular  * some improvements for attention  * format  * remove unused attn  * some fixes for moe layer and for decoder  * adapt _compute_yarn_parameters for deepseek  * format  * small fix  * fix for decoder forward  * add tests, small refactoring  * fix dummies  * fix init  * fix doc  * fix config docs  * add sequce doc, fix init for gate  * fix issues in tests  * fix config doc  * remove unused args  * some fixes and refactoring after review  * fix doc for config  * small fixes for config args  * revert config refactoring  * small refactoring  * minor fixes after rebase  * small fix after merge  * fix modular  * remove rotaryembd from public init  * small test fix  * some rotary pos calculation improvement  * fix format  * some improvements and fixes  * fix config  * some refactoring  * adjust some unit tests  * skip test  * small fixes and tests adjustment  * reapply modular  * fix all tests except Integration  * fix integration testzs  * cleanup BC stuff  * rope  * fix integrations tests based on a10  * style  ---------  Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co> Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",1913,0,1913
huggingface/transformers,accbd8e0fe7e535321fa6fa8de82c4260b500ddf,Raushan Turganbay,2025-07-09T14:10:38Z,[sliding window] revert and deprecate (#39301)  * bring back and deprecate  * oops  ---------  Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>,71,18,89
huggingface/transformers,1cefb5d788f5e1a5b59fd57394ed93cbe71f0d86,Cyril Vallez,2025-07-09T13:46:53Z,[modular] Allow method with the same name in case of @property decorator (#39308)  * fix  * add example  * fix  * Update modular_model_converter.py,264,8,272
huggingface/transformers,4798c05c64ddcca574fdce962a72466bdcb55a9e,Yih-Dar,2025-07-09T13:35:48Z,skip `test_torchscript_*` for now until the majority of the community ask for it (#39307)  fix  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,4,0,4
huggingface/transformers,fe5f3c85d292e34bed52e02a53edd5fa2acfc010,Yih-Dar,2025-07-09T11:49:33Z,fix `aria` tests (#39277)  * fix  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,16,0,16
huggingface/transformers,0687d481e2c71544501ef9cb3eef795a6e79b1de,Raushan Turganbay,2025-07-09T07:45:01Z,[flash attn 3] bring back flags (#39294)  * flash attn 3 flag  * fix copies,195,4,199
huggingface/transformers,25343aafee10da8a13b217a39f3825f88c6d8dbe,JJJYmmm,2025-07-09T05:03:44Z,Fix SDPA attention precision issue in Qwen2.5-VL (#37363)  * solve conflicts and remove  redundant attention_mask in qwenvit  * update decoded text check  * remove trailing whitespace,201,245,446
huggingface/transformers,0e1c2817455602d182bd8ebf5fba212e14fb187e,Yaswanth Gali,2025-07-08T19:46:32Z,[Tests] Update model_id in AIMv2 Tests (#39281)  * Update model_id in tests  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,5,5,10
huggingface/transformers,7ef592c96cbf616492ac4181a390d465189abec6,Biao Zhang,2025-07-08T17:08:48Z,"Update T5gemma (#39210)  * bug fix: add vocab_size to t5gemmaconfig for pipeline.  * Update checkpoint placeholder  * minor change  * minor change  * minor change: update example.  * fix: add vocab_size as an explict arg.  * buf fix:  remove vocab_size verification; instead, re-set encoder/decoder vocab size.  Note, in t5gemma, vocab size of encoder/decoder shoud be always the same.  * add `add_generation_prompt` for message preprocessing.",51,20,71
huggingface/transformers,1ecd52e50a31e7c344c32564e0484d7e9a0f2256,Quentin Lhoest,2025-07-08T15:06:12Z,Add torchcodec in docstrings/tests for `datasets` 4.0 (#39156)  * fix dataset run_object_detection  * bump version  * keep same dataset actually  * torchcodec in docstrings and testing utils  * torchcodec in dockerfiles and requirements  * remove duplicate  * add torchocodec to all the remaining docker files  * fix tests  * support torchcodec in audio classification and ASR  * [commit to revert] build ci-dev images  * [commit to revert] trigger circleci  * [commit to revert] build ci-dev images  * fix  * fix modeling_hubert  * backward compatible run_object_detection  * revert ci trigger commits  * fix mono conversion and support torch tensor as input  * revert map_to_array docs + fix it  * revert mono  * nit in docstring  * style  * fix modular  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,448,350,798
huggingface/transformers,1255480fd226129075e10c20842efd444f5b0e36,StevenBucaille,2025-07-08T15:03:04Z,[lightglue] add support for remote code DISK keypoint detector (#39253)  * feat: add trust_remote_code in LightGlueConfig  * fix: made sure trust_remote_code is provided only when necessary  * fix: make style  * docs: added missing trust_remote_code docstring  * refactor: refactored LightGlue config init  * fix: removed unnecessary argument,40,10,50
huggingface/transformers,838a0268b88b6e56783b401f9cacae9cb0cbb120,Yih-Dar,2025-07-08T13:36:05Z,fix flaky `test_generate_compile_model_forward` (#39276)  fix  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,3,0,3
huggingface/transformers,29d0030e237485a97ea568120d79a7988535ea63,Pavel Iakubovskii,2025-07-08T13:24:39Z,Refactor `PretrainedConfig.__init__` method to make it more explicit (#39158)  * cleanup  * fix no `__init__` test  * fix missing inits,135,102,237
huggingface/transformers,1580f6465347df4e8d7c12cd4d3dc603b8a689ad,Joao Gante,2025-07-08T10:44:01Z,[smollm3] add tokenizer mapping for `smollm3` (#39271)  add tok mapping to smollm3,1,0,1
huggingface/transformers,db05e4ff33cbb6b08ad882cfe47d50d4071a1daa,Kashif Rasul,2025-07-08T10:34:22Z,[pagged-attention] fix off-by-1 error in pagged attention generation (#39258)  * fix off-by-1 error in pagged attention generation  * formatting  * use update_with_token,5,1,6
huggingface/transformers,6f1a43896ce970d316aa1dff79ca33281fee244b,Joao Gante,2025-07-08T10:31:03Z,[CI] fix docs (#39273)  * fix docs  * add ko gloassary file to toctree,1,5,6
huggingface/transformers,fbdaa7b099e4253be4175e0201cd477e9de05363,Yaswanth Gali,2025-07-08T09:53:21Z,Add Aimv2 model (#36625)  * Model skelton  * changes  * temp push  * changes  * Added support for aimv2-native  * More changes  * More changes  * Stupid mistake correction  * Added config and refactor  * Added vison model  * update  * Refactor for lit variant  * Added Text Model  * Minor fixes  * nits  * update  * Preliminary tests  * More fixes  * Updated tests 🤗  * Refactor  * Updated testcase  * Updated config  * make fixup  * more fixes  * Bug fix and updates  * deadcode  * Fixes  * nit  * up  * Happy CI ✅  * Reduce LOC  * nit  * nit  * make style  * return_dict refactor  * bug fix  * fix  * doc update  * nit  * make fixup  * Minor update  * _init_weigths modifcation  * update tests  * Minor fixes post review  * Update w.r.t GradientCheckpointingLayer  * docs update  * update  * nit  * Use more Modular 😉  * Change name from AIMv2 to Aimv2  * Nit  * make style  * Add model doc pointer  * make style  * Update model doc section  * updates  * Modify attn mask and interface  * update test  * Final change  * Utilize flash and flex attn  * keep attn mask  * camelcase model name in test file  * Fix docstring  * Fix config warning finally and create_causal_mask  * disable torchscript  * remove unused arg  * remove from tests  * balance model size for tests  * fix device  * tests  * tests  * flaky test  * fix import  ---------  Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co> Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>,2977,2,2979
huggingface/transformers,d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1,Jingze Shi,2025-07-08T09:44:29Z,"Add Doge model (#35891)  * Add Doge Model  * Fix code quality  * Rollback an error commit  * Fix config for open-source weights  * Revert ""Fix config for open-source weights""  This reverts commit 229cdcac10a6a4274d1dd13b729bc14c98eb0c76.  * Add modular_doge  * Update Doge inherits from Llama  * Fix import bug  * [docs] Add usage of doge model  * Fix Doge import pretrainedconfig from modeling_utils to configuration_utils  * [docs] remove trust remote code from doge  * Fix dynamo bug in doge model  * Update docstrings  * Import apply_rotary_pos_emb and repeat_kv from Llama  * Fix all nits  * Fix code quality  * Fix some bugs  * Fix code quality  * Remove inherited `_update_causal_mask` from Llama This leads to incorrect weight initialization.  * Fix the wrong tensor orderings in DogeCDMoE  * Fix attention mask bug We have to provide attention_mask for dynamic mask computation  * Modify most implementations to inherit from Llama But there are two problems: 1. `flex_attention_forward` is not updated properly 2. `Example` error in the forward method of DogeForCausalLM  * Modify CDMoE for batch efficient implementation  * Uniform MoE configuration names, just like QwenMoE  * Fix code quality  * Fix code quality  * Fix code quality  * Add tp plan of CDMoE Module  * Hybird DMA with sliding window  * Update valid tokens greater than window size  * Fix code quality  * Add `convert_doge_weights_to_hf`  * Fix STATE_DICT_MAPPING in convert_doge_weights_to_hf.py  * Fix nits in modular_doge  * Fix code quality  * Fix all nits  * Fix all nits  * Make sure the attention function is updated inside the class  * Fix code quality issues in the Doge model and add a test for it  * Fix `test_generate`  * Fix code quality  * Fix nits fllowing suggestions  * Fix code quality  * Fix code quality issues  * Fix nits  * Fix code quality nits  * Fix the missing parameters in the configuration.  * Fix the missing parameters in the configuration.  * Fix nits  * Add initialization of attention  * Fix last nits  * Simplify dynamic mask generation logic  * Rename router_logits to gate_logits for matching latest changes of MixtralModel  * Rename typings for matching latest changes of MixtralModel  * Fixes typo in comment  * Update src/transformers/models/doge/modular_doge.py  Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>  * Fix code quality issues to match other modular  * Fix code quality issues to match other modular  * Fix the static compilation errors  * Update model weights link  * Fix code quality issues to match other modular  * reapply modular and support for new outputs  * style  * simplify a lot  * fix import location  * reapply modular  * fix  * fix integration test  ---------  Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com> Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co> Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",2624,0,2624
huggingface/transformers,d370bc64c63563ba610c08763828be71f4dc561c,Joonchen Liau,2025-07-08T09:39:31Z,Fix errors when use verl to train GLM4.1v model (#39199)  * Fix errors when use verl to train GLM4.1v model  * Support glm4v load from AutoModelForVision2Seq * Set glm4v model _checkpoint_conversion_mapping attr from None to {}  * Update modeling_auto.py,4,4,8
huggingface/transformers,5fb8bb3e1a897bb46a709e51fb393412e9a15ea8,Arthur,2025-07-08T09:38:11Z,"fix recompiles due to instance key, and deepcopy issues (#39270)  * fix recompiles due to instance key, and deepcopy issues  * dict",3,4,7
huggingface/transformers,356fd681098f4b33a6f95660a5a0252eae313348,Guang Yang,2025-07-08T08:59:37Z,"fix(generation): stop beam search per-instance when heuristic satisfied (#38778)  * fix(decoding): stop beam search per-instance when heuristic satisfied  Previously, when early_stopping is set to `False`, the early-stopping heuristic only halted generation when **all** batch instances reached the criterion. This caused instances that are impossible (suggested by the heuristic) to improve keep generating, leading to inconsistent and overlong outputs across the batch.  Now we apply the heuristic **per-instance**: once a certain instance of batch has its all beams impossibe to improve, we mark that instance finished while letting others continue. This restores expected behavior and ensures consistency in batched generation.  * Add test case GenerationIntegrationTests.test_beam_search_early_stop_heuristic  * Update naming improvement_possibility -> is_early_stop_heuristic_unsatisfied  * Add comments for early stop heuristic  * Update src/transformers/generation/utils.py  ---------  Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",109,30,139
huggingface/transformers,0b0ede8b2bc30d8f0125ab9a57eb60b94950fb3a,Pablo Montalvo,2025-07-08T08:41:44Z,remove broken block (#39255)  * remove broken block  * fixup,0,15,15
huggingface/transformers,a21557fa3e7f5b0723dd871909dc3917a0c35871,Yih-Dar,2025-07-08T08:38:25Z,Skip `test_eager_matches sdpa generate` and update an integration test for blip-like models (#39248)  * skip  * skip  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,26,3,29
huggingface/transformers,ea3c2c027769980c0501dc615ef7e755d206af62,gudwls215,2025-07-08T08:20:52Z,"Fix license text, duplicate assignment, and typo in constant names (#39250)  - Complete Apache License text in Italian documentation - Remove duplicate variable assignment in Perceiver converter - Fix typo in MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES constant",3,2,5
huggingface/transformers,b2816da8021b4e7568cb1e840a5d9aa1357c26a7,Yao Matrix,2025-07-08T08:18:26Z,"fix xpu failures on PT 2.7 and 2.8 w/o IPEX and enable hqq cases on XPU (#39187)  * chameleon xpu bnb groundtruth update on bnb triton backend since we are deprecating ipex backend  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * enable hqq uts on XPU, all passed  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * fix style  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * fix comment  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  ---------  Signed-off-by: YAO Matrix <matrix.yao@intel.com>",15,14,29
huggingface/transformers,17b3c96c00cd8421bff85282aec32422bdfebd31,Yuxuan Zhang,2025-07-08T06:22:04Z,"Glm 4 doc (#39247)  * update the glm4 model readme  * update test  * update GLM-4.1V model  * update as format  * update  * fix some tests  * fix the rest  * fix on a10, not t4  * nit: dummy import  ---------  Co-authored-by: raushan <raushan@huggingface.co>",154,76,230
huggingface/transformers,bbca9782ca1b8b358cc832a1b821aa1b450850da,Drew Ross,2025-07-07T22:56:57Z,Update LED model card (#39233)  * Update LED model card  * Remove extra arguments  * Apply suggestions from code review  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>,124,51,175
huggingface/transformers,41e865bb8dd373451a4db1874cf25252bdb0a1c6,Yih-Dar,2025-07-07T17:49:41Z,fix some flaky tests in `tests/generation/test_utils.py` (#39254)  fix  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,8,1,9
huggingface/transformers,93747d89eab1441af877064d0313bac536d60109,Cyril Vallez,2025-07-07T17:40:41Z,Simplify Mixtral and its modular children (#39252)  * simplify mixtral a lot  * fix  * other moes  * mixtral  * qwen3  * back  * Update modular_qwen3_moe.py,77,467,544
huggingface/transformers,3993ee1e988482d46384408c097aac28babad794,Mikhail Moskovchenko,2025-07-07T17:34:59Z,Add `segmentation_maps` support to MobileNetV2ImageProcessor (#37312)  * Add `segmentation_maps` support to mobilenet_v2 image processor and `reduce_labels` to mobilevit  * Changed mobilenetv2 tests to support fastimageprocessor  * added `segmentation_maps` support to fast image processor  * reverted to upstream/main  * Add optional  * Use autodocstring  * Changed docs  * Docs fix  * Changed fp to match beit fp  * Change typing imports  * Fixed repo inconsistency  * Added fast-slow equivalence tests  * Removed unnecessary call  * Add `reduce_labels` to Mobilevit fast processor  ---------  Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>,722,51,773
huggingface/transformers,b96f213fcfdaf8ccd18ad8864f70b39553cea331,Shohail Ismail,2025-07-07T16:57:42Z,Clarify per_device_train_batch_size scaling in TrainingArguments (#38… (#38857)  Clarify global batch size calculation in TrainingArguments (#38484),2,1,3
huggingface/transformers,969805256034d62965ca83e5aa4abbcbba9313ad,Joosun Hwang,2025-07-07T16:12:55Z,Add Korean translation for glossary.md (#38804)  * Add Korean translation for glossary.md  * Update docs/source/ko/glossary.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/ko/glossary.md  Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>  * Update docs/source/ko/glossary.md  Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>  * Update docs/source/ko/glossary.md  Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>  * Update docs/source/ko/glossary.md  Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>  * Update docs/source/ko/glossary.md  Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>  * Update docs/source/ko/glossary.md  Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>  * Update docs/source/ko/glossary.md  Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>  * Update docs/source/ko/glossary.md  Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>  * Update docs/source/ko/glossary.md  Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>  * Update docs/source/ko/glossary.md  Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>  * Update docs/source/ko/glossary.md  Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>  * Update docs/source/ko/glossary.md  Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>  ---------  Co-authored-by: Joosun40 <77312900+Joosun40@users.noreply.github.com> Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com> Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>,454,0,454
huggingface/transformers,bf203aa9da5af35e57cc78333537a2032f913692,Lucain,2025-07-07T13:58:36Z,Update tiny-agents example (#39245),1,3,4
huggingface/transformers,c4e39ee59c7ccc552e67889c1b81a574d5badf2e,kaixuanliu,2025-07-07T13:13:25Z,"adjust input and output texts for test_modeling_recurrent_gemma.py (#39190)  * adjust input and output texts for test_modeling_recurrent_gemma.py  Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>  * fix bug  Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>  * adjust  Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>  * update Expectation match  Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>  * fix  ---------  Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com> Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",15,4,19
huggingface/transformers,14cba7ad33279d18e42857251e56c944560dbe18,jiqing-feng,2025-07-07T13:12:02Z,enable xpu on kv-cache and hqq doc (#39246)  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>,19,16,35
huggingface/transformers,32db48db730e7a41e58ef043e50310275f7c629c,Cyril Vallez,2025-07-07T13:11:48Z,Fix patch helper (#39216)  remove -1,0,2,2
huggingface/transformers,a3618d485a321ab9389a250ca712c67775edf1cc,Pavel Iakubovskii,2025-07-07T13:05:28Z,"RotaryEmbeddings change `is not None` -> `isinstance(..., dict)` (#39145)  is None -> isinstance dict",53,53,106
huggingface/transformers,9b09fe479feb6ebf9d1e8ec0f84f009ebce7f36c,Yih-Dar,2025-07-07T13:04:26Z,fix `fastspeech2_conformer` tests (#39229)  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,14,16,30
huggingface/transformers,00e9efceab0958105aa1e64d63530e51daf6cda7,Zhen,2025-07-07T13:03:39Z,[bugfix] fix flash attention 2 unavailable error on Ascend NPU (#39166)  [bugfix] fix flash attention 2 error on Ascend NPU,10,16,26
huggingface/transformers,056fa73fae97f0db277939d89859139566dc4f81,Cyril Vallez,2025-07-07T12:52:57Z,[modular] Simplify logic and docstring handling (#39185)  * simplify a lot  * Update modular_model_converter.py  * finalize  * remove outdated functions  * apply it  * and examples,381,466,847
huggingface/transformers,f16fbfb89ad2c310ed998c3c9f8c9125dae6ae32,Xavier Dupré,2025-07-07T12:48:31Z,Make _compute_dynamic_ntk_parameters exportable (#39171)  * Make _compute_dynamic_ntk_parameters exportable  * add unit test,12,1,13
huggingface/transformers,4243bb844da660b387b3c409487e549754f7acc3,kaixuanliu,2025-07-07T12:47:04Z,"fix bug using FSDP V1 will lead to model device not properly set (#39177)  * fix bug using FSDP V1 will lead to model device not properly set  Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>  * update the code  Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>  ---------  Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>",4,5,9
huggingface/transformers,34c16167eb4f5733a357ac62f2e06c2a5d95bb0b,Yih-Dar,2025-07-07T12:43:50Z,Don't send new comment if the previous one is less than 30 minutes (unless the content is changed) (#39170)  fix  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,51,15,66
huggingface/transformers,b8f397e456251882c9f011596504a9c388065230,Daniel van Strien,2025-07-07T12:41:33Z,fix typo in Gemma3n notes (#39196),1,1,2
huggingface/transformers,5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5,Cyril Vallez,2025-07-07T12:36:43Z,"[modular] Follow global indexing and attribute setting, and their dependencies (#39180)  * export global indexing statements  * add example  * style  * examples",254,85,339
huggingface/transformers,8570bc29f3d994f0d96538987aedbe8ff383ea4a,Isotr0py,2025-07-07T11:54:18Z,Fix missing fast tokenizer/image_processor in whisper/qwen2.5-omni processor (#39244)  * fix missing fast tokenizer in whisper processor  Signed-off-by: Isotr0py <2037008807@qq.com>  * fix processor test  Signed-off-by: Isotr0py <2037008807@qq.com>  * fix qwen2.5 omni processor  Signed-off-by: Isotr0py <2037008807@qq.com>  ---------  Signed-off-by: Isotr0py <2037008807@qq.com>,6,6,12
huggingface/transformers,b283d52f7f89d9cf3c77cfef233c4cbf700959ff,Joshua Lochner,2025-07-07T10:14:08Z,[vjepa2] replace einsum with unsqueeze (#39234),1,1,2
huggingface/transformers,a325409a5051d68879030214e9c33180505f0d81,Rémi Ouazan,2025-07-07T09:42:33Z,Expectations re-order and corrected FA3 skip (#39195)  * Fix Expectations and a FA3 skip  * Fixed docstring  * Added context for Default expectation,16,8,24
huggingface/transformers,b0a8e0b8d7eba2af7346331dd1d48c50892867b2,zrohyun,2025-07-07T03:43:43Z,"[video processors] Support float fps for precise frame sampling (#39134)  * [video processors] Support float fps for precise frame sampling  Enable fractional fps values (e.g., 1.5, 29.97) in video processors for more precise frame sampling control.  - Change fps type from int to float across all video processors - Maintain backward compatibility with integer values  Extends: #38105  * [video processors] Refine fps typing to Union[int, float]  Change fps type from Optional[float] to Optional[Union[int, float]] for more explicit type information about supporting both integer and floating-point frame rates.  - Update type hints and docstrings across 8 files - Maintain backward compatibility - Clarify support for both int and float values  Extends: #38105  * Revert ""[video processors] Support float fps for precise frame sampling""  This reverts commit 7360d6e661b413ca0239e5ef61f9b1abbeab8e65.",20,20,40
huggingface/transformers,ca7e1a3756c022bf31429c452b2f313f043f32de,Arthur,2025-07-05T09:34:28Z,"Refactor the way we handle outputs for new llamas and new models (#39120)  * just update 2 files  * update other models as well just making fix-copies  * also add the changes needed to modeling utils  * put this on the pretrained model instead  * nits and fixes  * update generic, fix to use config value  * update other modelings  * use transformers kwargs instead  * update  * update  * update other models  * update  * updates  * update  * update  * update  * fix  * finally  * very small nits  * this fixes more tests  * fix other models as well!  * update modularqwen2  * update models based on qwen2  * update  * update  * remove the **flash stuff in favor of noraml kwargs  * update  * propagate gemma?  * remove output attentions  * propagate  * support cross attention edge case  * same  * test this  * fixes  * more fix  * update  * update  * fix conflicts  * update  * fix emu3  * fix emu3  * move the fix a bit  * quel enfer  * some fixes, loss_kwargs should never had been  * finish fixing gemma3n  * fix small lm3  * fix another one  * fix csm now  * fux csm and mistral  * fix mistral now  * small fixes  * fix janusss  * only for some models  * fixup  * phix phi3  * more fixes?  * dose this fix it?  * update  * holy shit it was just graph breaks  * protect torch  * updates  * fix samhq?  * fix moonshine  * more moonshine fixes, 3 failures left!  * nits  * generic needs to support more  * more fixes to moonshine!  * fix cross attention outputs!  * fix csm!  * nits  * fix stupid kosmos2  * current updates  * fixes  * use output recorder?  * nicer!  * a little bit of magic  * update  * fix protect  * fix  * small fixes  * protect import  * fix a bunch of more models  * fix fixups  * fix some of the last ones  * nit  * partly fix phi  * update  * fix import path  * make something that is fullgraph compatible just to be sure  * typing was wrong on llama so the rest was wrong as well  * fucking ugly but at least it is still exportable  * syle  * supposed to fix moonshine, it still breaks  * fix some default  * fix the last bits of sam  * update samhq  * more fixes to am hq  * nit  * fix all output+hidden states and output_attentions!  * fix?  * fix diffllama  * updates to fix initialization on the sam pips  * ups there was a bug  * fix the last sam hq test  * fix gotocr  * fix gotocr2!  * fixes  * skip stupid tests  * there was one left :)  * fixup  * fix fix copies issues with this test file  * fix copies for sam_hq  * rm some comments  * skip 2 more failing tests  * fix  * fix everything  * Apply suggestions from code review  Co-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com> Co-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>  * add more doc!  * fix public init  * fix modular qwen3  ---------  Co-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com> Co-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>",2005,5896,7901
huggingface/transformers,e6a8063ef1af16df964b644b07e1d17e96555d23,Yih-Dar,2025-07-04T11:35:53Z,Update expected values (after switching to A10) - part 8 - Final (#39220)  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,20,16,36
huggingface/transformers,cd8a041a4f6ecd8887bbf895493327edc82fc1b8,Yih-Dar,2025-07-04T10:48:10Z,Update expected values (after switching to A10) - part 7 (#39218)  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,60,14,74
huggingface/transformers,0cf27916f09a1a99af55ef4f2f3e8675372f38b6,Cyril Vallez,2025-07-04T07:01:56Z,Add packed tensor format support for flex/sdpa/eager through the mask! (#39194)  * Add the necesary logic to mask_utils  * add it everywhere  * Update masking_utils.py  * style  * Update masking_utils.py  * Update modeling_mimi.py  * Update masking_utils.py  * add support for more than batch size 1  * Update masking_utils.py  * add test  * style  * Update test_masking_utils.py  * Update masking_utils.py  * add require_token  * fix tests  * fix,303,9,312
huggingface/transformers,037755ed54208eefa77673b0af2a0b13e51f2fb1,Yih-Dar,2025-07-03T20:45:30Z,Update expected values (after switching to A10) - part 6 (#39207)  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,85,21,106
huggingface/transformers,1168f57abffd077d7d2687087aa10ba644a76a0d,Yih-Dar,2025-07-03T17:56:02Z,Update expected values (after switching to A10) - part 5 (#39205)  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,111,23,134
huggingface/transformers,7d9e52f376ad4b351ae696b0a62280cb9c63f70b,Lysandre Debut,2025-07-03T16:15:31Z,Fix continuous batching in `transformers serve` (#39149)  * Fix CB  * Nit  * Update src/transformers/commands/serving.py  Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>  * Add todos  ---------  Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>,38,28,66
huggingface/transformers,85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7,Joao Gante,2025-07-03T16:04:16Z,"[serve] Cursor support, move docs into separate page, add more examples (#39133)  * jan docs  * rm  * [cursor] tmp commit  * Cursor working :D  * Update docs/source/en/serving.md  Co-authored-by: Pedro Cuenca <pedro@huggingface.co>  * Update docs/source/en/serving.md  Co-authored-by: Pedro Cuenca <pedro@huggingface.co>  * Update docs/source/en/serving.md  Co-authored-by: Pedro Cuenca <pedro@huggingface.co>  * Update docs/source/en/serving.md  Co-authored-by: Pedro Cuenca <pedro@huggingface.co>  * Update docs/source/en/serving.md  Co-authored-by: Pedro Cuenca <pedro@huggingface.co>  * Update docs/source/en/serving.md  Co-authored-by: Pedro Cuenca <pedro@huggingface.co>  * Update docs/source/en/serving.md  Co-authored-by: Pedro Cuenca <pedro@huggingface.co>  * Update docs/source/en/serving.md  Co-authored-by: Pedro Cuenca <pedro@huggingface.co>  * Update src/transformers/commands/serving.py  Co-authored-by: Pedro Cuenca <pedro@huggingface.co>  * cursor docs  * try to fix agents/tools docs?  * try to fix agents/tools docs?  * Update docs/source/en/serving.md  Co-authored-by: Pedro Cuenca <pedro@huggingface.co>  * add transformers chat example with transformers serve  ---------  Co-authored-by: Pedro Cuenca <pedro@huggingface.co>",263,591,854
huggingface/transformers,e15b06d8dc6fa132550311d63c9758b580f39bcc,Pavel Iakubovskii,2025-07-03T14:22:47Z,"[typing] better return typehints for `from_pretrained` (#39184)  * config  * processor  * feature-extractor  * jukebox  * fixup  * update other methods in config  * remove ""PretrainedConfig"" annotations",36,27,63
huggingface/transformers,a25fc3592eec7a18aa20fe5d85bd335477896cbc,Yih-Dar,2025-07-03T13:13:06Z,Update expected values (after switching to A10) - part 4 (#39189)  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,294,134,428
huggingface/transformers,b31e9d19a6607aafdd921bc592897900712ba61d,Anton Vlasjuk,2025-07-03T10:02:58Z,[`Dia`] Change ckpt path in docs (#39181)  fix ckpt path,3,3,6
huggingface/transformers,18e0cae207a38d2c430b5fa08f9597312d1c1ab3,Ilyas Moutawwakil,2025-07-03T09:17:27Z,"Fix many HPU failures in the CI (#39066)  * more torch.hpu patches  * increase top_k because it results in flaky behavior when Tempreture, TopP and TopK are used together, which ends up killing beams early.  * remove temporal fix  * fix scatter operation when input and src are the same  * trigger  * fix and reduce  * skip finding batch size as it makes the hpu go loco  * fix fsdp (yay all are passing)  * fix checking equal nan values  * style  * remove models list  * order  * rename to cuda_extensions  * Update src/transformers/trainer.py",71,54,125
huggingface/transformers,bff964c429a5bfc8ca85789f20f37d6bfb60b294,Marc Sun,2025-07-03T09:07:11Z,Decouple device_map='auto' and tp_plan='auto'  (#38942)  * dissociate  * better place  * fix,6,4,10
huggingface/transformers,8178c43112295bf8c4ef04c667efbbbfd34b8bca,Wing Lian,2025-07-03T07:04:16Z,when delaying optimizer creation only prepare the model (#39152),1,1,2
huggingface/transformers,91221da2f1f68df9eb97c980a7206b14c4d3a9b0,Raushan Turganbay,2025-07-03T05:20:41Z,[glm4v] fix video inference (#39174)  fix video inference,6,6,12
huggingface/transformers,ebfbcd42da327b4a9f2d73c93a962be0a581faaa,Rémi Ouazan,2025-07-02T21:41:14Z,Test fixes for Aria (and some Expectation for llava_next_video) (#39131)  * Expectations for llava_next_video  * Updated image src in aria  * Fix test_small_model_integration_test  * Fix small model integration llama  * Fix a bunch of tests  * Style  * Shortened generation in test from 900 to 90,148,69,217
huggingface/transformers,37a239ca50885443a3216f56110a03f959509c80,Yih-Dar,2025-07-02T20:48:30Z,Update expected values (after switching to A10) - part 3 (#39179)  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,181,68,249
huggingface/transformers,9326fc332d4b8477fb1b990a5de486c70a94696d,Yih-Dar,2025-07-02T20:47:55Z,Update expected values (after switching to A10) - part 2 (#39165)  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * empty  * [skip ci]  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,420,193,613
huggingface/transformers,25cd65ac43ee1a96cef4692bda0b110d1e3c6903,Pedro Cuenca,2025-07-02T20:09:58Z,Random serve fixes (#39176)  * Fix index out of bounds exception on wrong kv reuse  * Prevent loading same model twice  ---------  Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com> Co-authored-by: Lysandre Debut <hi@lysand.re>,10,5,15
huggingface/transformers,548794b886a2186e9904ce6a90819eb2d0dfe266,Lysandre Debut,2025-07-02T20:06:47Z,[serve] Model name or path should be required (#39178)  * Model name or path should be required  * Fix + add tests  * Change print to log so it doesn't display in transformers chat,29,2,31
huggingface/transformers,2d561713f8e3eb3fcd219b44c9ea8f51db69c96c,Joao Gante,2025-07-02T17:29:16Z,[generate] document non-canonical beam search default behavior (#39000),15,3,18
huggingface/transformers,df12d87d184db59aed00b6b22c2daff7bac95204,Steven Liu,2025-07-02T14:56:29Z,[docs] ViTPose (#38630)  * vitpose  * fix?  * fix?  * feedback  * fix  * feedback  * feedback  * update sample image,187,177,364
huggingface/transformers,2b4a12b5bf5b5fa609c64268c600e81fe8623afc,Cyril Vallez,2025-07-02T13:55:05Z,Reduce Glm4v model test size significantly (#39173)  * fix test size  * Update test_modeling_glm4v.py,8,14,22
huggingface/transformers,e355c0a11c927d9e8f22409559c0fae76ccc598c,BUI Van Tuan,2025-07-02T13:03:57Z,Fix missing initializations for models created in 2024 (#38987)  * fix GroundingDino  * fix SuperGlue  * fix GroundingDino  * fix MambaModel  * fix OmDetTurbo  * fix SegGpt  * fix Qwen2Audio  * fix Mamba2  * fix DabDetr  * fix Dac  * fix FalconMamba  * skip timm initialization  * fix Encodec and MusicgenMelody  * fix Musicgen  * skip timm initialization test  * fix OmDetTurbo  * clean the code  Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>  * add reviewed changes  * add back timm  * style  * better check for parametrizations  ---------  Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>,229,98,327
huggingface/transformers,1125513a8da80d16e26cecfcbb508efc9038b5a7,Rémi Ouazan,2025-07-02T12:39:39Z,Blip2 fixes (#39080)  * Fixed some devices errors  * Fixed other device issues and more expectations  * Reverted support flags  * style  * More granular support  * Fixed some rebase stuff  * add a not None check before .to,50,18,68
huggingface/transformers,28df7f854ac4ec650c4a5057cc95a072d5efa5a8,Isotr0py,2025-07-02T11:57:15Z,Fix multimodal processor get duplicate arguments when receive kwargs for initialization (#39125)  * fix processor tokenizer override  Signed-off-by: Isotr0py <2037008807@qq.com>  * code format  Signed-off-by: Isotr0py <2037008807@qq.com>  * add regression test  Signed-off-by: Isotr0py <2037008807@qq.com>  * fix  Signed-off-by: Isotr0py <2037008807@qq.com>  * check image processor same  Signed-off-by: Isotr0py <2037008807@qq.com>  ---------  Signed-off-by: Isotr0py <2037008807@qq.com>,19,3,22
huggingface/transformers,b61023a1b760b207d99b699dafc1fbfde992c12c,Yaswanth Gali,2025-07-02T11:25:26Z,🚨🚨🚨 [eomt] make EoMT compatible with pipeline (#39122)  * Make EoMT compatible with pipeline  * Implicit patch offsets  * remove patch offsets from arg  * Modify tests  * Update example  * fix proc testcase  * Add few more args  * add pipeline test suite  * fix  * docstring fixes  * add pipeline test  * changes w.r.t review  * 🙈 MB  * should fix device mismatch  * debug  * Fixes device mismatch  * use decorator  * we can split mlp  * expected values update  ---------  Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>,113,92,205
huggingface/transformers,4d5822e65daa1ea14d199a8e7b893a01787cfcc1,Raushan Turganbay,2025-07-02T10:05:10Z,"[smolvlm] fix video inference (#39147)  * fix smolvlm  * better do as before, set sampling params in overwritten `apply_chat_template`  * style  * update with `setdefault`",40,12,52
huggingface/transformers,9b2f5b66d83e1b15dd1430f887f2037fd4039992,वेदांत,2025-07-02T09:45:50Z,fix default value of config to match checkpionts in LLaVa-OV models (#39163),1,1,2
huggingface/transformers,e8e0c76162263840661fc0ca0da3952861754759,Chong You,2025-07-02T02:11:03Z,Add activation sparsity reference in gemma3n doc (#39160)  Add activation sparsity reference in the description of gemma3n,2,1,3
huggingface/transformers,8e87adc45f20ba88360afbc29ab3f7a0063bf720,Yih-Dar,2025-07-01T21:27:22Z,fix `llama` tests (#39161)  * fix  * fix  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,13,19,32
huggingface/transformers,4c1715b6109184b062198793c3922ae1cffa79f9,Yih-Dar,2025-07-01T18:54:31Z,Update expected values (after switching to A10) (#39157)  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * fix  * empty  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,138,66,204
huggingface/transformers,ab59cc27fe1e166095f1b53e050a718fa7e86f34,Yih-Dar,2025-07-01T18:19:06Z,Suggest jobs to use in `run-slow` (#39100)  * pr  * pr  * pr  * pr  * pr  * pr  * pr  * pr  * pr  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,489,0,489
huggingface/transformers,db2f5354439f887f4ae0a46fb3f4a6dd4bec3b45,jiqing-feng,2025-07-01T18:06:37Z,update bnb ground truth (#39117)  * update bnb resulte  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * set seed to avoid sampling different results  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * fix int8 tests  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * fix typo  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * add comments  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  ---------  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>,10,0,10
huggingface/transformers,260846efadb9b03472427a46c30ba8f717d182c4,ybkurt,2025-07-01T17:10:29Z,fix: remove undefined variable (#39146),2,2,4
huggingface/transformers,cdfe49a4d0a364c4bdf2b828b67403994f00b092,rasmi,2025-07-01T16:29:16Z,Change `@lru_cache()` to `@lru_cache` to match styles from #38883. (#39093)  Match styles in #38883,1,1,2
huggingface/transformers,f46798193ecd617752e54099983598a912982b64,DavidS2106,2025-07-01T16:17:58Z,Fix: Ensure wandb logs config in offline mode (#38992)  * Fix: Ensure wandb logs config in offline mode  * Apply style fixes  ---------  Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com> Co-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>,5,1,6
huggingface/transformers,fe838d6631badce944d77c3822f09200d01951bb,Yih-Dar,2025-07-01T16:10:30Z,Fix missing fsdp & trainer jobs in daily CI (#39153)  fix  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,1,0,1
huggingface/transformers,128387757105c7c0b57b519ac2aaff217a20e3f0,StevenBucaille,2025-07-01T12:14:44Z,[superglue] fix wrong concatenation which made batching results wrong (#38850),2,2,4
huggingface/transformers,f8b88866f552e0eeb21059c90e2c30dba058c8e9,Raushan Turganbay,2025-07-01T11:33:20Z,"[VLMs] support passing embeds along with pixels (#38467)  * VLMs can work with embeds now  * update more models  * fix tests  * fix copies  * fixup  * fix  * style  * unskip tests  * fix copies  * fix tests  * style  * omni modality models  * qwen models had extra indentation  * fix some other tests  * fix copies  * fix test last time  * unrelated changes revert  * we can't rely only on embeds  * delete file  * de-flake mistral3  * fix qwen models  * fix style  * fix tests  * fix copies  * deflake the test  * modular reverted by fixes, fix again  * flaky test, overwritten  * fix copies  * style",1136,1710,2846
huggingface/transformers,20901f1d681669fa402f47edce49873432c3212e,Ayush Singh,2025-07-01T10:29:52Z,[typing] LlamaAttention return typehint  (#38998)  * helo llama  * helo llama  * helo llama  * apply modular  * fix dia  ---------  Co-authored-by: qubvel <qubvel@gmail.com>,12,12,24
huggingface/transformers,7a25f8dfdba4c710d278d8312ef2522c5996a894,Raushan Turganbay,2025-07-01T10:18:37Z,"[qwen2-vl] fix FA2 inference (#39121)  * fix FA2  * update is causal flag and remove mask for FA2  * update for FA2 with varlen path  * how the tests were passing with different devices?  * add comment and ref to the PR  * move mask preparation to base pretrained model  * seq len is the first dim, not second  * fix copies to fix GLM4V",363,199,562
huggingface/transformers,def96632394fae03689019d2ed552f4790eb7d21,Mehant Kammakomati,2025-07-01T10:03:22Z,feat: support indivisible shards for TP model loading and TPlizing. (#37220)  * feat: support uneven loading and sharding resolve merge conflicts Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>  * fix: allow for empty tensor computations  Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>  * test: add llama1b test case  Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>  * due to q_proj colwise it has to be multi of 2  Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>  * refactor: use slice API  Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>  * refactor: use slice API  Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>  * refactor: use slice API  Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>  * refactor: use slice API  Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>  ---------  Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>,58,8,66
huggingface/transformers,06c4a4d499aeb213c558d6fb59adf864a6062dad,jiqing-feng,2025-07-01T09:32:20Z,fix caching_allocator_warmup with tie weights (#39070)  * fix caching_allocator_warmup with tie weights  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * fix comment  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  ---------  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>,5,0,5
huggingface/transformers,e4355747213c32885ddc4128d46708f8fcf847be,Raushan Turganbay,2025-07-01T09:08:21Z,"🚨 Don't use cache in non-generative models (#38751)  * deprecate for 1 version  * style  * fix some tests  * fix esm  * skip for now, GC requires positional args but we have keyword args  * remove transpose for scores in modified models only  * skip fx trace tests",991,2350,3341
huggingface/transformers,dbc98328da2cabe7938423c51569252f2b49a5b3,Cyril Vallez,2025-07-01T08:34:53Z,Several fixes for Gemma3n (#39135)  * remove the skips  * fix the epsilon to a small value (does not make sense otherwise)  * safeguard  * overload test_eager_matches_sdpa  * Update test_modeling_common.py  * skip appropriate tests  * correct no_split_layer  * fix all devices issue  * fix backward  * fix,491,390,881
huggingface/transformers,d53518c5f2dd7ada022ff5b725c684c9ed89cb44,BUI Van Tuan,2025-07-01T07:47:53Z,Fix key mapping for VLMs (#39029)  * fix key mapping for VLMs  * use __mro__ instead  * update key mapping in save_pretrained,8,2,10
huggingface/transformers,3457e8e73e4f5532cc69059682b1ba4484d7e7e8,eustlb,2025-06-30T19:55:36Z,[Whisper] update token timestamps tests (#39126)  * fixes  * update comment  * update for A10  * all a10  * all a10  * all a10  * all a10  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,18,14,32
huggingface/transformers,fe35eca7bded3e6190f2d760849712d3031f6319,Drew Ross,2025-06-30T17:42:56Z,Update BigBirdPegasus model card (#39104)  * Update igbird_pegasus.md  * Apply suggestions from code review  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>,112,48,160
huggingface/transformers,29a3f5ed8c5588151419012408f394b4644d4aa6,Yao Matrix,2025-06-30T15:54:05Z,switch default xpu tp backend to pytorch built-in XCCL from pytorch 2.8 (#39024)  * switch default xpu tp backend to pytorch built-in XCCL from pytorch 2.8  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * Update docs/source/en/perf_infer_gpu_multi.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update perf_infer_gpu_multi.md  * Update perf_infer_gpu_multi.md  * Update perf_infer_gpu_multi.md  ---------  Signed-off-by: YAO Matrix <matrix.yao@intel.com> Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>,6,4,10
huggingface/transformers,9e0c865b8be73cfe7aac1b5aa146cd7839784c8a,Vladimir Gutuev,2025-06-30T15:53:43Z,docs: correct two typos in awesome-transformers.md (#39102)  * docs(awesome-projects): fix typo “Itt leverages” → “It leverages” (#39101)  closes #39101  * docs(awesome-projects): fix grammar “We provides” → “We provide” (#39101)  closes #39101,2,2,4
huggingface/transformers,03db2700abf84971351c7374a548a9d4fc156916,jiqing-feng,2025-06-30T14:56:55Z,Enable XPU doc (#38929)  * fix example with dataset  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * update torchao doc  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * update torchao doc  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * fix device type  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * revert torchao change  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * fix torchao doc  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * revert torchao change  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * update xpu torchao doc  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * update chat_templating_multimodal.md  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * use full name for int8  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  * revert int8 title  Signed-off-by: jiqing-feng <jiqing.feng@intel.com>  ---------  Signed-off-by: jiqing-feng <jiqing.feng@intel.com> Co-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>,75,8,83
huggingface/transformers,ea0ea392e57f8816f9ab8e5f740577a0343a1594,Joao Gante,2025-06-30T13:47:48Z,Fix chat (#39128),6,3,9
huggingface/transformers,ed36f8490eb3748f2424d854936cc6f816aeb486,Lysandre Debut,2025-06-30T13:25:36Z,Licenses (#39127)  * Licenses  * Licenses,27,1,28
huggingface/transformers,e8f90b5397df419f211498ef399f9255790b8428,Lysandre Debut,2025-06-30T13:10:53Z,Split `transformers chat` and `transformers serve`  (#38443)  * Next token  * Split chat and serve  * Support both generation methods  * Style  * Generation Config  * temp  * temp  * Finalize serving.py  Co-authored-by: =?UTF-8?q?c=C3=A9lina?= <hanouticelina@gmail.com>  * Finalize chat.py  * Update src/transformers/commands/serving.py  Co-authored-by: célina <hanouticelina@gmail.com>  * Lucain's comments  Co-authored-by: Lucain <lucain@huggingface.co>  * Update  * Last comments on PR  * Better error handling  * Better error handling  * CI errors  * CI errors  * Add tests  * Fix tests  * Fix tests  * [chat] Split chat/serve (built on top of lysandre's PR) (#39031)  * Next token  * Split chat and serve  * Support both generation methods  * Style  * Generation Config  * temp  * temp  * Finalize serving.py  Co-authored-by: =?UTF-8?q?c=C3=A9lina?= <hanouticelina@gmail.com>  * Finalize chat.py  * Update src/transformers/commands/serving.py  Co-authored-by: célina <hanouticelina@gmail.com>  * Lucain's comments  Co-authored-by: Lucain <lucain@huggingface.co>  * Update  * Last comments on PR  * Better error handling  * Better error handling  * CI errors  * CI errors  * Add tests  * Fix tests  * Fix tests  * streaming tool call  * abstract tool state; set tool start as eos  * todos  * server working on models without tools  * rm chat's deprecated flags  * chat defaults  * kv cache persists across calls  * add server docs  * link  * Update src/transformers/commands/serving.py  * Apply suggestions from code review  * i love merge conflicts  * solve multi turn with tiny-agents  * On the fly switching of the models  * Remove required positional arg  ---------  Co-authored-by: Lysandre <hi@lysand.re> Co-authored-by: =?UTF-8?q?c=C3=A9lina?= <hanouticelina@gmail.com> Co-authored-by: Lucain <lucain@huggingface.co>  * Protect names  * Fix tests  ---------  Co-authored-by: =?UTF-8?q?c=C3=A9lina?= <hanouticelina@gmail.com> Co-authored-by: Lucain <lucain@huggingface.co> Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>,915,310,1225
huggingface/transformers,539c6c2fa8abc9bb97218ac0b3c3d143ba800e05,Yih-Dar,2025-06-30T12:23:27Z,All CI jobs with A10 (#39119)  all a10  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,38,38,76
huggingface/transformers,ed9f252608389c4e5cb2c5a94f5cc47d76855842,Ryan Mullins,2025-06-30T12:10:51Z,docs: Gemma 3n audio encoder (#39087)  Updating Gemma 3n docs and docstrings to clarify the relationship between the newly trained audio encoder used in Gemma 3n and the USM model from the original paper.,12,12,24
huggingface/transformers,4a79bf947d0614d2a023b9137a32cf754ac241fe,Yuxuan Zhang,2025-06-30T10:16:22Z,Fix some bug for finetune and batch infer For GLM-4.1V (#39090)  * update  * 1,13,14,27
huggingface/transformers,2100ee654569d323bfb77266cd3a75070abfda97,Yao Matrix,2025-06-30T09:49:03Z,fix UT failures on XPU w/ stock PyTorch 2.7 & 2.8 (#39116)  * fix UT failures on XPU w/ stock PyTorch 2.7 & 2.8  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * zamba2  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * xx  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * internvl  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * tp cases  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  ---------  Signed-off-by: YAO Matrix <matrix.yao@intel.com>,120,52,172
huggingface/transformers,ccf2ca162e33f381e454cdb74bf4b41a51ab976d,Yih-Dar,2025-06-27T21:08:14Z,skip some `test_sdpa_can_dispatch_on_flash` (#39092)  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,18,1,19
huggingface/transformers,a11f69289572955f4be9d2bc7b7c5dd949722fc1,st81,2025-06-27T18:25:32Z,Fixes the failing test `test_is_split_into_words` in `test_pipelines_token_classification.py` (#39079)  * Fix test pipelines token classification for is_split_into_words  * Fix incorrect import format,6,4,10
huggingface/transformers,18143c76bfa86792d293d646bb795935c2266967,Sandeep Yadav,2025-06-27T17:35:30Z,Sandeepyadav1478/2025 06 19 deberta v2 model card update (#38895)  * [docs]: update deberta-v2.md model card  * chore: req updates  * chore: address code review feedback and update docs  * chore: review feedback and updates  * chore: model selection updates  * chores: quantizations review updates,102,57,159
huggingface/transformers,02a769b05860d2390e837309c3b41e99218b6555,Steven Liu,2025-06-27T16:38:21Z,[fix] Add FastSpeech2ConformerWithHifiGan (#38207)  * add to mapping  * oops  * oops  * add to config_mapping_names  * revert  * fix?  * config-mapping-names  * fix?  * fix?,7,1,8
huggingface/transformers,c2dc72bb5f15fcfbba061a8b243997bf424d67df,Benjamin Bossan,2025-06-27T16:33:11Z,"TST Fix PEFT integration test bitsandbytes config (#39082)  TST Fix PEFT integration test bitsandbytes config  The PEFT integration tests still used load_in_{4,8}_bit, which is deprecated, moving to properly setting BitsAndBytesConfig. For 4bit, also ensure that nf4 is being used to prevent  > RuntimeError: quant_type must be nf4 on CPU, got fp4",27,5,32
huggingface/transformers,c8064bea9a2482b741de87e2b7e4faa93181da72,Matej Sirovatka,2025-06-27T15:28:05Z,Fix: unprotected import of tp plugin (#39083),0,1,1
huggingface/transformers,dd7dc4a4a2281c4a3eda1247fc05e34149a55786,farrosalferro,2025-06-27T15:26:57Z,Add Fast Image Processor for Chameleon (#37140)  * Add Fast Image Processor for Chameleon  * add warning to resize and move blend_rgba to convert_to_rgb  * Remove unrelated files  * Update image_processing_chameleon_fast to use auto_docstring  * fix equivalence test  ---------  Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com> Co-authored-by: yonigozlan <yoni.gozlan@huggingface.co>,223,79,302
huggingface/transformers,6d773fc3bc936b4dfa9b97d46cc9250dddfa2e1f,Yih-Dar,2025-06-27T14:54:11Z,fix `dots1` tests (#39088)  fix  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,4,0,4
huggingface/transformers,c8764ab9353f7cd822f1184a0e9848cef5c04a6f,Tijana Vukovic,2025-06-27T14:49:47Z,guard torch distributed check (#39057)  * guard torch distributed check  * Update src/transformers/pipelines/base.py  ---------  Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>,1,1,2
huggingface/transformers,49d9fd49bd3d58853d461295bc2fd4f2c808de87,MinJu-Ha,2025-06-27T14:40:24Z,Add Fast Image Processor for mobileViT (#37143)  * Add image_processing_mobilevit_fast.py  * Fix copies  * update _preprocess for channel_flip  * Update for batched image processing  * Resolve merge conflicts with main  * Fix import order and remove trailing whitespace (ruff clean-up)  * Fix copy inconsistencies  * Add NotImplementedError for post_process_semantic_segmentation to satisfy repo checks  * Add auto_docstring  * Adjust style  * Update docs/source/en/model_doc/mobilevit.md  Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>  * Update src/transformers/models/mobilevit/image_processing_mobilevit_fast.py  Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>  * Update src/transformers/models/mobilevit/image_processing_mobilevit_fast.py  Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>  * Delete not used function  * test: add missing tests for  and  * Add post_process_semantic_segmentation to mobilevit_fast.py  * Add preprocess function to image_processing_mobilebit_fast.py  * ruff check for formatting  * fix: modify preprocess method to handle BatchFeature correctly  * Remove logic for default value assignment  Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>  * Remove normalization adn RGB conversion logic not used in slow processor  Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>  * Simplify return_tensors logic using one-liner conditional expression  Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>  * Remove unused normalization and format parameters  Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>  * add **kwargs and remove default values in _preprocess  * add slow_fast equivalence tests for segmentation  * style: autoformat code with ruff  * Fix slow_fast equivalence test  * merge + remove skipped test  ---------  Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com> Co-authored-by: yonigozlan <yoni.gozlan@huggingface.co>,397,115,512
huggingface/transformers,4336ecd1eaae778a24633dea6c62b3a90fb8afd1,Nahieli,2025-06-27T14:39:43Z,add fast image processor nougat (#37661)  * add fast image processor nougat  * test fixes  * docstring white space  * last fixes  * docstring_type  * tolerance unit test  * fix tolerance  * fix rtol  * remove traling white space  * remove white space  * note for tolerance unit test  * fix tests  * remove print  ---------  Co-authored-by: yonigozlan <yoni.gozlan@huggingface.co> Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>,499,47,546
huggingface/transformers,0c35280e58ea4a297c1a62f22523bc454301276b,Benjamin Bossan,2025-06-27T13:58:10Z,TST PEFT integration tests with pipeline generate (#39086)  Some PEFT integration tests involving text generation pipelines were failing since #38129 because the base model is too small to generate longer sequences. Setting max_new_tokens fixes this.,2,2,4
huggingface/transformers,993665a5ffc9bb985c2adb1a51b94d8bad9b040a,JINO ROHIT,2025-06-27T13:57:56Z,fixed typo for docstring in prepare_inputs method (#39071),1,1,2
huggingface/transformers,839893c86bf372ee35b2c8dd750d3cdc21a995f5,Yih-Dar,2025-06-27T13:44:10Z,fix `mistral3` tests (#38989)  * fix  * fix  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,9,14,23
huggingface/transformers,2b85b6ce1978c776585cc20bdb013334f1c91e6c,eustlb,2025-06-27T12:51:43Z,[Whisper] 🚨 Fix pipeline word timestamp: timestamp token is end of token time !!! (#36632)  * timestamp token is end of token time !!!  * ensure correct alignment between tokens and timestamp tokens  * ignore input tokens for DTW computation  * use num_frames to avoid token timestamp hallucinations  * token timestamps test updates !  * num_frames: deprecate and use attention_mask instead  * avoid breaking change  * fix the pipeline usage for chunk approach  * make style  * better logging  * better logging  * make style  * update tests with correct values,83,60,143
huggingface/transformers,9c8d3a70b8bf359150c960c4281aaa853498fe8c,eustlb,2025-06-27T12:32:03Z,Pipeline: fix unnecessary warnings (#35753)  * return attention mask  * use correct model input name  * fix  * make,13,5,18
huggingface/transformers,1750c518dda15a8b81cff276292674d61152dbf5,Yaswanth Gali,2025-06-27T12:18:18Z,✨ Add EoMT Model ||  🚨 Fix Mask2Former loss calculation (#37610)  * Initial Commit  * up  * More changes  * up  * Only mask_logits mismatch  * close enough logits debug later  * fixes  * format  * Add dummy loss  * Close enough processing for semantic seg  * nit  * Added panoptic postprocessor  * refactor  * refactor  * finally fixed panoptic postprocessor  * temp update  * Refactor ForUniversalSegmentation class  * nits and config update  * Few fixes and inference matches  * change mapping  * Added training support but loss slightly off 🥲  * Loss is matching 😀  * update  * Initial tests skelton  * changes  * tests update  * more modular  * initial tests  * updates  * better docstrings  * changes  * proc tests passing :)  * Image processor update  * tiny change  * QOL changes  * Update test w.r.t latest attn refactor  * repo-consistency fixes  * up  * Image proc fix and integration tests :)  * docs update  * integration tests  * fix  * docs update 🥰  * minor fix  * Happy CI  * fix  * obvious refactoring  * refactoring w.r.t review  * Add fask image proc skelton  * Fast Image proc and cleanups  * Use more modular  * tests update  * Add more tests  * Nit  * QOL updates  * change init_weights to torch default  * add eager func coz of make style  * up  * changes  * typo fix  * Updates  * More deterministic tests  * More modular  * go more modular 🚀  * up  * dump  * add supprot for giant ckpts  * overhaul  * modular  * refactor  * instace seg is ready  * cleanup  * forgot this  * docs cleanup  * minor changes  * EoMT - > Eomt  * Happy CI  * remove redundant comment  * Change model references  * final change  * check annealing per block  * My other PR changes 😂  ---------  Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>,4923,1,4924
huggingface/transformers,0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3,Yao Matrix,2025-06-27T12:01:53Z,fix a bunch of XPU UT failures on stock PyTorch 2.7 and 2.8 (#39069)  * fix a bunch of XPU UT failures on stock PyTorch 2.7 and 2.8  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * qwen3  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * quanto  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * models  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * fix style  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * idefics2  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  ---------  Signed-off-by: YAO Matrix <matrix.yao@intel.com>,53,31,84
huggingface/transformers,cb17103bd5e31373e090f2f37602dcc992c017e4,Mohamed Mekkouri,2025-06-27T11:51:46Z,Uninstallling Flash attention from quantization docker (#39078)  * update  * revert,3,0,3
huggingface/transformers,371c4711136386075bfb272692860c1d4ee9c1d2,BUI Van Tuan,2025-06-27T10:39:37Z,Fix initialization of OneFormer (#38901)  * fix initialization of OneFormer  * remove redundant initializations  * remove redundant initializations  * remove redundant initializations  * keep BC,57,52,109
huggingface/transformers,540a10848c26ebec9a0e749d3808333bdae08167,Yih-Dar,2025-06-27T10:28:10Z,fix `Gemma3nProcessorTest` (#39068)  * fix  * fix  * oups forgot style  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com> Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>,7,1,8
huggingface/transformers,0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0,Yaswanth Gali,2025-06-27T10:14:09Z,Cleanup Attention class for Siglip and dependent models (#39040)  * cleanup attention class  * More models  * more models  * Changes  * make style  * Should fix CI  * This should work 🙏,23,97,120
huggingface/transformers,1ccc73dee9018dad5dcbadff31851d7c663b8b51,eustlb,2025-06-27T09:27:42Z,[Whisper] fix shape mismatch in tests (#39074)  fix shape mismatch,1,1,2
huggingface/transformers,a52478253bbe522a420e88ea3940d4d98a935300,Steven Liu,2025-06-26T21:40:45Z,[docs] Tensor parallelism (#38241)  * updates  * feedback  * badges  * fix?  * fix?  * fix?  * fix?,212,209,421
huggingface/transformers,84e8696caebea4cc8afb16a62d5eaae29f01fdd9,Steven Liu,2025-06-26T21:21:54Z,[docs] @auto_docstring (#39011)  * refactor  * feedback,107,103,210
huggingface/transformers,018855de636538aeaf9f49c596f9682431d87f53,Drew Ross,2025-06-26T20:54:48Z,Update PEGASUS-X model card (#38971)  * Update PEGASUS-X model card  * Add cache_implementation argument in quantization code example  * Update CLI example  * Apply suggestions from code review  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Remove TensorFlow and Flax badges  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>,106,26,132
huggingface/transformers,757c26fb40cbeeef3a1288219503acd23febd034,Steven Liu,2025-06-26T19:25:14Z,[docs] Model contribution (#38995)  improve,5,5,10
huggingface/transformers,b372bb5ed1ef618739ee205e629204a866dd755e,Yih-Dar,2025-06-26T18:07:17Z,fix `layoutlmv3` tests (#39050)  * fix  * fix  * fix  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,4,2,6
huggingface/transformers,f171e7e884f4435a372b0690a50db251bc4302a8,StevenBucaille,2025-06-26T17:13:06Z,Update SuperPoint model card (#38896)  * docs: first draft to more standard SuperPoint documentation  * Apply suggestions from code review  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * docs: reverted changes on Auto classes  * docs: addressed the rest of the comments  * docs: remove outdated reference to keypoint detection task guide in SuperPoint documentation  * Update superpoint.md  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>,85,85,170
huggingface/transformers,2f50230c59ec9f17431236ed6625082cc385c76c,Yih-Dar,2025-06-26T16:48:14Z,fix `t5gemma` tests (#39052)  * fix  * fix  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,27,6,33
huggingface/transformers,23b7e73f0581a880370477597dc948e07c2f064b,Yih-Dar,2025-06-26T16:36:56Z,fix `test_compare_unprocessed_logit_scores` (#39053)  fix  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,1,1,2
huggingface/transformers,58c768922618cf11ce769fb8368c26c6db54c535,Anton Vlasjuk,2025-06-26T16:23:55Z,"[`Flex Attn`] Fix torch 2.5.1 incompatibilities (#37406)  * remove compile on mask creation, ensure kv blocks do not explode on indices  * trigger ci  * switch dynamic compilation to false  * patch new masking functions as well  * add len check  * i was wrong  * last comment",40,6,46
huggingface/transformers,5154497607970fbd8a03f89a767dffb65619b5ce,Lysandre,2025-06-26T16:04:36Z,Dev version,53,53,106
huggingface/transformers,0a8081b03d118da9a8c3fa143a03afe54a5c624e,Kyle Sayers,2025-06-26T15:56:33Z,[Modeling] Fix encoder CPU offloading for whisper (#38994)  * fix cpu offloading for whisper  Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>  * unskip offloading tests  Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>  * revert small change  Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>  * remove tests  Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>  ---------  Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>,2,18,20
huggingface/transformers,c63cfd6a833d629a74c098933017c61dd755969d,Ryan Mullins,2025-06-26T15:55:47Z,"Gemma 3n (#39059)  * Gemma 3n  * initial commit of Gemma 3n scaffold  * Fixing param pass through on Gemm3p5RMSNorm  * Adds Einsum layer to Gemma 3n  * Updating EinsumLayer API  * Undoing erroneous force push  * Reverting RMSNorm to with_scale by default  * Adds LAuReL to Gemma 3n  * Adds AltUp to Gemma 3n  * Adding Gemma3p5 overall and text config with vision and audio config placeholders (#3)  * Adding gemma3p5 text configs  * Adding audio config placeholders  * Adding a placeholder for vision configs  * Updating MobileNetVisionConfig, inheriting TimmWrapperConfig  * Updating text configs  * Update src/transformers/models/gemma3p5/modular_gemma3p5.py  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Removing altup configs to accept the suggested configs  * Update src/transformers/models/gemma3p5/modular_gemma3p5.py  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Updating altup config  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Addressing review comments and updating text configs  * Adding a config for activation sparsity  * Updating configs to pass through options to super class init and adjust some name prefixes  * Updating laurel and altup with corrected config values  * Normalizing sub_config initializers  ---------  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Updating MLP with activation sparsity (#2)  * Updating DecoderBlock for Gemma 3n (#3)  * Initial Gemm3nTextModel (#4)  NOTE: This implementation WILL CHANGE in the coming weeks, however, changes will be strictly additive and this will remain a suitable baseline for downstream implementations to reference.  * Adding KV Cache Sharing  * Adds Einsum layer to Gemma 3n  * Updating EinsumLayer API  * Refactored kv cache sharing in attention  * Adding KVStore for cache sharing  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update src/transformers/cache_utils.py  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Undoing erroneous force push  * Reverting RMSNorm to with_scale by default  * Adds LAuReL to Gemma 3n  * Updating KV Cache Sharing implementation  * Updating the q and k norm definitions in the attention module  * Fixing name error for q,k,v RMS norm to use the right 3n module  * Updating MLP with activation sparsity  * Updating DecoderBlock for Gemma 3.5  * Updating kv cache sharing implementation with the use of a cache buffer and refactoring some lines of code  * Isolating KV Cache logic to relevant components  * Fixing logic error in Gemma3nAttention.forward  * Refactoring caching contributions and fixing kv_store initialization  * Simplifying Configs  * Remove errant self from super init call  * Bug fix in the Attention module - changing self.head_dim to config.head_dim  * Bug fixes in the LaurelBlock and RMS Norm super init call  * removing redundant code from a merge  * Adding per_layer_inputs to TextModel  * Adding preprocess embeddings with altup  * Adds per-layer-to-single output and a host of TODOs  * Integrating altup predict with the model workflow and other minor bug fixes  * Using nn.Embedding temporarily for text model  * It goes forward  * Minor refactor of attention sparsity and RoPE initialization  * Fixing duplicate rope_scaling param bug when loading from pretrained  ---------  Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com> Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>  * Normalizing on altup_num_inputs config option  * regenerating modeling file after syncing to HEAD  * Use torch.std(..., unbiased=False) for activation sparsity (#8)  * Refactoring to a single QVK Norm (#13)  * AltUp: support scale_corrected_output (#14)  * Converts einsums to nn.Linear (#7)  * Converts einsums to nn.Linear  * Removing unused variables  * Aligning SharedKVCache with HybridCache (#11)  * Alinging SharedKVStore with HybridCache  * Remove KVStore. Refactor apply_rotary_pos_emb for sharing  * Addressing review comments  * Supporting split modality embeddings in Gemma3n (#10)  * Adding the Embedder class  * Update modular  Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>  * Update modular  Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>  * Update modular  Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>  * Update modular  Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>  * Update modular  Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>  * Update modular  Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>  * Addressing review comments, adding audio embedding layers, integrating embedder with the remaining architecture, adding a forward method for conditional generation  * Apply suggestions from code review  Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>  * Update modular  Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>  * Addressing review comments, prop drilling audio and vision configs to the text config  * Removing TODO's that have been addressed  * Simplify Embedder init and add audio embeddings  * Embeddings refactor. Adds Gemma3nAudioEmbedder and Gemma3nVisionEmbedder  * Refactoring vision and audio embeddings into ConditionalGeneration model  ---------  Co-authored-by: Ryan Mullins <ryan@ryanmullins.org> Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Updating attention mask for Gemma 3.5 (#15)  * xxx_token_index to xxx_token_id  * remvoing deprecated last_cache_position  * Removing references to SigLIP  * Always init per-layer inputs  * Using torch.finfo().min for epsilon_tensor  * Gemma3nDecoderLayer inherits from Gemma3DecoderLayer. Remove gating lambdas  * fix modular GEMMA3N_INPUTS_DOCSTRING  * Gemma3nAttention inherits from Gemma3Attention  * Modular inheritance fixes  * CausalLM conversion script for 4B model (#16)  * Add Gemma3n Audio Encoder (#6)  * initial commit of Gemma 3.5 scaffold  * Fixing param pass through on Gemm3nRMSNorm  * Adds Einsum layer to Gemma 3.5  * Updating EinsumLayer API  * Undoing erroneous force push  * Reverting RMSNorm to with_scale by default  * Adds LAuReL to Gemma 3n  * Adds AltUp to Gemma 3n  * Adding Gemma3n overall and text config with vision and audio config placeholders (#3)  * Adding gemma3n text configs  * Adding audio config placeholders  * Adding a placeholder for vision configs  * Updating MobileNetVisionConfig, inheriting TimmWrapperConfig  * Updating text configs  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Removing altup configs to accept the suggested configs  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Updating altup config  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Addressing review comments and updating text configs  * Adding a config for activation sparsity  * Updating configs to pass through options to super class init and adjust some name prefixes  * Updating laurel and altup with corrected config values  * Normalizing sub_config initializers  ---------  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Updating MLP with activation sparsity (#2)  * Updating DecoderBlock for Gemma 3.5 (#3)  * Initial Gemm3nTextModel (#4)  NOTE: This implementation WILL CHANGE in the coming weeks, however, changes will be strictly additive and this will remain a suitable baseline for downstream implementations to reference.  * Adding KV Cache Sharing  * Adds Einsum layer to Gemma 3.5  * Updating EinsumLayer API  * Refactored kv cache sharing in attention  * Adding KVStore for cache sharing  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update src/transformers/cache_utils.py  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Undoing erroneous force push  * Reverting RMSNorm to with_scale by default  * Adds LAuReL to Gemma 3n  * Updating KV Cache Sharing implementation  * Updating the q and k norm definitions in the attention module  * Fixing name error for q,k,v RMS norm to use the right Gemma 3n module  * Updating MLP with activation sparsity  * Updating DecoderBlock for Gemma 3.5  * Updating kv cache sharing implementation with the use of a cache buffer and refactoring some lines of code  * Isolating KV Cache logic to relevant components  * Fixing logic error in Gemma3nAttention.forward  * Refactoring caching contributions and fixing kv_store initialization  * Simplifying Configs  * Remove errant self from super init call  * Bug fix in the Attention module - changing self.head_dim to config.head_dim  * Bug fixes in the LaurelBlock and RMS Norm super init call  * removing redundant code from a merge  * Adding per_layer_inputs to TextModel  * Adding preprocess embeddings with altup  * Adds per-layer-to-single output and a host of TODOs  * Integrating altup predict with the model workflow and other minor bug fixes  * Using nn.Embedding temporarily for text model  * It goes forward  * Minor refactor of attention sparsity and RoPE initialization  * Fixing duplicate rope_scaling param bug when loading from pretrained  ---------  Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com> Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>  * Normalizing on altup_num_inputs config option  * Adding audio encoder config  * Adds high-level components for Audio Encoder  * Implement uniform reducer for Audio Encoder  * Adding placeholders for Conformer components in Audio Encoder  * Adding placeholders for SubSampleConvProjection components in Audio Encoder  * Adding SequenceLayer component placeholders  * Implementing Gemma3nAudioEncoder with nn.Sequential  * Implementing Gemma3nAudioSubSampleConvProjection with nn.Sequential  * Implementing Conformer model with SequenceLayers  * Use OrderedDict in nn.Sequential initializers  * Implements sl.Residual in Torch with nn.Sequential and OrderedDict  * Adopting a base SequenceLayer class with default forward() method  * Implementing sl.GatedLinearUnit in Torch  * Implementing sl.Swish in Torch  * Implementing sl.ReLU in Torch  * Implementing sl.Scale in Torch  * Removing sl.Dropout after tree-shaking  * Implementing sl.RMSNorm in Torch with fake shape  * Implementing sl.GroupNorm in Torch  * Implementing sl.Conv2d in Torch  * Implementing sl.Dense in Torch  * Removing sl.Delay layers, which act as pass-throughs  * Connecting shapes to configs in initializers  * Removing sl.Emit  * Implementing sl.ExpandDims in Torch  * Adding sl.GradientClipping to Torch  * Implementing sl.DenseShaped in Torch  * Implementing sl.LDPA in Torch  * Removing unused sl.CombinedQKVProj class  * Fixing erroneous type hint  * Implemnenting sl.DepthwiseConv1D in Torch  * Implementing sl.MaskInvalid in Torch  * Fixes for initialization  * Fixes for saving weights  * Removing einsums per feedback from HF staff  * Removing Sequence Layers idioms from audio encoder  * Fixes for reviewer comments  * CausalLM conversion script for 4B model  * inv_timescales to non-persistent buffer  * Addressing audio encoder Attention feedback  * Addressing Gemma3nAudioSSCPConvBlock feedback  * Addressing Gemma3nAudioConformerAttention feedback  * Addressing padding feedback  * Weights conversion loads audio state dict  * Always use vision_config so saving works  * Token id updates for configs  * Stubs for interleaving audio embs  * Addressing reviewer feedback  ---------  Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com> Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>  * Fixing cache access error  * Removing duplicate code from a bad merge  * Gemma 3n Text + Vision Part 1 (#17)  * testing utilities for numerics comparisons  * Corrected einsum to nn.Linear weights conversion  * Inherit scaled word embs from Gemma3 not Bart  * Fixing transposes for collapsed linears  * More transpose fixes  * numpy api fix  * RMSNorm: Explicit kwargs, scale_shift=0.0 when with_scale=True  * Force AltUp  to float32  * Updating debugging script for AudioEncoder debugging  * Support divide_weight_by_sqrt_fan_in from JAX for per-layer inputs  * Correcting attention einsum conversions  * RMSNorm in type of x  * Fixing douplicate laurel norm/gating  * KV sharing using the right previous indices  * Refactor kv shared index computation. Correct frac_shared_layers  * Use num_shared_layers instead of inferring from a fraction  * fixing a bug for logging  * Fix shared data_ptrs in altup inits  * rope: adjust proj -> norm -> rope to preserve computation (#20)  * rope: adjust proj -> norm -> rope to preserve computation  * Removing some breaking language model fluff in ConditionalGeneration  * Consolidate query_states transforms  ---------  Co-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com> Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Vectorize the loops in AltUp (#19)  * Vectorize the loops in AltUp  * fix typo  * Expanding to support batched inputs  * remove extra debug script  * Fix AltUp.forward  ---------  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Add 'scale_shift=0.0, with_scale=True' to the final norm in TextModel  * Convert norm to 1/sqrt (#21)  * Convert norm to 1/sqrt  * Scale shift change per Phil's rec  * Adding default activation sparsity  * Fixing 2B config in weights conversion script  * Fixing RMSNorm parameters - adding scale_shift and with_scale  * Correcting query pre-attention scaling  * Adding query_rescale_scalar to text config  * Adding layer_idx to MLP  * Permafix for input_layernorm  * Use 1/sqrt instead of rsqrt in DecoderLayer  * Fix o_proj conversion  * Conversion script update for vision encoder  * Removing logging for debugging timm model  * Fixing bugs in Gemma3nForConditionalGeneration for text generation  * Generating the modeling_gemma3n.py file  * Removing the addition of an erroneous line in the modeling file  * Adding gemma3n text model to modeling_auto  * Bugfix: Updating the interleaving of inputs_embeds and vision_embeds  * Updating the modeling file with the latest bugfix changes  * Updating models/auto for Gemma 3n  * using AutoTokenizer in forward test  * Adding processing_gemma3n.py  * Gemma 3n configured for AutoModel. Conversion script updated.  * Removing errant merge artifacts  ---------  Co-authored-by: Mayank Chaturvedi <imayank@google.com> Co-authored-by: Douglas Reid <douglas-reid@users.noreply.github.com> Co-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com> Co-authored-by: Xuan-Son Nguyen <thichthat@gmail.com> Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>  * Removing errant debugging statements from Gemma 3  * Gemma3n audio model (#18)  * testing utilities for numerics comparisons  * Implement CumulativeGroupNorm and add to SubSampleConvProjection and SSCPConvBlock  * Add audio version of forward script based on RyanMullins' implementation  * Updating to match encoder tests. WIP: config question needs resolving  * Updates to audio classes to enable end-to-end running  * Removing vestigial classes, cleaning up print statements  * Adding SiLU / Swish to audio conformer feed forward block  * Shifted Gemma3p5Audio naming prefix to Gemma3NanoAudio  * Adding outputs to audio test  * Fixes to padding in SSCP and 1D convolution, align RMS Norm with wider model  * Update forward test to load from local weights  * Update conversion to process / output audio layers  * Update __all__ to export audio encoder  * AutoModel registration for Gemma 3n Audio  * Use AutoModel for ConditionalGeneration.audio_tower  * Fixing input_proj_linear transpose  * Fixing Gemma3NanoAudioConformerAttention.post conversion  * Fixing Gemma3NanoAudioSSCPConvBlock.conv weights conversion  * Correcting indentation issue on Gemma3p5RMSNorm  ---------  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Text + Vision Part 2 (#23)  * Updates for ConditionalGeneration.get_image_features  * Adding a WIP draft of image_processing_gemma3p5.py  * Update src/transformers/models/gemma3p5/modular_gemma3p5.py  Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>  * Modular conversion after github suggested change  * Text + image gives good results  * Fixing image size preset  * Updating configs for the 2B variant in the conversion script  * Using final generation config in conversion script  ---------  Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com> Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>  * Audio Integration (#12)  * initial commit of Gemma 3n scaffold  * Fixing param pass through on Gemm3nRMSNorm  * Adds Einsum layer to Gemma 3n  * Updating EinsumLayer API  * Undoing erroneous force push  * Reverting RMSNorm to with_scale by default  * Adds LAuReL to Gemma 3n  * Adds AltUp to Gemma 3n  * Adding Gemma 3n overall and text config with vision and audio config placeholders (#3)  * Adding Gemma 3n text configs  * Adding audio config placeholders  * Adding a placeholder for vision configs  * Updating MobileNetVisionConfig, inheriting TimmWrapperConfig  * Updating text configs  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Removing altup configs to accept the suggested configs  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Updating altup config  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Addressing review comments and updating text configs  * Adding a config for activation sparsity  * Updating configs to pass through options to super class init and adjust some name prefixes  * Updating laurel and altup with corrected config values  * Normalizing sub_config initializers  ---------  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Updating MLP with activation sparsity (#2)  * Updating DecoderBlock for Gemma 3n (#3)  * Initial Gemma3nTextModel (#4)  NOTE: This implementation WILL CHANGE in the coming weeks, however, changes will be strictly additive and this will remain a suitable baseline for downstream implementations to reference.  * Adding KV Cache Sharing  * Adds Einsum layer to Gemma 3n  * Updating EinsumLayer API  * Refactored kv cache sharing in attention  * Adding KVStore for cache sharing  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update modular  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Update src/transformers/cache_utils.py  Co-authored-by: Ryan Mullins <ryanmullins@google.com>  * Undoing erroneous force push  * Reverting RMSNorm to with_scale by default  * Adds LAuReL to Gemma 3n  * Updating KV Cache Sharing implementation  * Updating the q and k norm definitions in the attention module  * Fixing name error for q,k,v RMS norm to use the right 3n module  * Updating MLP with activation sparsity  * Updating DecoderBlock for Gemma 3n  * Updating kv cache sharing implementation with the use of a cache buffer and refactoring some lines of code  * Isolating KV Cache logic to relevant components  * Fixing logic error in Gemma3nAttention.forward  * Refactoring caching contributions and fixing kv_store initialization  * Simplifying Configs  * Remove errant self from super init call  * Bug fix in the Attention module - changing self.head_dim to config.head_dim  * Bug fixes in the LaurelBlock and RMS Norm super init call  * removing redundant code from a merge  * Adding per_layer_inputs to TextModel  * Adding preprocess embeddings with altup  * Adds per-layer-to-single output and a host of TODOs  * Integrating altup predict with the model workflow and other minor bug fixes  * Using nn.Embedding temporarily for text model  * It goes forward  * Minor refactor of attention sparsity and RoPE initialization  * Fixing duplicate rope_scaling param bug when loading from pretrained  ---------  Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com> Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>  * Normalizing on altup_num_inputs config option  * Adding audio encoder config  * Adds high-level components for Audio Encoder  * Implement uniform reducer for Audio Encoder  * Adding placeholders for Conformer components in Audio Encoder  * Adding placeholders for SubSampleConvProjection components in Audio Encoder  * Adding SequenceLayer component placeholders  * Implementing Gemma3nAudioEncoder with nn.Sequential  * Implementing Gemma3nAudioSubSampleConvProjection with nn.Sequential  * Implementing Conformer model with SequenceLayers  * Use OrderedDict in nn.Sequential initializers  * Implements sl.Residual in Torch with nn.Sequential and OrderedDict  * Adopting a base SequenceLayer class with default forward() method  * Implementing sl.GatedLinearUnit in Torch  * Implementing sl.Swish in Torch  * Implementing sl.ReLU in Torch  * Implementing sl.Scale in Torch  * Removing sl.Dropout after tree-shaking  * Implementing sl.RMSNorm in Torch with fake shape  * Implementing sl.GroupNorm in Torch  * Implementing sl.Conv2d in Torch  * Implementing sl.Dense in Torch  * Removing sl.Delay layers, which act as pass-throughs  * Connecting shapes to configs in initializers  * Removing sl.Emit  * Implementing sl.ExpandDims in Torch  * Adding sl.GradientClipping to Torch  * Implementing sl.DenseShaped in Torch  * Implementing sl.LDPA in Torch  * Removing unused sl.CombinedQKVProj class  * Fixing erroneous type hint  * Implemnenting sl.DepthwiseConv1D in Torch  * Implementing sl.MaskInvalid in Torch  * Fixes for initialization  * Fixes for saving weights  * Removing einsums per feedback from HF staff  * Removing Sequence Layers idioms from audio encoder  * Fixes for reviewer comments  * Converting sl.Frontend to FeatureExtractor  * Updates for ConditionalGeneration.get_image_features  * Adding a WIP draft of image_processing_gemma3n.py  * Update modular  Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>  * Modular conversion after github suggested change  * Text + image gives good results  * Fixing image size preset  * Draft of audio data in chat template  * Removing image processing. Using SigLIP instead.  * Audio input going end-to-end  * Fixing dtype issues in audio encoder  * x-lib formatting consistency  * Adding example data  * Save preprocessor_config.json from conversion script  * Instrumentaiton for debugging  * Additional instrumentation for preprocessing debugging  * Updates to preprocessor, padding; produces correct end-to-end results on sample  * Tackling configuraiton TODOs  * Start of feature extractor refatcor  * Adds Numpy version of USM extractor, removes Torch version and dependencies  * Fixing AltUp.correct coef permute  * Supporting batches of single audio segment inputs  * Docstrings updates for config  * In-lining audio feature extraction  * Adjustments to conversion script and smoke test script  ---------  Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com> Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com> Co-authored-by: pculliton <phillipculliton@gmail.com>  * Gemma 3n renaming  * Removing test data and utilities  * Renaming test files  * Gemma 3n refactor  * Fix tokenizer config in conversion script  * Address reviewer feedback  * FeatureExtractor returns float32 by default  * Adding basic tests for audio, and input name for audio encoder  * Audio integration test, updates to model_id for other integration tests  * Use scales for q and k norms (#26)  * Update audio integration test to use HF dataset  * Reviewer feedback  * Expand embedding table to full vocab size in weights conversion  * Mix-n-match MatFormers for Gemma 3n (#25)  * Remove in-place operations (#30)  * chore: removing inplace ops  * remove [tensor] * n pattern  * chore: reviewer feedback in AudioEncoder and AltUp  * More grad clipping  * Dynamo compatibility  * fix: cache slicing error  * chore: simplify shared kv cache slicing  * chore: vision encoder rename in timm  * fix: image processor do_normalize=False  * fixup: style  * chore: model_doc  * fix: docs for code quality  * chore: repo consistency  * fix: RMSNorm in float as in prior Gemmas  * fix: per_layer_inputs = None  * chore: Gemma3nForCausalLM from Gemma3nForConditionalGeneration checkpoint  * chore: repo consistency  * Add initial unit tests for Gemma3nAudioFeatureExtractor (#27)  * Add initial unit tests for Gemma3nAudioFeatureExtractor  * Add basic unit tests for Gemma3nProcessor (#28)  Co-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>  * parameterize tests  ---------  Co-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>  * chore: code style  * fix: test cases  * style and consistency  * fix config in the test to be coherent with layer cache sharing  * fix hidden states in tests and code  * inits and mappings  * fix modality prefixes  * test order and prefixes  * fix test exception  * fix class order and reduce model size for faster tests  * restore _checkpoint_conversion_mapping to load Caual from Conditional  * fix config mapping!  * fix: reviewer feedback  ---------  Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com> Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com> Co-authored-by: raushan <raushan@huggingface.co> Co-authored-by: Mayank Chaturvedi <imayank@google.com> Co-authored-by: Douglas Reid <douglas-reid@users.noreply.github.com> Co-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com> Co-authored-by: Xuan-Son Nguyen <thichthat@gmail.com> Co-authored-by: pculliton <phillipculliton@gmail.com> Co-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com> Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>  * fix import test  * add model args  * auto_docstring  * replace test path  * consistency  * skip tests for now  * fix docstring for doc builder  * skip unused attr  ---------  Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com> Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com> Co-authored-by: raushan <raushan@huggingface.co> Co-authored-by: Mayank Chaturvedi <imayank@google.com> Co-authored-by: Douglas Reid <douglas-reid@users.noreply.github.com> Co-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com> Co-authored-by: Xuan-Son Nguyen <thichthat@gmail.com> Co-authored-by: pculliton <phillipculliton@gmail.com> Co-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com> Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com> Co-authored-by: Arthur <arthur.zucker@gmail.com>",8723,0,8723
huggingface/transformers,3e5cc1285503bbdb6a0a3e173b5ae90566862215,Joao Gante,2025-06-26T15:25:00Z,"[tests] remove tests from libraries with deprecated support (flax, tensorflow_text, ...) (#39051)  * rm tf/flax tests  * more flax deletions  * revert fixture change  * reverted test that should not be deleted; rm tf/flax test  * revert  * fix a few add-model-like tests  * fix add-model-like checkpoint source  * a few more  * test_get_model_files_only_pt fix  * fix test_retrieve_info_for_model_with_xxx  * fix test_retrieve_model_classes  * relative paths are the devil  * add todo",156,691,847
huggingface/transformers,cfff7ca9a27280338c6a57dfa7722dcf44f51a87,eustlb,2025-06-26T14:33:31Z,[Whisper] Pipeline: handle long form generation (#35750)  * handle long form generation  * add warning  * correct incorrect in place token change  * update test to catch edge case  * make style  * update warning  * add doc,64,17,81
huggingface/transformers,02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254,eustlb,2025-06-26T13:55:28Z,add _keep_in_fp32_modules_strict (#39058)  * add _keep_in_fp32_modules_strict  * complete test,111,17,128
huggingface/transformers,d973e62fdd86d64259f87debc46bbcbf6c7e5de2,vb,2025-06-26T12:52:57Z,fix condition where torch_dtype auto collides with model_kwargs. (#39054)  * fix condition where torch_dtype auto collides with model_kwargs.  * update tests  * update comment  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,19,11,30
huggingface/transformers,44b231671db25974cfebcdae34402ad5099bf37a,Raushan Turganbay,2025-06-26T12:06:52Z,[qwen2-vl] fix vision attention scaling (#39043)  scale lost its `-` when refactoring,4,6,10
huggingface/transformers,ae15715df138949328d18e1dd95fd9cb4efb8e09,emmmm,2025-06-26T11:56:31Z,polishing docs: error fixes for clarity (#39042)  * fix duplicate deprecate_models.py  * fix duplicate modular_model_converter.py,2,2,4
huggingface/transformers,3abeaba7e53512ef9c1314163dd7e462ab405ce6,Manuel de Prada Corral,2025-06-26T11:54:36Z,Create test for #38916 (custom generate from local dir with imports) (#39015)  * create test for #38916 (custom generate from local dir with imports),22,0,22
huggingface/transformers,25c44d4b68d4a0feafb3a5a3fc640d04cf59d5a9,Rémi Ouazan,2025-06-26T11:44:59Z,Internvl fix (#38946)  * Image processor compile fix (#38540)  * Added a compile-friendly versiom of resize to BaseImgProcessorFast  * Changed qwen2 processor to use its parent class .resize  * Style  * underlined issue only happens on AMD w/ comment and bool check  * Fixed some utils functions  * Fixed the same issue for bridgetower  * Fixed the same issue for llava_next  * Repo consistency for llava onevision  * Update src/transformers/image_processing_utils_fast.py  Co-authored-by: Mohit Sharma <mohit21sharma.ms@gmail.com>  ---------  Co-authored-by: Mohit Sharma <mohit21sharma.ms@gmail.com>  * Added an Expectation to an internvl test  * Made qwen2_vl use the resize method of its parent clas  * Changed to torch.where  ---------  Co-authored-by: Mohit Sharma <mohit21sharma.ms@gmail.com>,53,8,61
huggingface/transformers,f85b47d1b8820fefc8fbe2704a2fd67e908f9614,Anton Vlasjuk,2025-06-26T11:06:09Z,[`Generate`] Fix no grad on some models (#39008)  fixes on torch no grad for generate,5,0,5
huggingface/transformers,583db52bc6d5415a205724776136d094ff70c9a4,Jaeyong Sung,2025-06-26T11:04:23Z,"Add Dia model (#38405)  * add dia model  * add tokenizer files  * cleanup some stuff  * brut copy paste code  * rough cleanup of the modeling code  * nuke some stuff  * more nuking  * more cleanups  * updates  * add mulitLayerEmbedding vectorization  * nits  * more modeling simplifications  * updates  * update rope  * update rope  * just fixup  * update configuration files  * more cleanup!  * default config values  * update  * forgotten comma  * another comma!  * update, more cleanups  * just more nits  * more config cleanups  * time for the encoder  * fix  * sa=mall nit  * nits  * n  * refacto a bit  * cleanup  * update cv scipt  * fix last issues  * fix last nits  * styling  * small fixes  * just run 1 generation  * fixes  * nits  * fix conversion  * fix  * more fixes  * full generate  * ouf!  * fixes!  * updates  * fix  * fix cvrt  * fixup  * nits  * delete wrong test  * update  * update  * test tokenization  * let's start changing things bit by bit - fix encoder step  * removing custom generation, moving to GenerationMixin  * add encoder decoder attention masks for generation  * mask changes, correctness checked against ad29837 in dia repo  * refactor a bit already --> next cache  * too important not to push :)  * minimal cleanup + more todos  * make main overwrite modeling utils  * add cfg filter & eos filter  * add eos countdown & delay pattern  * update eos countdown  * add max step eos countdown  * fix tests  * fix some things  * fix generation with testing  * move cfg & eos stuff to logits processor  * make RepetitionPenaltyLogitsProcessor flexible  - can accept 3D scores like (batch_size, channel, vocab)  * fix input_ids concatenation dimension in GenerationMixin for flexibility  * Add DiaHangoverLogitsProcessor and DiaExponentialDecayLengthPenalty classes; refactor logits processing in DiaForConditionalGeneration to utilize new configurations and improve flexibility.  * Add stopping criteria  * refactor  * move delay pattern from processor to modeling like musicgen.  - add docs - change eos countdown to eos delay pattern  * fix processor & fix tests  * refactor types  * refactor imports  * format code  * fix docstring to pass ci  * add docstring to DiaConfig & add DiaModel to test  * fix docstring  * add docstring  * fix some bugs  * check  * porting / merging results from other branch - IMPORTANT: it very likely breaks generation, the goal is to have a proper forward path first  * experimental testing of left padding for first channel  * whoops  * Fix merge to make generation work  * fix cfg filter  * add position ids  * add todos, break things  * revert changes to generation --> we will force 2d but go 3d on custom stuff  * refactor a lot, change prepare decoder ids to work with left padding (needs testing), add todos  * some first fixes to get to 10. in generation  * some more generation fixes / adjustment  * style + rope fixes  * move cfg out, simplify a few things, more todos  * nit  * start working on custom logit processors  * nit  * quick fixes  * cfg top k  * more refactor of logits processing, needs a decision if gen config gets the new attributes or if we move it to config or similar  * lets keep changes to core code minimal, only eos scaling is questionable atm  * simpler eos delay logits processor  * that was for debugging :D  * proof of concept rope  * small fix on device mismatch  * cfg fixes + delay logits max len  * transformers rope  * modular dia  * more cleanup  * keep modeling consistently 3D, generate handles 2D internally  * decoder starts with bos if nothing  * post processing prototype  * style  * lol  * force sample / greedy + fixes on padding  * style  * fixup tokenization  * nits  * revert  * start working on dia tests  * fix a lot of tests  * more test fixes  * nit  * more test fixes + some features to simplify code more  * more cleanup  * forgot that one  * autodocs  * small consistency fixes  * fix regression  * small fixes  * dia feature extraction  * docs  * wip processor  * fix processor order  * processing goes brrr  * transpose before  * small fix  * fix major bug but needs now a closer look into the custom processors esp cfg  * small thing on logits  * nits  * simplify indices and shifts  * add simpler version of padding tests back (temporarily)  * add logit processor tests  * starting tests on processor  * fix mask application during generation  * some fixes on the weights conversion  * style + fixup logits order  * simplify conversion  * nit  * remove padding tests  * nits on modeling  * hmm  * fix tests  * trigger  * probably gonna be reverted, just a quick design around audio tokenizer  * fixup typing  * post merge + more typing  * initial design for audio tokenizer  * more design changes  * nit  * more processor tests and style related things  * add to init  * protect import  * not sure why tbh  * add another protect  * more fixes  * wow  * it aint stopping :D  * another missed type issue  * ...  * change design around audio tokenizer to prioritize init and go for auto - in regards to the review  * change to new causal mask function + docstrings  * change ternary  * docs  * remove todo, i dont think its essential tbh  * remove pipeline as current pipelines do not fit in the current scheme, same as csm  * closer to wrapping up the processor  * text to audio, just for demo purposes (will likely be reverted)  * check if it's this  * save audio function  * ensure no grad  * fixes on prefixed audio, hop length is used via preprocess dac, device fixes  * integration tests (tested locally on a100) + some processor utils / fixes  * style  * nits  * another round of smaller things  * docs + some fixes (generate one might be big)  * msytery solved  * small fix on conversion  * add abstract audio tokenizer, change init check to abstract class  * nits  * update docs + fix some processing :D  * change inheritance scheme for audio tokenizer  * delete dead / unnecessary code in copied generate loop  * last nits on new pipeline behavior (+ todo on tests) + style  * trigger  ---------  Co-authored-by: Arthur Zucker <arthur.zucker@gmail.com> Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com> Co-authored-by: Vasqu <antonprogamer@gmail.com>",5732,28,5760
huggingface/transformers,5995cfa0a07de86e3c53fe1f57378c956a5d03db,Alex Brooks,2025-06-26T07:45:57Z,Fix Bad Outputs in Fast Path for GraniteMoeHybrid (#39033)  Fix bug in previous state setting,6,3,9
huggingface/transformers,22b0a898787f9e34c2b9b4ac1e53d2497c44ff39,Avihu Dekel,2025-06-26T07:44:17Z,"Granite speech speedup + model saving bugfix (#39028)  * ensure the query is updated during training  avoid unused parameters that DDP does not like  * avoid a crash when `kwargs` contain `padding=True`  trainers often pass this argument automatically  * minor  * Remove mel_spec lazy init, and rename to mel_filters. this ensures save_pretrained will not crash when saving the processor during training https://github.com/huggingface/transformers/blob/d5d007a1a0f0c11a726a54c8f00bd71825f84d02/src/transformers/feature_extraction_utils.py#L595  * minor - most feature extractors has a `sampling_rate` property  * speedup relative position embeddings  * fix several issues in model saving/loading: - avoid modifying `self._hf_peft_config_loaded` when saving - adapter_config automatically points to the original base model - a finetuned version should point to the model save dir. - fixing model weights names, that are changed by adding an adapter.  * minor  * minor  * minor  * fixing a crash without peft active  * add todo to replace einsum",30,9,39
huggingface/transformers,1d45d90e5d1552eccb6d8cc9b7bba283ccefb808,Joao Gante,2025-06-25T17:29:10Z,[tests] remove TF tests (uses of `require_tf`) (#38944)  * remove uses of require_tf  * remove redundant import guards  * this class has no tests  * nits  * del tf rng comment,21,2504,2525
huggingface/transformers,d37f7517972f67e3f2194c000ed0f87f064e5099,Matt,2025-06-25T16:31:26Z,Two ReDOS fixes (#39013)  * two_redos_fixes  * Fix two redos issues  * Just don't use RE at all,7,8,15
huggingface/transformers,551e48f182673cacd8ae91d839dd6962558d7b9e,eustlb,2025-06-25T16:09:00Z,[Kyutai-STT] correct model type + model id (#39035)  * correct model type + model id  * udpate doc  * init fix  * style !!!,29,23,52
huggingface/transformers,dad0e87c79d338f41176166b2e1e0591a87a81a1,Anton Lozhkov,2025-06-25T15:12:15Z,Add SmolLM3 (#38755)  * init smollm3  * integration tests  * config quirks  * docs stub  * rests round 2  * tests round 3  * tests round 4  * bring SWA back  * config checker pls  * final checkpoint  * style and copies  * Update src/transformers/models/smollm3/modular_smollm3.py  Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>  * Update src/transformers/models/smollm3/modular_smollm3.py  Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>  ---------  Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>,1879,0,1879
huggingface/transformers,3233e9b7c3745705c7047a823a19a6ac889239aa,Enno Hermann,2025-06-25T15:07:52Z,refactor: remove custom BarkLayerNorm (#39003)  `nn.LayerNorm` supports `bias=False` since Pytorch 2.1,5,18,23
huggingface/transformers,3c1d4dfbac964dfc98c83cb30835e9058edecd63,Marcel Ambo Ndowah,2025-06-25T14:55:22Z,Fix grammatical error in models documentation (#39019),1,1,2
huggingface/transformers,858f9b71a8bc39b8ba64f9ca88194b195215aae9,Quentin Lhoest,2025-06-25T14:31:20Z,"Remove script datasets in tests (#38940)  * remove trust_remote_code  * again  * Revert ""Skip some tests for now (#38931)""  This reverts commit 31d30b72245aacfdf70249165964b53790d9c4d8.  * again  * style  * again  * again  * style  * fix integration test  * fix tests  * style  * fix  * fix  * fix the last ones  * style  * last one  * fix last  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",154,293,447
huggingface/transformers,3c322c9cdf7d950ae54e0fa737de8435967aa01c,Marc Sun,2025-06-25T14:28:44Z,fix gemma3 grad acc (#37208)  * fix gemma3 grad acc  * fix  * fix  * fix  * fix  * rmv print  * rm  * Update setup.py  * Apply style fixes  * propagate the changes  ---------  Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com> Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com> Co-authored-by: Arthur <arthur.zucker@gmail.com>,13,8,21
huggingface/transformers,860b898d038f55c866d7ae07ba69bba69aa346de,Umar Butler,2025-06-25T14:11:18Z,fix: astronomical loss with ModernBERT when using gradient checkpointing (#38982) (#38983)  * fix: astronomical loss with ModernBERT when using gradient checkpointing  * update the modling fix  ---------  Co-authored-by: Arthur <arthur.zucker@gmail.com>,2,2,4
huggingface/transformers,a2eb75c891f6866cc9aeb66896be59f6c4ce100e,EduardDurech,2025-06-25T12:39:27Z,"Support for Flash Attention 3 (#38972)  * Support `flash_attn_3` Implements fwd and tests for Flash Attention 3 https://github.com/Dao-AILab/flash-attention/commits/main/hopper  - Includes checks for dropout>0 and ALiBi in `modeling_utils.PreTrainedModel._check_and_enable_flash_attn_3` (Dropout will likely be supported soon, so this will need to be updated and `modeling_flash_attention_utils._flash_attention_forward` at the `if _IS_FLASH_ATTN_3_AVAILABLE: ...`  An example Llama implementation is included in `modeling_llama.py` but other models would still need to be updated  Based on https://github.com/huggingface/transformers/pull/36190 which has model implementations and examples which could be merged  * Add tests for Flash Attention 2 and 3 parity  * ci fix  * FA2 compatibiity - `_prepare_flash_attention_from_position_ids` ->`prepare_fa2_from_position_ids` - Remove bettertransformer check in Flash Attention 3 - Merge tests - Add licensing  * ci fix  * Test naming consistency  * ci fix  * Deprecation warning for `prepare_fa2_from_position_ids`  * ci fix",697,261,958
huggingface/transformers,de98fb25a3772b8fc4a31e55cb0b0560d97353af,Yuan Wu,2025-06-25T10:40:01Z,Fix the seamless_m4t cannot work on Gaudi (#38363)  * Fix the seamless_m4t cannot work on Gaudi  Signed-off-by: yuanwu <yuan.wu@intel.com>  * Refine the patch  Signed-off-by: yuanwu <yuan.wu@intel.com>  * Fix seamless_m4t_v2 crash  Signed-off-by: yuanwu <yuan.wu@intel.com>  * Use the patched_gather  Signed-off-by: yuanwu <yuan.wu@intel.com>  * Remove debug logs  Signed-off-by: yuanwu <yuan.wu@intel.com>  * Remove useless modifications  Signed-off-by: yuanwu <yuan.wu@intel.com>  * Add hpu check  Signed-off-by: yuanwu <yuan.wu@intel.com>  * Add comments  Signed-off-by: yuanwu <yuan.wu@intel.com>  ---------  Signed-off-by: yuanwu <yuan.wu@intel.com> Co-authored-by: Ilyas Moutawwakil <57442720+IlyasMoutawwakil@users.noreply.github.com>,22,0,22
huggingface/transformers,7503cb911356abce1fc3b614193bd4384fee89cc,redmoe-moutain,2025-06-25T09:38:25Z,[Model] add dots1 (#38143)  * add dots1  * address comments  * fix  * add link to dots1 doc  * format  ---------  Co-authored-by: taishan <rgtjf1@163.com>,1239,0,1239
huggingface/transformers,3ef889690649c082849c667be17b757c32955229,Biao Zhang,2025-06-25T09:05:10Z,Encoder-Decoder Gemma (#38332)  * Initial submit  * Fix bugs: 1. add __init__ file 2. tied word embedding 3. support flash/flex attention 4. model saving and loading  * Code refactor: * Rename encdecgemma to t5gemma. * Split attention into self- and cross-attention * Split stack into encoder and decoder * Add test cases * Add auto configuration  * Update configurations.  * Fix bugs related to copy and attribute checks  * Fix type union  * Fix merge errors  * run ruff format  * Run make style and update tests.  * Add t5gemma model doc.  * ruff and style formatting.  * Add missed module config.  * Add dummy checkpoint link to pass tests (need updated when real checkpoints are uplioaded.).  * Update model doc.  * Minor updates following Arthur's comments: * replace docstrings with auto_docstrings * remove checkpoint layers * remove deprecate_kwargs  * fix rebase errors  * Fix docstring issues.  * fix t5gemma doc issue.  * run ruff format  * Updates: * split encoder-only model out * make t5gemmamodel encoder-decoder only * update token and sequence classification * update tests,5148,0,5148
huggingface/transformers,af9870265e817e57541d90c1797cb68959eb7b1e,Yuxuan Zhang,2025-06-25T08:43:05Z,GLM-4.1V Model support (#38431)  * 20250508 Model Architecture  * Update modeling_glm4v.py  * Update modeling_glm4v.py  * Update modeling_glm4v.py  * update 1447  * 0526  * update  * format  * problem  * update  * update with only image embed diff  * Final  * upload  * update  * 1  * upload with ruff  * update  * update  * work  * 1  * 1  * update with new note  * 2  * Update convert_glm4v_mgt_weights_to_hf.py  * Update tokenization_auto.py  * update with new format  * remove rmsnrom  * draft with videos  * draft  * update  * update  * fix for review problem  * try to remove min_pixel  * update  * for test  * remove timestamps  * remove item  * update with remove  * change  * update 2200  * update  * Delete app.py  * format  * update  * Update test_video_processing_glm4v.py  * 1  * 2  * use new name  * Update test_video_processing_glm4v.py  * remove docs  * change  * update for image processors update  * 2108  * 2128  * Update modular_glm4v.py  * 1  * update some  * update  * rename  * 1  * remove tests output  * 2  * add configuration  * update  * Update test_video_processing_glm4v.py  * fix simple forward tests  * update with modular  * 1  * fix more tests  * fix generation test  * fix beam search and init  * modular changed  * fix beam search in case of single-image/video. Fails if multiple visuals per text  * update processor  * update test  * pass  * fix beam search  * update  * param correct  * Update convert_glm4v_mgt_weights_to_hf.py  * 1  * Update test_modeling_glm4v.py  * 4  * 2  * 2123 video process  * 2  * revert  * 1  * 2  * revert processing  * update preprocesor  * changed  * 1  * update  * update  * 6  * update  * update  * update  * Delete tmp.txt  * config  * Update video_processing_glm4v.py  * apply modular correctly  * move functions  * fix order  * update the longest_edge  * style  * simplify a lot  * fix random order of classes  * skip integration tests  * correctly fix the tests  * fix TP plan  ---------  Co-authored-by: raushan <raushan@huggingface.co> Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co> Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>,6848,1,6849
huggingface/transformers,7b3807387b5b24a98fc66101268972ac8e25d7ed,null-pointer-access,2025-06-25T08:29:00Z,Drop unnecessary tokens in GPT2Model generation (#39016)  Drop unnecessary tokens in GPT2Model generation.  Co-authored-by: Yi Pan <conlesspan@outlook.com>,6,4,10
huggingface/transformers,e212ff9e6aec58fc76086a1c6f5448b0c259dd18,Raushan Turganbay,2025-06-25T08:23:37Z,[video processor] support torchcodec and decrease cuda memory usage (#38880)  * don't move the whole video to GPU  * add torchcodec  * add tests  * make style  * instrucblip as well  * consistency  * Update src/transformers/utils/import_utils.py  Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>  * Update src/transformers/utils/import_utils.py  Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>  * Update src/transformers/video_utils.py  Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>  ---------  Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>,129,9,138
huggingface/transformers,11d0feacce679f6931f5d032c74cd8167abb0db7,NielsRogge,2025-06-25T08:00:13Z,[AutoModelForMaskGeneration] Remove duplicate code (#38622)  Remove duplicate code,0,0,0
huggingface/transformers,3ee72af6b6133be5280a1abcf2cb7b497555f537,efsotr,2025-06-25T07:58:34Z,Fix graph break in torch.compile when using FA2 with attention_mask=None and batch size > 1 (#37332)  * Fix graph break in torch.compile when using FA2 with attention_mask=None and batch size > 1  * fix code format  * add test; replace position_ids with query_states becasue position_ids.shape[0] is always 1  * add assert loss is not nan,43,2,45
huggingface/transformers,ae32f1ad1102fbce259382dec7dd86e39ee23337,ranzhejiang,2025-06-25T07:48:50Z,Add zero dim tensor check when using flash_attention (#38280)  * Add zero dim tensor check when using flash_attention  Signed-off-by: ranzhejiang <zhejiang.ran@intel.com>  * Add zero dim tensor check when using flash_attention  Signed-off-by: ranzhejiang <zhejiang.ran@intel.com>  ---------  Signed-off-by: ranzhejiang <zhejiang.ran@intel.com>,7,0,7
huggingface/transformers,ca402e2116f5917ce0a03659b779a02a555b285f,StevenBucaille,2025-06-24T22:32:07Z,[LightGlue] Fixed attribute usage from descriptor_dim to keypoint_detector_descriptor_dim (#39021)  fix: fix descriptor dimension handling in LightGlue model,10,12,22
huggingface/transformers,48b6ef02380f993a6e8dfa0c355f722c2b7b96ed,Marcel Ambo Ndowah,2025-06-24T18:48:15Z,"Add Hugging Face authentication procedure for IDEs (PyCharm, VS Code,… (#38954)  * Add Hugging Face authentication procedure for IDEs (PyCharm, VS Code, etc.)  * Update quicktour.md  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",17,0,17
huggingface/transformers,ea9a30923e5ea4d4afb02c41b1ab34093af3a700,Dmitry,2025-06-24T18:24:50Z,[HPU][Critical Issue Fix] ThreadPool instead of Pool for parallel pre-processing (#39002)  * ThreadPool instead of Pool for parallel pre-processing  * ThreadPool only if hpu available,5,24,29
huggingface/transformers,995666edb5e9760e163567ee0dccba9a4394cbcd,ivarflakstad,2025-06-24T18:16:56Z,Skip sdpa dispatch on flash test due to unsupported head dims (#39010),4,0,4
huggingface/transformers,f367c6337db43015d41a893e4338c2dd2963bd8a,ivarflakstad,2025-06-24T18:13:36Z,Update self-comment-ci.yml user list (#39014)  add ivarflakstad to self-comment-ci.yml,1,1,2
huggingface/transformers,67d36dc1d727d887b0ec91cc8e296ef1d216a792,Tugsbayasgalan Manlaibaatar,2025-06-24T17:43:40Z,Fix bugs in DynamicCache (#37880)  * Fix bugs in DynamicCache  * Updarte  * Update  * Lint  * lint  * Rename test  * update  * update,97,1,98
huggingface/transformers,6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7,eustlb,2025-06-24T16:01:15Z,Add kyutai stt (#38909)  * first draft  * cleaner version  * udpate tests + modeling  * add tests  * init  * udpate test_modeling_common  * fix tests  * csm Processor draft  * convertion update  * mimi cache padding convolutions draft  * mimi streaming udpates  * update mimi padding cache test  * udpate cache padding mimi test  * make style mimi  * updates generate moshi asr  * moshi asr integration tests (single + batched)  * update tests  * update conversion script  * good default sliding window value  * udpdate generate  * update test checkpoint  * nit  * fix mimi  * fix codec prefix  * revert  * revert  * update config  * update config  * unnecessary mimi input restriction  * remove delay in tokens  * remove _prepare_4d_causal_attention_mask_with_cache_position and _update_causal_mask  * test update  * modular update  * make style  * nit  * rename  * create codec model generation config at init  * remove delay  * max_new_tokens/length warning  * correct conv1 padding cache import for modular  * nit  * fix on encoder_past_key_values  * convert modular  * move frame_size to config  * move frame_size to config  * update test name  * handle first token is bos  * better handling of max_new_tokens  * fix  * fix batch size in test input prep  * update docstring  * convert modular  * make style  * make style  * add feature extractor  * correct modular convention name for feature_extraction file  * update convertion script  * doc processor  * update doc  * udpate init  * update model type  * fixes  * update tests  * fix  * make  * add doc  * nit  * fix  * doc  * auto mappings  * doc  * nit  * convert modular  * doc  * nit  * extend _keep_in_fp32_modules to enforce fp32  * renaming to stt  * doc update + test update  * doc fixes  * doc fix  * doc fix  * fix musicgen tests  * fix musicgen tests  * make style  * fix musicgen tests  * correct frame_rate config param for mimi  * update mimi test  * revert update mimi test  * enforce cpu test  * move cache init in cache class  * convert modular  * docstring update  * update model id  * feature_extractor -> feature_extraction (SEW)  * convert modular  * update model id,3999,199,4198
huggingface/transformers,08bf7f1afee8c1127a28053cf452c44cf7e04d9c,Mohamed Mekkouri,2025-06-24T15:38:54Z,Add kernelize to transformers (#38205)  * fix  * fix  * fix flow  * remove non compiling path  * change  * style  * fix  * update  * update pin  * revert,13,43,56
huggingface/transformers,be10d4df60bec044ac0c1ab6fd326479874baafc,Avihu Dekel,2025-06-24T15:06:52Z,"Granite speech - minor fixes to support training with the HF trainer (#38833)  * ensure the query is updated during training  avoid unused parameters that DDP does not like  * avoid a crash when `kwargs` contain `padding=True`  trainers often pass this argument automatically  * minor  * Remove mel_spec lazy init, and rename to mel_filters. this ensures save_pretrained will not crash when saving the processor during training https://github.com/huggingface/transformers/blob/d5d007a1a0f0c11a726a54c8f00bd71825f84d02/src/transformers/feature_extraction_utils.py#L595  * minor - most feature extractors has a `sampling_rate` property",9,24,33
huggingface/transformers,e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5,Cyril Vallez,2025-06-24T15:04:33Z,Fix undeterministic order in modular dependencies (#39005)  * sort correctly  * Update modeling_minimax.py  * Update modular_model_converter.py,238,1602,1840
huggingface/transformers,bdf5fb70aa11782cce22027d76879f71f4e41c1e,7mile,2025-06-24T14:33:48Z,Skip non-selected experts for qwen3_moe (#38133)  * fix(qwen3moe): skip experts with no workload  * avoid tolist and also update other moe models  * fix: should squeeze 0-dim only,12,10,22
huggingface/transformers,719058c6255aa877eabd4e0e1fb69460a1680e30,Tanuj Rai,2025-06-24T14:21:36Z,Update attention_visualizer.py (#37860),1,1,2
huggingface/transformers,9f42c1f192cf2dcd9f05a2d8374e298aba1ef576,Mylon Jones,2025-06-24T13:24:02Z,Added scikit-learn to the example image-classification requirements.txt (#37506)  Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>,2,1,3
huggingface/transformers,1636a7bcb942370bb4098c8e67e4c3d3fd6a1740,Cyril Vallez,2025-06-24T13:23:52Z,Fixes for Arcee model (#39001)  * fix modular  * Update modular_arcee.py  * fix,32,99,131
huggingface/transformers,71de20b818c3aa9715fb3d0e26f448ec534b03d2,Crystalcareai,2025-06-24T13:05:29Z,"Add Arcee model support (#38621)  * Add Arcee model support to transformers  - Add ArceeConfig and model mappings for all task types (CausalLM, SequenceClassification, QuestionAnswering, TokenClassification) - Add auto-loading support through AutoModel, AutoConfig, and AutoTokenizer - Use LlamaTokenizer for tokenization - Add FX graph support for Arcee models - Create lazy loading module structure for Arcee  * feat: update YARN scaling and RoPE validation for Arcee model  * feat: add auto_docstring checkpoint config to Arcee model classes  * docs: add pre-trained model weights reference to Arcee configuration files  * refactor: move RoPE utilities to dedicated modeling_rope_utils module  * Add comprehensive test suite for Arcee model  - Add test_modeling_arcee.py following standard transformers test patterns - Include tests for all model variants (CausalLM, SequenceClassification, QuestionAnswering, TokenClassification) - Add specific test for ReLU² activation in ArceeMLP - Add RoPE scaling tests including YARN support - Follow CausalLMModelTest pattern used by similar models  * Add documentation for Arcee model  - Add comprehensive model documentation with usage examples - Include all model variants in autodoc - Add to table of contents in proper alphabetical order - Fixes documentation coverage for Arcee model classes  * Make style/fixup  * fix copyright year  * Sync modular conversion  * revert in legacy supported models in src/transformers/utils/fx  * cleaned redundant code in modular_arcee.py  * cleaned testing  * removed pretraining tp  * fix styles  * integration testing  ---------  Co-authored-by: Pranav <veldurthipranav@gmail.com> Co-authored-by: Pranav <56645758+pranav4501@users.noreply.github.com>",1605,0,1605
huggingface/transformers,23c89a67321ddd85a6e291ed30c421b0bb351b9e,Anton Vlasjuk,2025-06-24T12:42:10Z,[`Attention`] Small fix on output attentions (#38948)  small fix,1,1,2
huggingface/transformers,4f650040a68c915c4e9fa70c4a7a62714e471d65,Dianana,2025-06-24T12:24:56Z,Removing extra space in large command for speech-pretraining example (#38705)  Removing extra space in Large command,2,2,4
huggingface/transformers,d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd,Raushan Turganbay,2025-06-24T08:53:52Z,[qwen] refactor attentions for vision/audio (#38930)  * refactor attentions in vision/audio  * remove fa2 import  * make config the only args  * pass along kwargs from modality encoders  * style,409,846,1255
huggingface/transformers,2e4c045540c3bd1eed226babd20af3941f956c58,vb,2025-06-24T08:39:18Z,🔴 Update default `dtype` for pipelines to `auto` (#38882)  * check typing  * Fallback to fp32 if auto not supported.  * up.  * feedback from review.  * make style.,61,33,94
huggingface/transformers,21cb353b7b4f77c6f5f5c3341d660f86ff416d04,casinca,2025-06-23T19:33:10Z,"[docs] Typos - Single GPU efficient training features (#38964)  * Typos  - corrected bf16 training argument - corrected header for SDPA  * improved readability for SDPA suggested by @stevhliu  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",2,2,4
huggingface/transformers,f9be71b34d5efbba50bc787848c6e421822b14ba,Yih-Dar,2025-06-23T15:42:46Z,Fix `rag` (#38585)  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,61,18,79
huggingface/transformers,9eac19eb598f10f76b1f276a36a8c1f6690d200e,Yusuf Shihata,2025-06-23T15:31:32Z,[Feature] Support `is_split_into_words` in the `TokenClassificationPipeline`. (#38818)  * some fixes  * some fixes  * now the pipeline can take list of tokens as input and is_split_into_words argument  * now the pipeline can take list of tokens as input and is_split_into_words argument  * now the pipeline can take list of tokens as input and is_split_into_words argument and we can handle batches of tokenized input  * now the pipeline can take list of tokens as input and is_split_into_words argument and we can handle batches of tokenized input  * solving test problems  * some fixes  * some fixes  * modify tests  * aligning start and end correctly  * adding tests  * some formatting  * some formatting  * some fixes  * some fixes  * some fixes  * resolve conflicts  * removing unimportant lines  * removing unimportant lines  * generalize to other languages  * generalize to other languages  * generalize to other languages  * generalize to other languages,141,11,152
huggingface/transformers,2ce02b98bfe135c0a94ab169c5d95c88b91de3c3,Yih-Dar,2025-06-23T15:07:18Z,fix `mistral` and `mistral3` tests (#38978)  * fix  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,24,12,36
huggingface/transformers,b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9,Yoni Gozlan,2025-06-23T14:39:41Z,"Add support for auto_docstring with model outputs (#38242)  * experiment auto_docstring model outputs  * Fix PatchTSMixer  * Add check model output docstring to check_auto_docstring and fix all model outputs docstring  * add reordering of docstring in check_docstrings  * add check for redundant docstring in check_docstrings, remove redundant docstrings  * refactor check_auto_docstring  * make style  * fix copies  * remove commented code  * change List-> list Tuple-> tuple in docstrings  * fix modular  * make style  * Fix modular vipllava  ---------  Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",6972,8993,15965
huggingface/transformers,0c98f24889f4dd7ca9f35f16186b59a66add2654,kallewoof,2025-06-23T14:32:16Z,"fix: add __bool__ operator to tokenizer to avoid bloated asserts (#38899)  * fix: add __bool__ operator to tokenizer to avoid bloated asserts  When a user does 'assert tokenizer' to ensure that the tokenizer is not None, they inadvertently set off a rather expensive process in the '__len__()' operator. This fix adds a trivial '__bool__()' that returns True, so that a None tokenizer asserts and an actual tokenizer returns True when asserted, without calling length op.  * typo",6,0,6
huggingface/transformers,d29482cc9194afbe59f3203a9f5df89ab48a2ec9,Yoni Gozlan,2025-06-23T14:17:25Z,Add Idefics2/3 and SmolVLM Fast image processors + improvements for fast image processors (#38157)  * add working idefics2 fast and improvements for fast nested images processing  * add fast image processors idefics 3 and smolvlm  * cleanup tests  * fic doc idefics2  * PR review and fix issues after merge  * Force providing disable_grouping to group_images_by_shape  * simplify group_images_by_shape  * fix modular  * Fix nits after review,2025,427,2452
huggingface/transformers,1a96127e465b54048fe8ad5638bf0fc11ce94f39,Rémi Ouazan,2025-06-23T13:13:27Z,Break tie in Expectations and gemma3 fixes (#38943)  * Added major / minor version to Expectations ordering  * Added fixes to gemma3  * Style,25,4,29
huggingface/transformers,84d19be41e0131e6f2a660fe6af8b77094906af7,Pavel Iakubovskii,2025-06-23T12:24:48Z,"Apply GradientCheckpointingLayer to the whole repo (#38913)  * first batch (4)  * align  * altclip  * beit  * bert  * yolos  * dino, pvt_v2  * bark, bart, bert_generation  * big_bird, biogpt  * blnderbot, bloom  * bridgetower  * camambert, canine, chameleon  * chinese clip, clap, clip  * codegen, conditional detr, convbert  * dab_detr, data2vec  * dbrx, deberta  * deberta, decicion_tranformer, deformable_detr  * deit, deta, mctct  * detr, dinov2, distilbert  * donut, dpt, electra  * ernie, esm, falcon  * flava, fnet, falcon_mamba  * focalnet, git, gpt2  * gpt - bigcode, neo, neox  * gptj, groupvit  * idefics2, idefics3  * ijepa, imagegpt, internvl  * jetmoe, kosmos2, layoutlm  * layoutlm2-3, led  * lilt, longformer, longt5, luke  * m2m, mamba1-2  * marian, markuplm, mask2former  * maskformer  * mbart, megatron_bert, mimi  * mixtral, mlcd  * mobilevit1-2, modernbert  * moshi, mpt, mra  * mt5, musicgen  * mvp, nemotron  * nllb_moe  * nystromformer, omdet_turbo  * opt, owlvit, owlv2  * pegasus, pegasus_x, presimmon  * phimoe, pix2struct, pixtral  * plbart, pop2piano, prophetnet  * qwen2*  * qwen2, qwen3 moe,  rec gemma  * rembert  * roberta  * roberta prelayernorm  * roc_bert, roformer, rwkv  * sam, sam_hq  * seggpt, smolvlm, speech_to_text  * splinter, stablelm, swin  * swin2sr, switch_transformer, t5, table_transformer  * tapas, time_series_tranformer, timesformer  * trocr, tvp, umt5  * videomae, vilt, visual_bert  * vit, vit_mae, vit_msn  * vitpose_backbone, vits, vivit  * whisper. x_clip, xglm  * xlm_roberta, xmod  * yoso  * zamba  * vitdet, wav2vec2, wav2vec2_bert  * unispeech, wav2vec_conformer  * wavlm  * speecht5  * swinv2  * sew / _d  * seamless_mt4 / _v2  * deprecated models update  * bros  * gemma2, gemma3  * got, hiera, hubert, llama4, mllama, oneformer, phi, olmoe, informer  * fixup  * Add use_cache=False and past_key_value=None to  GradientCheckpointingLayer  * fixup  * fix prophetnet  * fix bigbird_pegasus  * fix blenderbot  * fix mbart  * fix mvp  * fix zamba2  * fix bart  * fix blenderbot_small  * fix codegen  * Update gradient checkpointing layer to support more past_key_values arg names  * fix data2vec vision  * fix deformable_detr  * fix gptj  * fix led  * fix m2m_100  * add comment  * fix nnlb_moe  * Fix pegasus_x  * fix plbart  * udop  * fix-copies: beit, wav2vec2  * fix gpt_bigcode  * fixup  * fix t5  * fix switch_transformers  * fix longt5  * fix mt5  * update tapas  * fix blip2  * update blip  * fix musicgen  * fix gpt2, trocr  * fix copies  * !!! Revert zamba, mllama  * update autoformer  * update bros  * update args / kwargs for BERT and copies  * 2nd round of updates  * update conditional detr  * Pass encoder_hidden_states as positional arg  * Update to pass encoder_decoder_position_bias as positional arg  * fixup  * biogpt modular  * modular gemma2  * modular gemma3  * modular gpt_neox  * modular informer  * modular internvl  * modular mixtral  * modular mlcd  * modular modernbert  * modular phi  * modular qwen2_5_omni  * modular qwen2_5_vl  * modular sam_hq  * modular sew  * wav2vec2_bert  * modular wav2vec2_conformer  * modular wavlm  * fixup  * Update by modular instructblipvideo  * modular data2vec_audio  * nit modular mistral  * apply modular minimax  * fix modular moonshine  * revert zamba2  * fix mask2former  * refactor idefics",2514,5281,7795
huggingface/transformers,07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8,Cyril Vallez,2025-06-23T11:44:50Z,Remove dead protected imports (#38980)  * remove them  * more,17,110,127
huggingface/transformers,74f5e4a1fad5a578e5ecb3b47fe48dc98ba21ff2,Cyril Vallez,2025-06-23T10:40:01Z,"[modular] CLI allows positional arguments, and more defaults names for the optional arg (#38979)  * More defaults  * Update modular_model_converter.py",21,9,30
huggingface/transformers,334bf913dca3f8f85312d264c0f7da5607384ced,Vensen,2025-06-23T09:50:51Z,"Fix(informer): Correct tensor shape for input_size=1 (#38856)  * Fix(time_series): Correct scaler tensor shape in base model  The create_network_inputs function in TimeSeriesTransformerModel handled the scaler's loc and scale tensors inconsistently. When input_size=1, the tensors were not squeezed, leading to downstream dimension errors for models like Informer.  This commit refactors the logic to unconditionally apply .squeeze(1), which correctly handles all input_size cases and fixes the bug at its source.  Fixes #38745  * Fix(time_series): Correct scaler tensor shape in base model  The create_network_inputs function in TimeSeriesTransformerModel handled the scaler's loc and scale tensors inconsistently. When input_size=1, the tensors were not squeezed, leading to downstream dimension errors for models like Informer.  This commit refactors the logic to unconditionally apply .squeeze(1), which correctly handles all input_size cases and fixes the bug at its source.  Fixes #38745  ---------  Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>",16,4,20
huggingface/transformers,c184550dafcc214fd10cddec98675a8c68a6440f,Benoqtr,2025-06-23T09:25:56Z,Fix DTensor import compatibility for PyTorch < 2.5 (#38836),3,2,5
huggingface/transformers,984ff89e7306ad33c46f76afc9aa78d40a8c01d8,Ilyas Moutawwakil,2025-06-23T08:56:51Z,Gaudi3 CI (#38790),618,14,632
huggingface/transformers,2166b6b4ff09f6dd3867ab982f262f66482aa968,DongKyu Kang,2025-06-20T20:46:19Z,Update blip model card (#38513)  * Update docs/source/en/model_doc/blip.md  * fix(docs/source/en/model_doc/blip.md): fix redundent typo error  * fix (docs/source/en/model_doc/blip.md): modify of review contents  * fix(docs/source/en/model_doc/blip.md): modify code block  * Update blip.md  ---------  Co-authored-by: devkade <mouseku@moana-master> Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>,58,17,75
huggingface/transformers,166e823f770477b17988020b2476a796d49836a6,Manuel de Prada Corral,2025-06-20T16:36:57Z,Fix custom generate from local directory (#38916)  Fix custom generate from local directory: 1. Create parent dirs before copying files (custom_generate dir) 2. Correctly copy relative imports to the submodule file. 3. Update docs.,11,2,13
huggingface/transformers,3d34b92116c26518f476be8c40250c4d89de3cc3,Yih-Dar,2025-06-20T16:10:35Z,Switch to use A10 progressively (#38936)  * try  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,73,144,217
huggingface/transformers,b8059e1f8f9ad245d71fbe2d18723d735ffccfec,Yih-Dar,2025-06-20T15:28:32Z,Fix more flaky `test_initialization` (#38932)  * try  * try  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,56,5,61
huggingface/transformers,5ee60f970a332b12458b113a18cc74999f8c5880,Cyril Vallez,2025-06-20T15:18:06Z,Correctly raise error for awq quantization (#38945)  fix warning,3,1,4
huggingface/transformers,8ac2d75353f4c3eca90f5aba0ab65e31e3b20b1b,Ákos Hadnagy,2025-06-20T12:17:21Z,Pin PyTorch extras for AMD containers (#38941)  * Pin additional Torch packages  * Remove unused def  ---------  Co-authored-by: ivarflakstad <69173633+ivarflakstad@users.noreply.github.com>,4,0,4
huggingface/transformers,9120567b02a551d198337e21bee8c1465f389ab2,Pavel Iakubovskii,2025-06-20T12:00:09Z,Add kwargs for timm.create_model in TimmWrapper (#38860)  * Add init kwargs for timm wrapper  * model_init_kwargs -> model_args  * add save-load test  * fixup,36,4,40
huggingface/transformers,ff95974bc67aa0a843d1045e4d2379352e334e2d,Raushan Turganbay,2025-06-20T11:49:29Z,[static cache] fix device map per layer in VLMs (#38488)  return lm as decoder,162,36,198
huggingface/transformers,aa42987c1e3a0cf1c18a5783274f0d8cc8409b53,Cyril Vallez,2025-06-20T10:06:48Z,Remove `ALL_LAYERNORM_LAYERS` (#38922)  * remove it everywhere  * Update trainer_pt_utils.py  * Update trainer_pt_utils.py  * style  * sort list in test  * CIs  * use recursion same way as before (for intermediate layer names),16,74,90
huggingface/transformers,38a9b707862dc017f111eb02a2bba61e35a74104,Yao Matrix,2025-06-20T09:42:44Z,add pytorch-xpu Dockerfile (#38875)  * first commit  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * use rls pytorch  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  ---------  Signed-off-by: YAO Matrix <matrix.yao@intel.com>,93,0,93
huggingface/transformers,9bcdd5cde9411477cba66bc9e6d1c59e80b60b60,Rémi Ouazan,2025-06-20T09:22:32Z,Modernbert fixes (#38912)  * Removed deprecated argument in modernbert RotaryEmbedding  * Skip test_sdpa_can_dispatch_on_flash for modernbert  ---------  Co-authored-by: ivarflakstad <69173633+ivarflakstad@users.noreply.github.com> Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>,6,2,8
huggingface/transformers,31d30b72245aacfdf70249165964b53790d9c4d8,Yih-Dar,2025-06-20T09:05:49Z,Skip some tests for now (#38931)  * try  * [test all]  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,17,0,17
huggingface/transformers,0725cd6953803b8aacfc85288cbfb83dea30c469,Cyril Vallez,2025-06-19T17:25:20Z,Remove deprecated classes in modeling_utils.py (#38919)  * remove deprecated classes  * style,0,452,452
huggingface/transformers,797860c68cfd8bd3ad38ce312540445073f76b30,Hamza Benchekroun,2025-06-19T15:54:08Z,"feat: add flexible Liger Kernel configuration to TrainingArguments (#38911)  * feat: add flexible Liger Kernel configuration to TrainingArguments  Add support for granular Liger Kernel configuration through a new `liger_kernel_config` parameter in TrainingArguments. This allows users to selectively enable/disable specific kernels (rope, swiglu, cross_entropy, etc.) instead of the current approach that rely on default configuration.  Features: - Add `liger_kernel_config` dict parameter to TrainingArguments - Support selective kernel application for all supported models - Maintain full backward compatibility with existing `use_liger_kernel` flag  Example usage: ```python TrainingArguments(     use_liger_kernel=True,     liger_kernel_config={         ""rope"": True,         ""swiglu"": True,         ""cross_entropy"": False,         ""fused_linear_cross_entropy"": True     } ) Closes #38905  * Address comments and update Liger section in Trainer docs",94,4,98
huggingface/transformers,89b35be618256d2a4a2458322a0653c57e8fa986,Matt,2025-06-19T14:22:59Z,"Allow make-fixup on main branch, albeit slowly (#38892)  * Allow make-fixup on main branch, albeit slowly  * Make the other style checks work correctly on main too  * More update  * More makefile update",27,11,38
huggingface/transformers,9a02e7602d01c98946d755c38b51930bf8b43901,Gabe Goodhart,2025-06-19T14:20:42Z,feat: Add granite architectures to auto tokenizer name mappings (#38802)  Branch: GraniteTokenizerMapping  Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>,4,0,4
huggingface/transformers,54a02160eb030da9be18231c77791f2eb3a52216,Matt,2025-06-19T13:53:52Z,Fix ReDOS in tokenizer digit substitution (#38844)  * Fix regexes vulnerable to ReDOS  * Let's just use regex  * Import regex/re correctly,14,7,21
huggingface/transformers,af6120b3eb2470b994c21421bb6eaa76576128b0,ivarflakstad,2025-06-19T13:11:01Z,Skip sdpa tests if submodule does not support sdpa (#38907),24,0,24
huggingface/transformers,5d26a387359d669d74f14effbdc859f907133647,Yih-Dar,2025-06-19T11:50:33Z,Fix `FalconMambaIntegrationTests` (#38566)  * update  * update  * update  * update  * update  * update  * update  * update  * update  * update  * update  * update  * update  * update  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,52,12,64
huggingface/transformers,a9ce8c69c946eae3431828871366bb4112d4ec1b,Yao Matrix,2025-06-19T11:48:23Z,align xpu's autocast behavior w/ cuda by using device agnostic torch APIs (#38284)  * siwtch to device agnostic autocast in nemotron to align xpu behavior w/ cuda  Signed-off-by: Matrix Yao <matrix.yao@intel.com>  * fix issue  Signed-off-by: Matrix Yao <matrix.yao@intel.com>  * fix style  Signed-off-by: Matrix Yao <matrix.yao@intel.com>  * use torch.cast as other modeling code for decision_transformer&gpt2&imagegpt  Signed-off-by: Matrix Yao <matrix.yao@intel.com>  * refine  Signed-off-by: Matrix Yao <matrix.yao@intel.com>  * update get_autocast_gpu_dtype to device agnostic one  Signed-off-by: Matrix YAO <matrix.yao@intel.com>  * fix style  Signed-off-by: Matrix YAO <matrix.yao@intel.com>  * fix comments  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * fix style  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  ---------  Signed-off-by: Matrix Yao <matrix.yao@intel.com> Signed-off-by: Matrix YAO <matrix.yao@intel.com> Signed-off-by: YAO Matrix <matrix.yao@intel.com> Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>,138,37,175
huggingface/transformers,0a53df1a77a9978ddf958e1f8f7a257181f180cb,Yuanyuan Chen,2025-06-19T11:45:51Z,Fix unnecessary super calls (#38897)  Signed-off-by: cyy <cyyever@outlook.com>,40,44,84
huggingface/transformers,b949747b54b6d81c5e4ab93c4d98ebc7a5901b31,Yih-Dar,2025-06-19T08:56:34Z,Fix `fsmt` tests (#38904)  * fix 1  * fix 2  * fix 3  * fix 4  * fix 5  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,15,2,17
huggingface/transformers,11738f85377df4072e96c5842369c000c3bd46c0,eustlb,2025-06-19T03:35:32Z,[phi-4] use mel filters from audio utils (#36966)  * use mel_filter_bank from audio utils  * Apply style fixes  ---------  Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com> Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>,10,74,84
huggingface/transformers,f7b21822e32fba8bd92a939db7f352d1623f09e4,Lucain,2025-06-19T03:06:25Z,Use `raise from e` in `hub.py` utility (#37241)  Use raise from e  Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>,1,1,2
huggingface/transformers,3756bf192c9b4beffda0270cee8df5164226e27c,Isaac Breen,2025-06-19T02:35:33Z,Add support for specifying revisions when pushing to Hub via internal Trainer call (#36852)  * Update training_args.py  * Update trainer.py  * fixes  * fix  * remove extraneous comments  * explicit revision arg  * add msg  * fixup  * fix field name  * rename field revision to hub_revision  * restore gradient_checkpointing doc  * fix ws  ---------  Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>,19,1,20
huggingface/transformers,458e0b376ce92aad3217610762802f0b5a704205,Dhruv,2025-06-18T23:01:25Z,"Update bamba model card (#38853)  * Update bamba model card  * Update the doc for bamba  * Update docs/source/en/model_doc/bamba.md  Bamba paragraph  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/bamba.md  Bamba collection url  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/bamba.md  Update Padding-Free Training to Notes heading  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/bamba.md  update examples  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/bamba.md  Update additional info  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/bamba.md  consistent casing  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/bamba.md  simplify sentences  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Include pipeline and cli examples + fix formatting  * Apply suggestions from code review  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/bamba.md  update cli id  * Update quantization example  * Fix auto code formatter changes  * Update cli command + include BambaModel  * Update docs/source/en/model_doc/bamba.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",97,54,151
huggingface/transformers,ea013348737fbd0efdefa38f9cad30443a810fd3,Raushan Turganbay,2025-06-18T20:39:56Z,"[video processor] fix slow tests (#38881)  * we need to check against mapping to be safe  * need to check only when inferring from image type, otherwise messes custom code  ---------  Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",6,1,7
huggingface/transformers,b922b22ec2e458978dbd89038ad4b47885b34195,Sam Rae,2025-06-18T17:33:29Z,"36978 | Fast image processor for DPT model (#37481)  * chore: ran codegen script  * test: test_image_processor_properties  * test: test_image_processor_from_dict_with_kwargs  * test: wip - test_padding  * test: test_padding  * test: test_keep_aspect_ratio  * wip  * test  * test: wip  * test: wip  * test: test_call_segmentation_maps, wip  * chore: tidy up  * test: test_call_segmentation_maps  * fix: test_save_load_fast_slow  * test: reduce labels  * chore: make fixup  * chore: rm comment  * chore: tidy  * chore remove comment  * refactor: no need to infer channel dimesnion  * refactor: encapsulate logic for preparing segmentation maps  * refactor: improve readability of segmentation_map preparation  * improvement: batched version of pad_image  * chore: fixup  * docs  * chore: make quality  * chore: remove unecessary comment  * fix: add SemanticSegmentationMixin  * feat: add post_process_depth_estimation to fast dpt image processor  * chore: fix formatting  * remove max_height, max_width  * fix: better way of processin segmentation maps - copied from Beit Fast processor  * chore: formatting + remove TODO  * chore: fixup styles  * chore: remove unecessary line break  * chore: core review suggestion to remove autodocstring  * fix: add do_reduce_labels logic + refactor - refactor preprocess logic to make it consistent with other processors - add missing reduce labels logic  * refactor: remove deprecated mixin  * chore: fixup  * use modular for dpt + final nit changes  * fix style  ---------  Co-authored-by: Samuel Rae <samuelrae@Samuels-Air.fritz.box> Co-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",1018,158,1176
huggingface/transformers,c27f628e98744f156e653d7eeab3afe018cd14b0,Ashu_kun,2025-06-18T16:38:58Z,Docs: Add custom fine-tuning tutorial to TrOCR model page (#38847)  * Update trocr.md  Docs: add community fine‑tuning notebook link to TrOCR page  * apply suggested changes from PR review  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/trocr.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>,1,0,1
huggingface/transformers,0a289d1630ae32e867211a98dd5fc6102e556a28,Keshav Singh,2025-06-18T16:26:46Z,"log: Add logging when using split_batches and per_device_train_batch_size (#38633)  * log: Add logging when user uses split_batches and per_device_train_batch_size  * refactor: remove whitespace from blank line  * Update src/transformers/training_args.py  Change logging level to info  Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>  ---------  Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",5,0,5
huggingface/transformers,c55d8063557d4cfa6a15df44bdb2f29c7de51247,Quyu Kong,2025-06-18T16:26:22Z,[bugfix] fix ATTN_MASK_NPU device mismatch error on multi-device NPU … (#38876)  [bugfix] fix ATTN_MASK_NPU device mismatch error on multi-device NPU setups,12,9,21
huggingface/transformers,9cd7570f34fdb833ed874b2eba4d4ea3ae9ccb03,Matt,2025-06-18T13:45:01Z,Fix loop var naming (#38885),4,4,8
huggingface/transformers,1fc67a25c6d6080467fae4f35efcefe5f13b7409,Yuanyuan Chen,2025-06-18T13:38:08Z,More PYUP fixes (#38883)  More pyup fixes  Signed-off-by: cyy <cyyever@outlook.com>,273,355,628
huggingface/transformers,12d4c5b66f7ff2765bacfeee214e7504671824bf,Wing Lian,2025-06-18T13:10:22Z,null deepspeed_plugin in args for wandb callback fake trainer (#38867),1,0,1
huggingface/transformers,3620b32cc853593abb629f0e680d79125e6ece4a,Stefan,2025-06-18T13:09:58Z,Fixed markdown for BertTokenizer's '[CLS]' token. (#38506),1,1,2
huggingface/transformers,cb0f60419231ebec83b9ced356bb47a74a4009ea,艾梦,2025-06-18T13:09:00Z,Fix HQQ model param device transfer issue (#38466)  * Fix HQQ model param device transfer issue  * modify a comment  * clear the code and add test for hqq device/dtype  * fix test hqq code quality of imports  ---------  Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>,85,4,89
huggingface/transformers,c77bcd889fa0cf3dc7ee3a21c99dc3c2871cc9ad,Yih-Dar,2025-06-18T12:36:03Z,Fix `qwen3_moe` tests (#38865)  * try 1  * try 2  * try 3  * try 4  * try 5  * try 6  * try 7  * try 8  * try 9  * try 10  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,44,65,109
huggingface/transformers,5a95ed5ca0826c867e35e52f698db4d8fc907bcb,Cyril Vallez,2025-06-18T07:46:22Z,"🚨🚨 Fix initialization of Mask2Former (#38864)  * Correctly fix init  Co-authored-by: BUI Van Tuan <buivantuan07@gmail.com>  * add back the block, breaking BC but this is correct author's code  * override the test for params needing it  ---------  Co-authored-by: BUI Van Tuan <buivantuan07@gmail.com>",68,32,100
huggingface/transformers,309e8c96f2065a02167be7142077ffc49e817c34,Yih-Dar,2025-06-18T07:39:17Z,Fix `phi4_multimodal` tests (#38816)  * fix  * fix  * fix  * fix  * fix  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,26,10,36
huggingface/transformers,3526e25d3d24d555377e444a2147e0f8ff318403,Yao Matrix,2025-06-18T07:20:49Z,enable misc test cases on XPU (#38852)  * enable misc test cases on XPU  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * fix style  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * tweak bamba ground truth on XPU  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * remove print  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * one more  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  * fix style  Signed-off-by: YAO Matrix <matrix.yao@intel.com>  ---------  Signed-off-by: YAO Matrix <matrix.yao@intel.com>,40,31,71
huggingface/transformers,d058f81e5bac1a52a7e9acfd30526f7bcbcae40a,Matt,2025-06-17T18:58:47Z,Post-PR fixes! (#38868)  * Post-PR fixes!  * make fix-copies,40,41,81
huggingface/transformers,508a7040556dc6b45f09174c662a9632284b2445,Matt,2025-06-17T18:37:18Z,"No more Tuple, List, Dict (#38797)  * No more Tuple, List, Dict  * make fixup  * More style fixes  * Docstring fixes with regex replacement  * Trigger tests  * Redo fixes after rebase  * Fix copies  * [test all]  * update  * [test all]  * update  * [test all]  * make style after rebase  * Patch the hf_argparser test  * Patch the hf_argparser test  * style fixes  * style fixes  * style fixes  * Fix docstrings in Cohere test  * [test all]  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",14906,14941,29847
huggingface/transformers,a396f4324bec03e0329bcd67a58648b95d557599,SohamPrabhu,2025-06-17T18:02:18Z,Update roc bert docs (#38835)  * Moved the sources to the right  * small Changes  * Some Changes to moonshine  * Added the install to pipline  * updated the monshine model card  * Update docs/source/en/model_doc/moonshine.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/moonshine.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/moonshine.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/moonshine.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Updated Documentation According to changes  * Fixed the model with the commits  * Changes to the roc_bert  * Final Update to the branch  * Adds Quantizaiton to the model  * Finsihed Fixing the Roc_bert docs  * Fixed Moshi  * Fixed Problems  * Fixed Problems  * Fixed Problems  * Fixed Problems  * Fixed Problems  * Fixed Problems  * Added the install to pipline  * updated the monshine model card  * Update docs/source/en/model_doc/moonshine.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/moonshine.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update docs/source/en/model_doc/moonshine.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Updated Documentation According to changes  * Fixed the model with the commits  * Fixed the problems  * Final Fix  * Final Fix  * Final Fix  * Update roc_bert.md  ---------  Co-authored-by: Your Name <sohamprabhu@Mac.fios-router.home> Co-authored-by: Your Name <sohamprabhu@Sohams-MacBook-Air.local> Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>,63,24,87
huggingface/transformers,3ae52cc312e0aa737d40887ec5c1356609883c59,Md. Muhaimin Rahman,2025-06-17T17:30:03Z,Update CvT documentation with improved usage examples and additional … (#38731)  * Update CvT documentation with improved usage examples and additional notes  * initial update  * cvt  * Update docs/source/en/model_doc/cvt.md  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>  * Update cvt.md  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>,57,29,86
huggingface/transformers,e5a9ce48f711b1f26eef3f7047a13b8235e4a71b,StevenBucaille,2025-06-17T16:10:23Z,"Add LightGlue model (#31718)  * init  * chore: various changes to LightGlue  * chore: various changes to LightGlue  * chore: various changes to LightGlue  * chore: various changes to LightGlue  * Fixed dynamo bug and image padding tests  * refactor: applied refactoring changes from SuperGlue's concat, batch and stack functions to LightGlue file  * tests: removed sdpa support and changed expected values  * chore: added some docs and refactoring  * chore: fixed copy to superpoint.image_processing_superpoint.convert_to_grayscale  * feat: adding batch implementation  * feat: added validation for preprocess and post process method to LightGlueImageProcessor  * chore: changed convert_lightglue_to_hf script to comply with new standard  * chore: changed lightglue test values to match new lightglue config pushed to hub  * chore: simplified convert_lightglue_to_hf conversion map  * feat: adding batching implementation  * chore: make style  * feat: added threshold to post_process_keypoint_matching method  * fix: added missing instructions that turns keypoints back to absolute coordinate before matching forward  * fix: added typehint and docs  * chore: make style  * [run-slow] lightglue  * fix: add matches different from -1 to compute valid matches in post_process_keypoint_matching  * tests: added CUDA proof tests similar to SuperGlue  * chore: various changes to modeling_lightglue.py  - Added ""Copies from"" statements for copied functions from modeling_superglue.py - Added missing docstrings - Removed unused functions or classes - Removed unnecessary statements - Added missing typehints - Added comments to the main forward method  * chore: various changes to convert_lightglue_to_hf.py  - Added model saving - Added model reloading  * chore: fixed imports in lightglue files  * [run-slow] lightglue  * chore: make style  * [run-slow] lightglue  * Apply suggestions from code review  Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>  * [run-slow] lightglue  * chore: Applied some suggestions from review  - Added missing typehints - Refactor ""cuda"" to device variable - Variable renaming - LightGlue output order changed - Make style  * fix: added missing grayscale argument in image processor in case use of SuperPoint keypoint detector  * fix: changed lightglue HF repo to lightglue_superpoint with grayscale default to True  * refactor: make keypoints `(batch_size, num_keypoints, keypoint_dim)` through forward and unsqueeze only before attention layer  * refactor: refactor do_layer_keypoint_pruning  * tests: added tests with no early stop and keypoint pruning  * refactor: various refactoring to modeling_lightglue.py  - Removed unused functions - Renamed variables for consistency - Added comments for clarity - Set methods to private in LightGlueForKeypointMatching - Replaced tensor initialization to list then concatenation - Used more pythonic list comprehension for repetitive instructions  * refactor: added comments and renamed filter_matches to get_matches_from_scores  * tests: added copied from statement with superglue tests  * docs: added comment to prepare_keypoint_matching_output function in tests  * [run-slow] lightglue  * refactor: reordered _concat_early_stopped_outputs in LightGlue class  * [run-slow] lightglue  * docs: added lightglue.md model doc  * docs: added Optional typehint to LightGlueKeypointMatchingOutput  * chore: removed pad_images function  * chore: set do_grayscale default value to True in LightGlueImageProcessor  * Apply suggestions from code review  Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>  * Apply suggestions from code review  Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>  * docs: added missing LightGlueConfig typehint in nn.Module __init__ methods  * docs: removed unnecessary code in docs  * docs: import SuperPointConfig only from a TYPE_CHECKING context  * chore: use PretrainedConfig arguments `num_hidden_layers` and `num_attention_heads` instead of `num_layers` and `num_heads`  * chore: added organization as arg in convert_lightglue_to_hf.py script  * refactor: set device variable  * chore: added ""gelu"" in LightGlueConfig as hidden_act parameter  * docs: added comments to reshape.flip.reshape instruction to perform cross attention  * refactor: used batched inference for keypoint detector forward pass  * fix: added fix for SDPA tests  * docs: fixed docstring for LightGlueImageProcessor  * [run-slow] lightglue  * refactor: removed unused line  * refactor: added missing arguments in LightGlueConfig init method  * docs: added missing LightGlueConfig typehint in init methods  * refactor: added checkpoint url as default variable to verify models output only if it is the default url  * fix: moved print message inside if statement  * fix: added log assignment r removal in convert script  * fix: got rid of confidence_thresholds as registered buffers  * refactor: applied suggestions from SuperGlue PR  * docs: changed copyright to 2025  * refactor: modular LightGlue  * fix: removed unnecessary import  * feat: added plot_keypoint_matching method to LightGlueImageProcessor with matplotlib soft dependency  * fix: added missing import error for matplotlib  * Updated convert script to push on ETH org  * fix: added missing licence  * fix: make fix-copies  * refactor: use cohere apply_rotary_pos_emb function  * fix: update model references to use ETH-CVG/lightglue_superpoint  * refactor: add and use intermediate_size attribute in config to inherit CLIPMLP for LightGlueMLP  * refactor: explicit variables instead of slicing  * refactor: use can_return_tuple decorator in LightGlue model  * fix: make fix-copies  * docs: Update model references in `lightglue.md` to use the correct pretrained model from ETH-CVG  * Refactor LightGlue configuration and processing classes  - Updated type hints for `keypoint_detector_config` in `LightGlueConfig` to use `SuperPointConfig` directly. - Changed `size` parameter in `LightGlueImageProcessor` to be optional. - Modified `position_embeddings` in `LightGlueAttention` and `LightGlueAttentionBlock` to be optional tuples. - Cleaned up import statements across multiple files for better readability and consistency.  * refactor: Update LightGlue configuration to enforce eager attention implementation  - Added `attn_implementation=""eager""` to `keypoint_detector_config` in `LightGlueConfig` and `LightGlueAttention` classes. - Removed unnecessary logging related to attention implementation fallback. - Cleaned up import statements for better readability.  * refactor: renamed message into attention_output  * fix: ensure device compatibility in LightGlueMatchAssignmentLayer descriptor normalization  - Updated the normalization of `m_descriptors` to use the correct device for the tensor, ensuring compatibility across different hardware setups.  * refactor: removed Conv layers from init_weights since LightGlue doesn't have any  * refactor: replace add_start_docstrings with auto_docstring in LightGlue models  - Updated LightGlue model classes to utilize the new auto_docstring utility for automatic documentation generation. - Removed legacy docstring handling to streamline the code and improve maintainability.  * refactor: simplify LightGlue image processing tests by inheriting from SuperGlue  - Refactored `LightGlueImageProcessingTester` and `LightGlueImageProcessingTest` to inherit from their SuperGlue counterparts, reducing code duplication. - Removed redundant methods and properties, streamlining the test setup and improving maintainability.  * test: forced eager attention implementation to LightGlue model tests  - Updated `LightGlueModelTester` to include `attn_implementation=""eager""` in the model configuration. - This change aligns the test setup with the recent updates in LightGlue configuration for eager attention.  * refactor: update LightGlue model references  * fix: import error  * test: enhance LightGlue image processing tests with setup method  - Added a setup method in `LightGlueImageProcessingTest` to initialize `LightGlueImageProcessingTester`. - Included a docstring for `LightGlueImageProcessingTester` to clarify its purpose.  * refactor: added LightGlue image processing implementation to modular file  * refactor: moved attention blocks into the transformer layer  * fix: added missing import  * fix: added missing import in __all__ variable  * doc: added comment about enforcing eager attention because of SuperPoint  * refactor: added SuperPoint eager attention comment and moved functions to the closest they are used  ---------  Co-authored-by: Steven Bucaille <steven.bucaille@buawei.com> Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",3632,2,3634
huggingface/transformers,2507169bf658e39e6ffe89a04b32e3729b218b73,Yih-Dar,2025-06-17T13:21:36Z,Fix `qwen3` tests (#38862)  * fix  * update  * update  * update  * update  * update  * update  * format  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,25,28,53
huggingface/transformers,41e0c921cbb2b2c9f82dd998d7b02bf680d037dd,Mikhail Moskovchenko,2025-06-17T12:56:46Z,Improve `auxiliary_in_channels` default behavior in UperNet (#37540)  Improve auxiliary_in_channels behavior in UperNet  Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>,8,4,12
huggingface/transformers,c61ca64aaa0618cc8bf53ef8f22a424230d163da,Yih-Dar,2025-06-17T08:55:24Z,Fix `qwen2_5_vl` tests (#38845)  * fix  * breakpoint()  * breakpoint()  * update  * update  * update  * update  * update  * update  ---------  Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>,20,16,36
huggingface/transformers,37367c7d9fd23401c26e79a2b26253ab2d1b7236,Kimish Patel,2025-06-17T08:38:20Z,"Allow customization of sdpa in executorch.py (#38827)  Earlier PR put executorch specific sdpa and mask function in the export function. This prevent any customization that can be done to sdpa, prior to export. By moving this to __init__, we still keep the original behavior but allow users like optimum-executorch to override sdpa by setting model.config._attn_implementation.",4,4,8
huggingface/transformers,9c878d2f64d38de8d59eba09553552595b28da96,Jingxiang Zhang,2025-06-17T07:33:36Z,Fix incorrect width ratio calculation in Llama4 image processor (#38842),1,1,2
huggingface/transformers,bf370e446b3d7aba686e3e979d84673313abb0d3,Raushan Turganbay,2025-06-17T07:20:16Z,[video processor] fix BC when no video config if found (#38840)  fix auto video processor,7,7,14
huggingface/transformers,e61160c5dbd470c4644e6c248d2acb64f763b6d5,Dhruv,2025-06-16T21:21:18Z,Remove merge conflict artifacts in Albert model doc (#38849),25,70,95
huggingface/transformers,64e9b049d96e7c42abfacc44b94d1b00f7a6c7db,Vanshu,2025-06-16T17:46:30Z,Updated aya_vision.md (#38749)  * Update aya_vision.md  * Suggested changes made to aya_vision.md  * Quantization Example added - aya_vision.md  * Polished - aya_vision.md  * Update aya_vision.md  ---------  Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>,167,140,307
huggingface/transformers,5ab0f447ab6aa988f7117e0b529115b2071befde,Shawn Tan,2025-06-16T15:15:42Z,GraniteMoeHybrid: Allow for only shared expert case. (#38801)  * Allow for only shared expert case.  * Style,26,7,33
huggingface/transformers,a7593a1d1f45150ca4b4fbf09deff4d9766ada84,Yusuf Shihata,2025-06-16T15:01:22Z,[BugFix] QA pipeline edge case: `align_to_words=True` in `QuestionAnsweringPipeline` can lead to duplicate answers (#38761)  * fixing the problem align_to_words=True leading to duplicate solutions  * adding tests  * some fixes  * some fixes  * changing the handle_duplicate_answers=False by default  * some fixese  * some fixes  * make the duplicate handling the default behaviour and merge duplicates  * make the duplicate handling the default behaviour,39,9,48
huggingface/transformers,18c7f32daa1512b8b459775736699ff173a29360,Drew Ross,2025-06-16T14:44:40Z,Fix broken tag in Longformer model card (#38828),1,1,2
huggingface/transformers,b44b04ee9af23589bd7855def584356e1627d88a,VolodymyrBg,2025-06-16T14:38:51Z,Fix broken notebooks link in Italian training docs (#38834),1,1,2
huggingface/transformers,9300728665aaeb0ebf4db99f9d9fbce916b4a183,Cyril Vallez,2025-06-16T08:39:25Z,Fix peft integration (#38841)  Update peft.py,20,1,21
huggingface/transformers,608884960ea74e3ac69b732ce6b6285e697fcf97,Cyril Vallez,2025-06-16T08:23:51Z,add default mapping to peft integration,7,5,12

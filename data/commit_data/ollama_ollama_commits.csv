repo,sha,author,date,message,additions,deletions,total_changes
ollama/ollama,4261a3b0b264430489921a1b4a16a6267711d595,先知,2025-07-11T22:15:00Z,"docs: update modelfile.md to reflect current default num_ctx (#11189)  As in the commit 44b466eeb2e42e9ce2852c69d7cddb7ebac5daf8, the default context length has been increased to 4096.",1,1,2
ollama/ollama,acef9b4c1b4bc97dba88ed02cc707635b96074de,Jesse Gross,2025-07-07T20:10:14Z,"ggml: Use assigned layers when reporting loading stats  Reporting params.NumGPULayers can be misleading because it is the requested number of layers, not the actual number that is loaded. While they are often the same, there are cases where they might mismatch, such as if the GPU backend is missing.",11,9,20
ollama/ollama,9a43994c45f8da1b21fd302d5ef000cee36c4e16,Jesse Gross,2025-07-10T23:55:34Z,"ggml: Disable unused pipeline parallelism  We're not currently using it, even in cases where we could. Disabling it improves generation performance by 10-30% with multiple GPUs.",1,1,2
ollama/ollama,f8a6e8881975b2964aa2179e74c4426b4a455d0f,Daniel Hiltgen,2025-07-11T19:21:54Z,Only load supported models on new engine (#11362)  * Only load supported models on new engine  Verify the model is supported before trying to load  * int: testcase for all library models,261,0,261
ollama/ollama,35fda7b4af556e7eeef2b5dcb3638435382b2576,Jesse Gross,2025-06-26T00:13:32Z,"ggml: Report ordinal IDs for AMD GPUs on Windows  We don't get valid UUIDs for AMD GPUs on Windows, so the best option is to use the ordinal IDs. This brings us in line with what we currently do on the Ollama server - the only exception is AMD GPUs on Linux, which falls back to using ordinal IDs. The GGML implementation has no fallback but it doesn't appear to occur for any of the GPUs that we support.  It's also possible that there are collisions between ordinal IDs for different libraries - however the only places where we use them are AMD on Windows and Metal on Mac, which can never occur on the same system.",45,33,78
ollama/ollama,66fb8575ced090a969c9529c88ee57a8df1259c2,Daniel Hiltgen,2025-07-08T22:38:04Z,doc: add MacOS docs (#11334)  also removes stale model dir instructions for windows,56,14,70
ollama/ollama,20c3266e943f62ef7947f00b563de5f6c790ecb7,Daniel Hiltgen,2025-07-08T19:08:37Z,"Reduce default parallelism to 1 (#11330)  The current scheduler algorithm of picking the paralellism based on available VRAM complicates the upcoming dynamic layer memory allocation algorithm.  This changes the default to 1, with the intent going forward that parallelism is explicit and will no longer be dynamically determined.  Removal of the dynamic logic will come in a follow up.",4,6,10
ollama/ollama,34088dbcfb47546fc0f375276173467bc8bbed29,Daniel Hiltgen,2025-07-08T18:59:06Z,API/CLI context enhancements (#11331)  * API: expose context size of loaded models  * CLI: add context UX  This adds a column in the ps output to show the models context size.,14,9,23
ollama/ollama,43107b15b9bcff51ef1c5391c273fd1a747f6d0a,Parth Sareen,2025-07-07T23:53:13Z,add `tool_name` to api.md (#11326),1,0,1
ollama/ollama,1f91cb0c8ccf734f060a7ed065e991233daa0448,Parth Sareen,2025-07-07T22:53:42Z,template: add tool result compatibility (#11294),348,8,356
ollama/ollama,12d8ad0d38ac0c7bbeba26501e839a7cc4e3b213,Daniel Hiltgen,2025-07-07T21:07:43Z,ci: modularization (#11324)  switch a few constants to variables,7,7,14
ollama/ollama,592d21e7dbffff5bd4c285503a44d87ffaa6fd1b,Jesse Gross,2025-06-27T23:19:44Z,"Revert ""ggml: Temporarily disable reporting UUIDs""  The root cause was an unclean upgrade - this code is fine.  This reverts commit 45f216a9c7e65bd30ab0e2b1b9fdb7cb2ad9436d.",2,5,7
ollama/ollama,5a08b01f5b5ee1a78917fdea0a677c946148c68d,Jeffrey Morgan,2025-07-06T00:20:42Z,readme: update Ollama icon size,1,1,2
ollama/ollama,4f473e224c643db9e7b25bd61b3a1a6e2a748b2b,Daniel Hiltgen,2025-07-05T23:07:09Z,"int: add performance integration tests (#11173)  usage example:   go test --tags=integration,perf -count 1 ./integration -v -timeout 1h -run TestModelsPerf 2>&1 | tee int.log   cat int.log | grep MODEL_PERF_HEADER | cut -f2- -d: > perf.csv   cat int.log | grep MODEL_PERF_DATA | cut -f2- -d: >> perf.csv",124771,31,124802
ollama/ollama,9d60bb44cf314db73dedbfa621e7f725999a8bc8,Daniel Hiltgen,2025-07-05T23:06:30Z,doc: add NVIDIA blackwell to supported list (#11307),2,0,2
ollama/ollama,f371260e75b86d77ac22671c24881f696c88973c,Vincent RAMPAL,2025-07-05T23:02:33Z,Update base image to Ubuntu 24.04 LTS (#9681),1,1,2
ollama/ollama,c9e6d7719e91d0bfa3bc6e73ddce0f5c7c3c26f1,Daniel Hiltgen,2025-07-03T16:48:45Z,doc: Update link for mac install (#11288)  Favor the dmg now.,1,1,2
ollama/ollama,2c4ce403344de1b45b1a9312a19b8f8be473664d,Daniel Hiltgen,2025-07-02T23:38:36Z,mimic logs for layers on new engine (#11278)  This adds some extra logs to make the new engine a bit more consistent with the llama engine.,18,0,18
ollama/ollama,5d8c1735296299c3d81bb40f00038398dc729579,XuKecheng,2025-07-01T16:46:15Z,readme: add NativeMind to community integrations (#11242),1,0,1
ollama/ollama,44b17d2bfa0073e012679152421c0b69671d380e,Jeffrey Morgan,2025-06-30T15:59:03Z,"tools: fix parsing tool calls with empty arguments, missing required fields (#11233)",86,32,118
ollama/ollama,3b8b692218bf0da28859c89b71f4f5731f29002c,Attogram Project,2025-06-29T21:59:54Z,readme: add ollama-bash-toolshed to community integrations (#11224),1,0,1
ollama/ollama,4129af9205763a113719c7ef102d5c6ff0f1e2e8,Michael Yang,2025-06-27T18:45:33Z,chore: cleanup comments + unused vars (#11225),0,4,4
ollama/ollama,45f216a9c7e65bd30ab0e2b1b9fdb7cb2ad9436d,Jesse Gross,2025-06-27T18:11:49Z,"ggml: Temporarily disable reporting UUIDs  This is causing segfaults, so disable it. Currently UUIDs are only used for debugging purposes, although they planned to be used in additional ways in the future.  Bug #11211",5,2,7
ollama/ollama,d0b32def60b413407ddf4b4b063ba105a1ef2f92,Michael Yang,2025-06-27T04:49:35Z,skip quantizing per_layer_token_embd (#11207)  this tensor isn't compatible with cuda when quantized to q4_K so skip it,2,0,2
ollama/ollama,11ffc36157029a73dab45ed11e2d2281dc2b58f9,Daniel Hiltgen,2025-06-26T17:32:48Z,ci: multi-stage release process (#11001),11,138,149
ollama/ollama,ba04902670cd5945ded682c1c9de2220475b9c38,Jeffrey Morgan,2025-06-26T07:19:44Z,fs/ggml: add multiplier in graph estimates (#11208),6,1,7
ollama/ollama,3944602f512b840953be00b6d028a0ab08b9b5ab,Jeffrey Morgan,2025-06-26T07:11:23Z,fs/ggml: add missing architecture to OllamaEngineRequired() (#11206),1,0,1
ollama/ollama,73b642e6f341287163c784e1e99a18426ee2ccea,Michael Yang,2025-06-26T04:47:09Z,add new gemma model (#11204)  * update patches  * cherry pick metal mean kernel  * cherry pick cuda mean kernel  * gemma3n,6084,54,6138
ollama/ollama,ad118d8b1306063d0ab2e967a20bd7914a135b28,Daniel Hiltgen,2025-06-25T04:00:15Z,ci: arm sbsa fixes (#11194),6,6,12
ollama/ollama,f08534137b127e1159f4b162881b8c2defc93158,Daniel Hiltgen,2025-06-25T03:27:43Z,ci: include dependencies,12,6,18
ollama/ollama,4b4a90f233ff807994f8de78b1f9b1687b6328a4,Daniel Hiltgen,2025-06-25T01:59:22Z,ci: pick up arm sbsa cuda libs (#11192),6,5,11
ollama/ollama,03274a6b2f2f6bd5cb109d5aa00c9b9d183500cf,Daniel Hiltgen,2025-06-25T01:45:01Z,ci: recombine linux amd64 binaries (#11188)  Glue the rocm and archive builds back together.,12,2,14
ollama/ollama,cc6463ebca22e07b08db58eb335263d82844132f,Devon Rifkin,2025-06-25T00:50:02Z,Merge pull request #10238 from ollama/drifkin/array-head-count-simple  ggml: fix crash for array head counts,111,31,142
ollama/ollama,405d2f628fe59ef1269861650d16e2943645c45b,Daniel Hiltgen,2025-06-24T22:27:09Z,"ci: rocm parallel builds on windows (#11187)  The preset CMAKE_HIP_FLAGS isn't getting used on Windows. This passes the parallel flag in through the C/CXX flags, along with suppression for some log spew warnings to quiet down the build.",11,4,15
ollama/ollama,a3f7dd3e98df803695f1ae165bc61a1b52142449,Devon Rifkin,2025-06-24T21:20:05Z,Merge branch 'main' into drifkin/array-head-count-simple,86,72,158
ollama/ollama,c85c0ebf895016c36bab10be4dd92f594c400df3,Daniel Hiltgen,2025-06-24T20:26:55Z,CI: switch windows to vs 2022 (#11184)  * CI: switch windows to vs 2022  * ci: fix regex match,12,6,18
ollama/ollama,10a8e04a8dcdd06de4ccaa3c2fe19452d2714b15,Daniel Hiltgen,2025-06-23T22:52:50Z,"avoid context overflow (#11175)  For smaller context models, make sure we do not exceed the training size.",7,0,7
ollama/ollama,1c6669e64cc8a482fbf1e35c0249f17b35a4e87a,Daniel Hiltgen,2025-06-23T21:07:00Z,"Re-remove cuda v11 (#10694)  * Re-remove cuda v11  Revert the revert - drop v11 support requiring drivers newer than Feb 23  This reverts commit c6bcdc4223c50071b59a19c42cc54ec9932f696f.  * Simplify layout  With only one version of the GPU libraries, we can simplify things down somewhat.  (Jetsons still require special handling)  * distinct sbsa variant for linux arm64  This avoids accidentally trying to load the sbsa cuda libraries on a jetson system which results in crashes.  * temporary prevent rocm+cuda mixed loading",67,66,133
ollama/ollama,b2b270ad5d6e4ad1ab7132364cfae06e21918926,Devon Rifkin,2025-06-23T17:37:31Z,Merge branch 'main' into drifkin/array-head-count-simple,17824,11495,29319
ollama/ollama,2bb69b40c7f6f2783290f0b7e4f7d5ec0a41f69c,AJ,2025-06-23T16:21:12Z,readme: add ai-hub to community integrations (#11169),1,0,1
ollama/ollama,65bff664cb39ed16a1fa814b0228e4e48d7234ba,Daniel Hiltgen,2025-06-20T19:32:51Z,build speedups (#11142)  Enable parallel building of the GPU architectures.,3,2,5
ollama/ollama,c088ac0e79a4a995e8a5a3733f7db2a981ac3364,Michael Yang,2025-06-20T18:12:01Z,convert: utility for merging tensors (#11069),174,53,227
ollama/ollama,0a066cfd91abdddc6ee172776974a6720a3072d3,Michael Yang,2025-06-20T18:11:40Z,"Reapply ""feat: incremental gguf parser (#10822)"" (#11114) (#11119)  * Reapply ""feat: incremental gguf parser (#10822)"" (#11114)  This reverts commit a6e64fbdf28f0d6cb97cc7f022ca493b905fe895.  * fix older ggufs",1362,169,1531
ollama/ollama,87b7af6ceef2b4d96374dbff5070b41b17d3f138,Jesse Gross,2025-06-19T21:39:20Z,"ggml: Check return status for computation.  We don't check the return status after computing the graph, which can silently lead to bad outputs if we try to keep going and future computation succeeds. This appears to happens in certain cases on Apple M2 devices.  Fixes #11070",3,1,4
ollama/ollama,f2527b08fba57d606e12cb21b583249c11724d7a,Daniel Hiltgen,2025-06-19T19:10:19Z,int: add coverage for older models (#11137)  Verified these fail on 0.9.1 and pass on HEAD.,2,0,2
ollama/ollama,8bcb3125c1b416b43aa431b2b3b105d933eca697,Jeffrey Morgan,2025-06-18T19:58:50Z,benchmark: remove unused benchmark test (#11120)  Removes a test under benchmark/ that is unused,0,237,237
ollama/ollama,6baf1e31e2e5b28c4ce6d145f4524448c9747204,Jeffrey Morgan,2025-06-18T14:30:49Z,"Revert ""Revert ""ggml: Export GPU UUIDs"" (#11115)"" (#11117)  Reverts PR #11115. The original change was mistakingly reverted instead of #10822",151,0,151
ollama/ollama,ed567ef43b5822423bd165f5f57fb6bad5fce1b3,Jeffrey Morgan,2025-06-18T12:45:00Z,"Revert ""ggml: Export GPU UUIDs"" (#11115)  This reverts commit aaa7818000c42a82fc030212c35ef83f9799efd7.",0,151,151
ollama/ollama,a6e64fbdf28f0d6cb97cc7f022ca493b905fe895,Jeffrey Morgan,2025-06-18T12:42:44Z,"Revert ""feat: incremental gguf parser (#10822)"" (#11114)  This reverts commit 6b04cad7e816d1a119559e092d59f4fbaa6c3a0b.",169,1362,1531
ollama/ollama,60cfa2a203201e8513b7da26035f74495e498bc7,曹家巧,2025-06-18T12:21:45Z,cache: fix comment function name in cache.go (#11110),1,1,2
ollama/ollama,55bbf3b4a1766fc99e32ebd39f0dd5749152b55d,Jeffrey Morgan,2025-06-18T12:20:43Z,tools: return empty arguments object instead of null (#11113),17,12,29
ollama/ollama,6bda1d24798e40fc9ea1419c6ce22c6cdcc9dfe2,Jeffrey Morgan,2025-06-17T17:51:43Z,"tools: fix parsing tool calls without any parameters (#11101)  Fixes issue where tool calls that don't expect any parameters were not being parsed. This also fixes two additional issues: one where 2+ tool calls would not be correctly parsed, and cases where tool calls with invalid parameters would still get parsed",297,46,343
ollama/ollama,9e125d884cf995dfae7fcd74690d525e4326a517,Jeffrey Morgan,2025-06-16T23:03:16Z,model: treat 'user defined' tokens as special tokens (#11077),17,1,18
ollama/ollama,a6fbfc880c3de9b57e341db374907e2fedda9fa6,Michael Yang,2025-06-16T17:42:32Z,gguf: fix write order (#11068)  * ggml: test write gguf order * ggml: fix write tensor order,69,55,124
ollama/ollama,502028968ddca04bd19c0859a73fb4e0cbeac3e1,NGC13009,2025-06-16T04:27:49Z,readme: add ollama-launcher to community integrations (#11080),1,0,1
ollama/ollama,5a8eb0e1510a5a35b80649f2b88e9231716b6850,Phil,2025-06-14T15:54:03Z,readme: add GPTranslate to community integrations (#11071),1,0,1
ollama/ollama,9f8a18ec050ef67fca11d4f9bea0508eece93a68,Jeffrey Morgan,2025-06-12T21:18:54Z,tools: loosen tool parsing to allow for more formats (#11030),1153,2090,3243
ollama/ollama,6b04cad7e816d1a119559e092d59f4fbaa6c3a0b,Michael Yang,2025-06-12T18:04:11Z,feat: incremental gguf parser (#10822)  * incremental gguf parser * gguf: update test to not rely on gguf on disc * re-use existing create gguf * read capabilities from gguf kv * kv exists * update tests * s/doneFunc/successFunc/g * new buffered reader  ---------  Co-authored-by: Bruce MacDonald <brucewmacdonald@gmail.com>,1362,169,1531
ollama/ollama,45f56355d557b7130c7c07bbd6e1b634a758d946,Michael Yang,2025-06-11T19:10:54Z,"feat: uneven splits (#11048)  The current splitDim function only operates on tensors that are split evenly which isn't always the case, e.g. a QKV tensor. This change allows the function to be used for arbitrary splits",344,20,364
ollama/ollama,0dabb4ef6a1aab240a59b6bb4ef82372d335e3a9,Michael Yang,2025-06-11T19:10:35Z,"skip tokenizer.model if possible (#11050)  if tokenizer.json is already copied, skip tokenizer.model",12,7,19
ollama/ollama,2e77aa1ae70372388bd4b08b9957e5198d566a22,Michael Yang,2025-06-11T19:10:15Z,"use nn.Linear in place of ml.Tensor (#11049)  while nn.Linear.Forward isn't applicable for sparse MLP, it's still a nice container for the tensors",12,12,24
ollama/ollama,deaabe292d86b712e061bebe7fdd6be6690f539b,Attogram Project,2025-06-10T21:14:51Z,readme: add ollama-multirun to community integrations (#11038),1,0,1
ollama/ollama,af21a5ac397c1d2ce62881e0962bbff9d6da31f2,Jeffrey Morgan,2025-06-10T16:34:23Z,readme: update quickstart link text to Gemma 3,1,1,2
ollama/ollama,f63d7f68eb206cf403fb9c7dca7978d16204e268,Jeffrey Morgan,2025-06-10T16:33:54Z,readme: update quickstart example to Gemma 3,2,2,4
ollama/ollama,82ad1dbc07dc2db39c6f502eb148ff3ce00d96b8,Daniel Hiltgen,2025-06-09T23:29:57Z,"mac: handle ""keep"" named apps (#11031)  When a user elects to keep the existing app, the new Ollama is named `Ollama 2.app` This fixes the app startup flow to handle this naming pattern.",5,4,9
ollama/ollama,feeabdadd2b272b40747f3e7e74957c40ba2800c,Daniel Hiltgen,2025-06-08T16:34:52Z,spawn desktop quickly (#11011)  Give the desktop app a hint to start fast.,2,2,4
ollama/ollama,fc0309615e42c32989e060e733d871e16617874e,Krzysztof Jeziorny,2025-06-07T03:30:04Z,docs: update link to AMD drivers in linux.md (#10973),2,2,4
ollama/ollama,09d308d6b6c7995e3fb565e0ecfa184d49205f00,Jeffrey Morgan,2025-06-07T03:29:14Z,"Revert ""server: add model capabilities to the list endpoint (#10174)"" (#11004)  This reverts commit 09430011936652cf55925184aaed6f2cebf62a75.",19,37,56
ollama/ollama,a8ed68bd9383ffec346fd1b3cf60d94c032bbec8,Daniel Hiltgen,2025-06-06T21:06:29Z,"launch app hidden (#10962)  When starting the app in the background, start it hidden.",2,5,7
ollama/ollama,2ae65ae471c9d51d343f401da16c05b98b99a842,Daniel Hiltgen,2025-06-06T21:06:09Z,win: handle more than 2048 processes (#10997)  Fix an array out of bounds crash,10,1,11
ollama/ollama,a3b6886b7da0339e63ebf41e6ba5c6b06438a123,Devon Rifkin,2025-06-06T19:02:20Z,move thinking logic into its own package (#10990)  move thinking logic into its own package,281,269,550
ollama/ollama,c6a6d7294dd50b9216918fe72fd92bc4ae572ac0,Hunter Wittenborn,2025-06-06T16:07:29Z,docs: fix typo in development.md (#10998),1,1,2
ollama/ollama,2cf007c9d120583ee9ad7ad39b276f2ff81eeb62,Devon Rifkin,2025-06-05T19:19:14Z,Merge pull request #10987 from ollama/drifkin/export-thinking-parser  export ThinkingParser,35,35,70
ollama/ollama,0683efa6379ba69384bb5876f1013d9ef38f1ab0,Devon Rifkin,2025-06-05T17:22:32Z,export ThinkingParser,35,35,70
ollama/ollama,09430011936652cf55925184aaed6f2cebf62a75,JasonHonKL,2025-06-04T18:39:48Z,server: add model capabilities to the list endpoint (#10174),37,19,56
ollama/ollama,5c42800fca4da07d1c362c0f190429993e53c3b5,HardCodeDev,2025-05-31T02:50:16Z,readme: add SimpleOllamaUnity to community integrations (#10817),1,0,1
ollama/ollama,65f10c2823d540837a9e79202522957194377735,Parth Sareen,2025-05-30T22:18:09Z,tools: resiliency upgrade to name and arg extraction from template (#10917),97,69,166
ollama/ollama,aaa7818000c42a82fc030212c35ef83f9799efd7,Jesse Gross,2025-04-24T18:48:49Z,ggml: Export GPU UUIDs  This enables matching up devices and information reported by the backend with system management libraries such as nvml to get accurate free memory reporting.,151,0,151
ollama/ollama,f15ffc432061e3d96b3412219a3a0f673b579a12,Jesse Gross,2025-05-14T00:26:46Z,"llm: Make ""POST predict"" error message more informative  ""POST predict"" basically means that the runner has crashed, which can have many reasons. However, many people think this is a specific error and either report only this message or group together unrelated bugs. This replaces it with a more friendly and helpful message.",2,1,3
ollama/ollama,5f57b0ef4268a6bd9e8043d54c351a608a7e1bca,Devon Rifkin,2025-05-29T02:38:52Z,"add thinking support to the api and cli  (#10584)  - Both `/api/generate` and `/api/chat` now accept a `""think""`   option that allows specifying whether thinking mode should be on or   not - Templates get passed this new option so, e.g., qwen3's template can   put `/think` or `/no_think` in the system prompt depending on the   value of the setting - Models' thinking support is inferred by inspecting model templates.   The prefix and suffix the parser uses to identify thinking support is   also automatically inferred from templates - Thinking control & parsing is opt-in via the API to prevent breaking   existing API consumers. If the `""think""` option is not specified, the   behavior is unchanged from previous versions of ollama - Add parsing for thinking blocks in both streaming/non-streaming mode   in both `/generate` and `/chat` - Update the CLI to make use of these changes. Users can pass `--think`   or `--think=false` to control thinking, or during an interactive   session they can use the commands `/set think` or `/set nothink` - A `--hidethinking` option has also been added to the CLI. This makes   it easy to use thinking in scripting scenarios like   `ollama run qwen3 --think --hidethinking ""my question here""` where you   just want to see the answer but still want the benefits of thinking   models",1195,49,1244
ollama/ollama,aa25aff10d1ccc6dd4e85952678d63946bdf89dc,Patrick Devine,2025-05-27T23:50:57Z,"client: add request signing to the client (#10881)  If OLLAMA_AUTH is set, sign each request w/ a timestamp and pass the signature in the token header",52,0,52
ollama/ollama,ea79003180205680000bacf97466fc9d78d71f5e,Jesse Gross,2025-05-27T20:33:57Z,"kvcache: Skip computing causal mask for worst case graph reservation  Computing an attention mask for a large context and max batch is expensive - over 100ms. Models like Gemma3 that have multiple types of caches and custom attention masks need to do this 4 times, so this adds approximately 500ms to startup time when using 128k context  When we are reserving the worst case graph, we don't need the mask, only its shape, so we can skip this.",12,1,13
ollama/ollama,9239a254e054d24b0de3358ba8c4bd9b50730bfd,Kyle Steere,2025-05-27T18:28:48Z,server: abort download on empty digest  Signed-off-by: Kyle Steere <kyle.steere@chainguard.dev>,4,0,4
ollama/ollama,066d0f474671fd38532f4a245533158312a68e75,Parth Sareen,2025-05-27T01:59:06Z,tools: relax JSON parse constraints for tool calling (#10872),54,35,89
ollama/ollama,aea6fb9b5862932ea4622e854be9ddae8c320658,Parth Sareen,2025-05-27T00:16:00Z,tools: remove newline stripping (#10869),2,10,12
ollama/ollama,012cf65340ccd44de76017c56eb553b0803fde95,RAPID ARCHITECT,2025-05-26T19:05:03Z,readme: add AWS Strands Agents SDK example to community integrations (#10865),1,0,1
ollama/ollama,a45231af47035e89031625d47210663881a4e2cf,Min Yoo,2025-05-24T20:18:32Z,"readme: Add macLlama to community integrations (#10790)  This commit updates the README to include macLlama within the community integrations section.  macLlama is a native macOS application built for lightweight and efficient LLM interaction.  Key features include:  *   **Lightweight & Native:** Designed to be resource-friendly and perform optimally on macOS. *   **Chat-like Interface:** Provides a user-friendly, conversational interface. *   **Multiple Window Support:** Allows users to manage multiple conversations simultaneously.  The primary goal of macLlama is to offer a simple and easy-to-run LLM experience on macOS.",1,0,1
ollama/ollama,2307fc2bcd8e9067ed5575d7570b69fff9cc4f38,Daniel Hiltgen,2025-05-24T20:17:53Z,tests: drop llama3.2-vision embedding tests (#10837),0,1,1
ollama/ollama,6623898198fece95e6c5cef7ad55d9d1ecd1da1f,frob,2025-05-24T20:17:26Z,docs: remove unsupported quantizations (#10842),0,10,10
ollama/ollama,eda472df1bd420517ca05c59ba0096e8b518fb69,frob,2025-05-24T20:17:04Z,server: add hint to the error message when model path access fails (#10843),2,2,4
ollama/ollama,f18e0cb5508450bd14db5ec8015709d2c4ab820f,Jesse Gross,2025-05-23T22:37:32Z,ml: Improve slog formatting for BackendMemory,36,0,36
ollama/ollama,e8b981fa5d7c1875ec0c290068bcfe3b4662f5c4,Parth Sareen,2025-05-23T21:19:31Z,tools: refactor tool call parsing and enable streaming (#10415),1868,340,2208
ollama/ollama,884d26093c80491a3fe07f606fc04851dc317199,Parth Sareen,2025-05-23T01:53:31Z,llama: add minimum memory for grammar (#10820),1,1,2
ollama/ollama,1f371ea92f7ebe4edd208b6732753473b2c4d0cd,Jesse Gross,2025-05-19T17:43:56Z,"ml: Panic rather than return error on tensor allocation failure  FromFloatSlice and FromIntSlice return an error if the shape doesn't match the passed data or if memory can't be allocated. Since these are inputs, the memory being allocated is system memory rather than VRAM.  In many cases, the caller can't really handle the error and panics.  Empty and Zeros directly panic if they can't allocate memory.  This makes things consistent by panicing for the first two cases, removing a fair amount of error handling code. This is also consistent with how Go typically handles these situations.",68,209,277
ollama/ollama,73d6a82cce18f84ff5c67148783224cf25b30b32,Jesse Gross,2025-04-17T18:00:25Z,"ollamarunner: Memory usage reporting  This provides granular information about the backend memory allocations required by the runner:  - Per backend  - Per layer  - Weights, cache and graph  - Allocation status  This can be used for debugging and validating memory estimates.",220,74,294
ollama/ollama,6db8a3771c29d070ef165cca0d7e8dbda3fc341e,Jesse Gross,2025-05-16T21:05:08Z,"ggml: Report graph memory for failed allocations  GGML has a function to report the allocated size of a backend buffer. However, this returns 0 if we tried to allocate a buffer and it failed. For memory management purposes, it's important to know how much we were trying to allocate. This extends the API to report attempted sizes for all buffers and whether it succeeeded.",212,4,216
ollama/ollama,d950ff12c09c07a1cda7242373071fb9e7af9ddc,Daniel Hiltgen,2025-05-22T21:31:36Z,"sched: fix runner leak during reloading unload (#10819)  When the same model is being reloaded rapidly with client connections being canceled before the model finishes loading, the queued unload event could cause a leak of runners by deleting a different runner from the loaded list.",11,0,11
ollama/ollama,adff143bcda0c7ab4ca3a85dc3db5a81552368c7,Michael Yang,2025-05-22T18:30:49Z,fix: mllama quality (#10807)  * fix mllama convert  - transform attn_gate and ffn_gate - swap attention heads for vision models  * fix mllama  the mlp gate which was applied in the wrong place,40,37,77
ollama/ollama,fbe6ae285a23baddb14c5bbce26d4fcb837503e4,Bruce MacDonald,2025-05-22T17:48:08Z,"server: improve tensor quantization fallback logic (#10806)  Fall back to alternative quantization types when a tensor's dimensions aren't divisible by the block size required for the original desired quantization type. If retried quantization types fail, the system ultimately falls back to F16 (half-precision floating point) which has a block size of 1 and can handle any tensor dimension.",22,6,28
ollama/ollama,fdd4d479a3cbaff1a7fe849e38a145652f87d611,Daniel Hiltgen,2025-05-22T16:12:32Z,integration: add qwen2.5-vl (#10815)  Replace the older llava model with qwen2.5 for vision tests Skip split-batch test on small VRAM systems to avoid excessive test time,2,1,3
ollama/ollama,61aeaf7e813bf307f1b28480c7ee2aed639b28f7,Michael Yang,2025-05-21T20:55:31Z,remove support for multiple ggufs in a single file (#10722)  * remove support for multiple ggufs in a single file  this was an attempt to make it easier to import multimodal models into ollama. this was rarely used and error prone so remove it  * fix: create fused model from blob,14,35,49
ollama/ollama,7359b0270767d1ebda33598d738a4263a4238b3a,Daniel Hiltgen,2025-05-21T17:46:56Z,win: detect background upgrade in progress (#10785)  Give the user a helpful error instead of showing connection refused errors.,51,3,54
ollama/ollama,c890011322fbdd325ef9f16e425fe1f5213a24fe,Michael Yang,2025-05-21T17:21:24Z,feat: port qwen2 model (#10782),194,24,218
ollama/ollama,e0ed984cde1f6191e38ac2d7f4415ffd619a631f,Michael Yang,2025-05-21T17:21:07Z,feat: qwen3 dense and sparse models (#10708)  * feat: qwen3 dense * feat: qwen3moe * fix llama4 moe,258,1,259
ollama/ollama,139f84cf21f8d8107f69c1404f17a8840c6d67d0,Michael Yang,2025-05-21T16:52:52Z,fix cmakelists (#10804)  this fixes an issue introduced in #10788,1,1,2
ollama/ollama,375839ea2d05a056d02f934f02e953b41f1d444d,Michael Yang,2025-05-21T16:39:38Z,chore: disable debug in binary libraries (#10788),4,2,6
ollama/ollama,69b2fe9282323a57cd3557bed9b598b465d1b3a6,Michael Yang,2025-05-21T16:39:20Z,fix: qwen25vl assign samebatch in multimodal input (#10789)  setting samebatch on the vision start token is problematic because it will be shared with other inputs that also use images. this will cause the input to be cached and the runner will not see SameBatch. SameBatch will also be incorrect since it may be for a different image.  assigning samebatch to the input tokens resolves this by ensure it's assigned correctly to inputs corresponding to the image.  not setting same batch correctly may cause panics during inference since images are no longer guaranteed to be in the same batch.,2,1,3
ollama/ollama,9ed8bf14cb885509281d63731cda16637a7e0bd2,Michael Yang,2025-05-20T22:51:08Z,ml: add more rope options (#10775),116,83,199
ollama/ollama,e6a800ca11cb52b24fa5afc5245ed1277811fbe9,DarkCaster,2025-05-20T17:41:15Z,llama: fix incorrect initialization of C.struct_common_sampler_cparams.penalty_present (#10779),1,1,2
ollama/ollama,ff180c3466e7f3ee21658465958c9ece6de2d5c0,Michael Yang,2025-05-19T22:06:35Z,fix llama and mistral3 models (#10774)  * fix llama model  * fix mistral3.1 model  do not set default vision layers,13,32,45
ollama/ollama,3fe74fba42b8d496a1ab3e8298bdc9b8ffb0f336,Jesse Gross,2025-05-19T18:40:44Z,"llm: Use first layer as memory buffer in estimation  This is a partial revert of 0478d44 ""Fixed over vram allcation dure to small initial layer sizes.""  Previously we used the size of the first layer as an extra reserved amount of space to buffer our memory estimates. The above commit changed this to use the largest layer. However, this had performance impacts on more models than the original commit was trying to fix.  There is just a heuristic without an ideal solution so this goes back to the historic behavior.  Fixes: #10765, #10756, #10752, #10726",6,7,13
ollama/ollama,1a0cfd080a2d3e65519c241b7561bf5aa49468ff,Daniel Hiltgen,2025-05-19T20:54:54Z,avoid kv truncation during create (#10761),1,1,2
ollama/ollama,94ab428e3f77fdd9d9c833b369bb40980c65049a,Jesse Gross,2025-04-17T20:42:40Z,"ggml: Seperate tensor load from backend creation  Currently, when the backend is created, the tensors are loaded at the same time, which is a slow operation. This separates them to be two steps:  - Create backend, including enumerating tensors and memory allocation  - Loading tensor data  This allows more flexibility in managing model loading.",123,107,230
ollama/ollama,d75557747357bfb3afd441a0cc207ec944bd3a18,Jesse Gross,2025-05-13T18:36:52Z,"llm: Estimate projector memory correctly for Ollama engine  The Llama engine always places vision projectors on the first GPU if one exists. However, the Ollama engine groups it with the output layer, which means the projector is only offloaded if all other layers are offloaded. The memory estimation code always assumes the former layout - this changes it to use the correct layout based on the engine.  This addresses two impacts of the current behavior:  - In multi-GPU setups, we can crash with OOM errors when we try to    allocate memory on a full GPU while another still has space.  - If the vision projector is large, it may prevent us from offloading    anything when we could have fit some of the text layers.",18,14,32
ollama/ollama,a2cc8571c5b2b8f77a8a5e2f65cb7aaa56482dc4,Jesse Gross,2025-05-13T20:04:20Z,"llm: Consistently track unassigned model data  In some cases, if we fail to assign a piece of the model to a GPU then we lose track of this data. Although it doesn't change the memory allocation, it does affect the total size of the model reported by tools such as ollama ps (and also the percent offloaded).  This makes it look like setting num_gpu isn't reflected in ollama ps, which isn't true but the offloading percent may appear to not change.  Spreading the model across more GPUs will continue to impact the reported total size of the model.",13,9,22
ollama/ollama,7edfdd2f5f48a7be035cec23b4acd12f7c112e1c,Ronald Wilson,2025-05-18T19:43:22Z,"readme: add TinyNotepad to community integrations (#10763)  This PR adds Tiny Notepad, a lightweight, notepad-like interface to chat with local LLMs via Ollama.   - It’s designed as a simple, distraction-free alternative.  - The app supports basic note-taking, timestamped logs, and model parameter controls.  - Built with Tkinter, it runs entirely offline and available via PyPI.  Aims to provide a lightweight easy to run and install interface for ollama.",1,0,1
ollama/ollama,333e360422744e92275af2c1c2d5bc039ad97e8f,Michael Yang,2025-05-16T20:40:23Z,model: handle multiple eos tokens (#10577)  * get eos_token_id from generation_config.json  * refactor  * include both ids and strings in trace  * comments  * remove special case for gemma3 special vocab (#10743),282,182,464
ollama/ollama,27da2cddc514208f4e23539e00485b880e3e2191,Daniel Hiltgen,2025-05-15T23:33:23Z,Fix lingering Q4_0 help reference (#10720),1,1,2
ollama/ollama,feb8923adada675b19dc1bc20f39ed6cfb0b99da,Bruce MacDonald,2025-05-15T22:45:52Z,cmd: add ellipses to truncated show metadata (#10717)  When a piece of information has been truncated in the show output an ellipses to indicate that more data has not been displayed,44,7,51
ollama/ollama,fe623c2cf44e672dde4552985d9f758a9d09605d,Jesse Gross,2025-04-07T20:59:11Z,ollamarunner: Multi-modal worst case graph  We currently preallocate compute graph memory for the worst case batch of text tokens. This adds support for doing the same for images.  Note that image models are more complicated than text models in how they process their inputs so there may be cases where this approach isn't completely generic for all models. It covers all currently supported models though.,88,14,102
ollama/ollama,3c14461d5d2280723b3f961fb99ad128b3eee9af,Jesse Gross,2025-05-05T20:32:11Z,"ollamarunner: Separate text and multimodal graphs  For some multimodal models (such as gemma3), we create a single graph that generates the image embedding and then use this in the text model. The embedding tensor is completely opaque to the runner.  However, this doesn't work if we need to use the embedding in multiple batches. This can arise if the embedding is larger than the batch size. In these cases (as with llama4), we would like to create views that are more appropriately sized. However, if we do this then the original source tensor is used in multiple graphs, which isn't allowed. To avoid that problem, models with this pattern compute the embedding tensor on first use and recreate the individual views. There is no longer a single vision and text graph.  This codifies the pattern of separating vision and text graphs. The logic of computing tensors on demand is moved to the runner, so models no longer have to worry about this. It also gives the runner visibility into the multimodal tensors, which is important for memory management.",239,184,423
ollama/ollama,499ae7311fd26cb4e655ebea69712de3e242f629,Jesse Gross,2025-05-09T23:51:47Z,"ollamarunner: Base cached tokens on current prompt  When we restore a sequence from the cache, we split the prompt into the already used tokens (stored in the cache) and new tokens that need to be processed. Currently, the references to the used tokens are coming from the stored previous sequence.  However, even though we know that the used tokens are semantically equivalent to the prefix of the prompt, tokens can contain pointers which are no longer valid. As a result, it is better to get the used tokens from the prompt, which has currently valid pointers.  This doesn't currently have any impact because it isn't possible to reuse the pointers (which are tensors) anyways. However, it becomes an issue once we can.",2,2,4
ollama/ollama,ef202789fad6b8a8ab51f4d2ff5450067e3d1f65,Michael Yang,2025-05-15T20:44:44Z,fix pixel values padding (#10718)  * panic if trying to pad 4d  * fix pixel values padding,10,3,13
ollama/ollama,55760195e654992ca5f364aa191b24611b3b7531,Michael Yang,2025-05-15T19:15:01Z,"fix mllama conversion (#10716)  cross attention Q and K projections needs to have their heads swapped, similar to non-cross attention Q and K tensors",4,3,7
ollama/ollama,bd68d3ae50c67ba46ee94a584fa6d0386e4b8522,Bruce MacDonald,2025-05-14T23:42:30Z,ggml: update qwen25vl vision size estimate (#10711),6,16,22
ollama/ollama,ff80718e9c9a08dd10759cdee5c81db366e38368,Daniel Hiltgen,2025-05-14T21:54:18Z,fix crash in old clients with quantization progress (#10710)  Older clients assumed the digest was at least 19 characters long so increase the size of the dummy digest to avoid array out of bounds crashes.,1,1,2
ollama/ollama,0aa8b371ddd24a2d0ce859903a9284e9544f5c78,Bruce MacDonald,2025-05-14T03:58:02Z,model: add Qwen2.5-VL support (#10385),1619,10,1629
ollama/ollama,23125648b8748d9f2ec93c3038db8689f5693f6e,Michael Yang,2025-05-14T00:36:02Z,chore: update mllama to use ollama engine (#10637),785,4354,5139
ollama/ollama,0478d440f0ba62202bc4b98043ae4a7d0b85e4ba,tej,2025-05-13T23:42:39Z,Fixed over vram allcation dure to small initial layer sizes.  Co-authored-by: Tej Kiran <kiran.tej@amd.com> Co-authored-by: Michael Yang <mxyng@pm.me> Co-authored-by: Tej Kiran <itej89@gmailcom>,13,11,24
ollama/ollama,8cc33f4c2ba9347b3de3b5fe197d486df741d3e4,Parth Sareen,2025-05-13T22:39:27Z,llama: fix memory leak for grammar (#10696),3,0,3
ollama/ollama,f46df4e5d2e964ccfd0f23f9377240b6d9897ed8,Jeffrey Morgan,2025-05-13T21:02:08Z,llama: fix defrag patch to defragment when no slots are available (#10695),50,7,57
ollama/ollama,c6bcdc4223c50071b59a19c42cc54ec9932f696f,Daniel Hiltgen,2025-05-13T20:12:54Z,"Revert ""remove cuda v11 (#10569)"" (#10692)  Bring back v11 until we can better warn users that their driver is too old.  This reverts commit fa393554b927f154145488c852297a2330cb5f13.",58,11,69
ollama/ollama,4b903f088aa8d14404e5650d42db8c15530803d5,Jeffrey Morgan,2025-05-13T20:11:11Z,llama: fix crash on snowflake embedding model (#10690),8,8,16
ollama/ollama,c7f4ae7b9c8976b4d50c59eb87e9582ea9c5c82f,Jeffrey Morgan,2025-05-13T03:41:42Z,server: add webp image input support (#10653),24,7,31
ollama/ollama,526b2ed10296cc3d1ae89121eedcbbbe257741a3,Michael Yang,2025-05-13T00:29:46Z,fix vocabulary (#10679),28,26,54
ollama/ollama,a7240c6d636836f0bca01790038d7194f519604b,Bruce MacDonald,2025-05-12T23:08:42Z,models: remove unused qwen2vl processing (#10677),0,152,152
ollama/ollama,9d6df9080502adcb6f25950e3d829ab05ec8cfc8,Daniel Hiltgen,2025-05-12T22:23:31Z,"Follow up to #10363 (#10647)  The quantization PR didn't block all unsupported file types, which this PR fixes.  It also updates the API docs to reflect the now reduced set of supported types.",88,382,470
ollama/ollama,0cefd46f23ed60d5b41a90e6b6a1864e4214da3b,Jeffrey Morgan,2025-05-12T19:17:26Z,llama: update to commit de4c07f93 (#10655),7953,4239,12192
ollama/ollama,ad035ad595295d0026a5a94f8180962bbf0fa935,Bruce MacDonald,2025-05-12T19:04:20Z,convert: quantize from safetensors needs kv (#10675)  When creating a quantized model from safetensors we need the array KV values to be loaded.Changing this value to -1 loads the KV values on the returned layer to be used and saved during quantization.,1,1,2
ollama/ollama,f95a1f2bef74cc5badd12b8a6f873d47298595ad,Michael Yang,2025-05-12T18:43:00Z,feat: add trace log level (#10650)  reduce prompt log to trace level,141,114,255
ollama/ollama,82a9e9462ab28415aab53fa47e33d70849274941,HardCodeDev,2025-05-11T20:44:51Z,readme: add UnityCodeLama to community integrations (#10665),1,0,1
ollama/ollama,76724e2f2912537cc20a1b11175d21231563ffff,HardCodeDev,2025-05-11T20:40:41Z,readme: add OllamaPlusPlus C++ library to community integrations (#10664),1,0,1
ollama/ollama,ecf14a220f852528bae860ec09b52590f06e6744,frob,2025-05-10T18:57:30Z,llama: allocate grammar buffer based on schema length (#10649),2,2,4
ollama/ollama,69ce44b33c49ad7035d75cac6b35a96b1ccc2e33,frob,2025-05-10T18:31:04Z,envconfig: Remove no longer supported max vram var (#10623)  Co-authored-by: Richard Lyons <frob@cloudstaff.com>,0,2,2
ollama/ollama,5969674cf167a9c65aa3d556de0064cf9ad1572d,Michael Yang,2025-05-10T18:27:15Z,feat: add threshold to dump options (#10639)  ml.Dump will preserve default values if not specified,40,16,56
ollama/ollama,867d75b21e4cc13d3cf74f1db1707508103d0030,AliAhmedNada,2025-05-10T17:36:40Z,readme: add ojira to community integrations (#10648),1,0,1
ollama/ollama,3fa78598a1e86faee9390ded4b43e78ca3bef816,Bruce MacDonald,2025-05-10T01:05:43Z,cmd: strip single quotes from image page (#10636),25,0,25
ollama/ollama,0d6e35d3c67cf37de1c425d178c71d7351083013,Michael Yang,2025-05-08T20:17:30Z,"fix: stream accumulator exits early (#10593)  the stream accumulator exits as soon as it sees `api.ProgressResponse(status=""success"")` which isn't strictly correctly since some requests may have multiple successes, e.g. `/api/create` when the source model needs to be pulled.",96,12,108
ollama/ollama,20c5fd39c8b275c0c7d7e7be8ce03d48aa32c64e,Devon Rifkin,2025-05-08T18:46:52Z,Merge branch 'main' into drifkin/array-head-count-simple,6285,3240,9525
ollama/ollama,6e9a7a256856bf1119c992cf7da39c05276f386c,Michael Yang,2025-05-08T18:42:14Z,"lint: enable usetesting, disable tenv (#10594)",55,87,142
ollama/ollama,b585a5812191a998fbd331f786e7ec31b6aae6a7,Michael Yang,2025-05-08T18:17:41Z,chore: remove unused ZipReader type (#10621),0,58,58
ollama/ollama,fa9973cd7f51a662226490a853b38c2cfa602a80,Jeffrey Morgan,2025-05-08T15:31:08Z,api: remove unused sampling parameters (#10581),3,30,33
ollama/ollama,3d9498a42550766b53e1acb35b076fcb560d6b13,Jesse Gross,2025-05-08T00:16:07Z,ollamarunner: Use correct constant to remove cache entries  The correct constant to remove all entries to the end of the sequence for the Ollama engine is math.MaxInt32. -1 is used by the old engine.  The impact of this is currently minimal because it would only occur in situations that are not supported by the implemented models or rarely used options.,1,1,2
ollama/ollama,3098c8b29b4da4b47f1c1c2969e52b9455274797,Daniel Hiltgen,2025-05-07T17:35:12Z,CI: trigger downstream release process (#10508),16,0,16
ollama/ollama,5e380c3b42741541d01cea0c821f4e01aa4e432e,Daniel Hiltgen,2025-05-07T16:38:17Z,"sched: fix race leading to orphaned runners (#10599)  If a model is loading, and the request context is canceled during the load by a client closing the connection, and another request is inbound for the same model with a different configuration (context size, etc.) thus requiring a reload, two unload events can be in flight.  The first shuts down the original model load, but the second one caused the loss of the new reloading runner reference, thus triggering the leak.  The primary fix is detecting the duplicate unload and ignoring the second instance.  The load routine is also hardened to ensure we detect clobbering an already present runner and unload it with a warning.",40,20,60
ollama/ollama,392de84031e71cbd97ffe19b89ccf6cfeed9c7b3,Jeffrey Morgan,2025-05-07T06:08:03Z,api: remove unused RetrieveModelResponse type (#10603),4,11,15
ollama/ollama,af31ccefc00ff7e98839dd0066c8ddb7a44a7ccb,Daniel Hiltgen,2025-05-07T00:36:38Z,fix data race in WriteGGUF (#10598)  err in the go routine should not be shared with the outer scope,1,1,2
ollama/ollama,fa393554b927f154145488c852297a2330cb5f13,Daniel Hiltgen,2025-05-07T00:33:19Z,remove cuda v11 (#10569)  This reduces the size of our Windows installer payloads by ~256M by dropping support for nvidia drivers older than Feb 2023.  Hardware support is unchanged.  Linux default bundle sizes are reduced by ~600M to 1G.,11,58,69
ollama/ollama,307e3b3e1d3fa586380180c5d01e0099011e9c02,Aharon Bensadoun,2025-05-06T21:47:35Z,readme: add Flufy to community integrations (#9719),1,0,1
ollama/ollama,4090aca97b768e093832a5b2ed101bb867168d5c,Devon Rifkin,2025-05-06T21:45:37Z,server: send 405 instead of 404 for unallowed methods (#10275)  Fixes: #5483,11,0,11
ollama/ollama,92ce438de06c6225ffc004600df7a1ec5a439f8e,Michael Yang,2025-05-06T20:05:01Z,server: remove internal cmd (#10595),0,599,599
ollama/ollama,424810450f3043e97aca539f1250d149a26cd99e,Daniel Hiltgen,2025-05-06T18:20:48Z,"Move quantization to new backend (#10363)  * Move quantization logic to GGML via new backend  This moves the model aware logic to Go code and calls GGMLs quantization code for model creation.  * Remove ""add model quantizations""  This is no longer needed now that quantization is implemented in Go+GGML code directly.",1850,436,2286
ollama/ollama,95e744beeb82f725579932336eeabc0de019cbf4,Michael Yang,2025-05-06T17:49:22Z,discover: fix compiler warnings (#10572),11,7,18
ollama/ollama,3b2d2c8326c245f0210a549777d0a77c2ccd92d1,Jeffrey Morgan,2025-05-05T21:54:40Z,"api: remove unused or unsupported api options (#10574)  Some options listed in api/types.go are not supported in newer models, or have been deprecated in the past. This is the first of a series of PRs to clean up the API options",8,24,32
ollama/ollama,d931ee8f22d38a87d4ff1886ccf56c38697f3fa0,Michael Yang,2025-05-05T18:59:26Z,create blobs in parallel (#10135)  * default max term height * error on out of tree files,110,22,132
ollama/ollama,70736007976543a9f54bbdb9e484478ad3af21a0,Jesse Gross,2025-05-05T17:37:16Z,"ggml: Reduce log level of ""key not found""  Most of the time this is not an error.",1,1,2
ollama/ollama,b1c40138dadfd32550ae03391bdee63ef5e2b8bb,Daniel Hiltgen,2025-05-05T18:08:12Z,win: lint fix (#10571),1,1,2
ollama/ollama,17466217e55091e4380b7b3fc7684a2476fdba57,Ashok Gelal,2025-05-05T16:06:46Z,Hide empty terminal window (#8668)  This hides the LlamaServer blank window when chatting outside of the terminal (say like with an app like Msty). This has no other side effects when invoking it the regular way.,2,1,3
ollama/ollama,1703d1472ed2ff257fe338ddf83e199316f8b5b7,Jeffrey Morgan,2025-05-05T16:01:33Z,server: fix panic when runner.Options is nil (#10566),3,1,4
ollama/ollama,913905028badc32a8283750c5342340113269865,Jeffrey Morgan,2025-05-05T15:02:39Z,all: fix cgo compiler warnings on windows (#10563),8,6,14
ollama/ollama,7e5c8eee5c65bcf7c0d46d5fe3b084fd70d36015,湛露先生,2025-05-04T22:37:59Z,file close check and close. (#10554)  Signed-off-by: zhanluxianshen <zhanluxianshen@163.com>,2,0,2
ollama/ollama,6a74bba7e7e19bf5f5aeacb039a1537afa3522a5,Daniel Hiltgen,2025-05-03T20:11:48Z,win: ensure ollama paths come first (#10549)  For all search path env vars make sure our dirs are first to avoid potentially finding other incompatible libraries on the users system.  Also fixes a minor build script glitch for windows rocm,5,5,10
ollama/ollama,76ea735aafe8cd3165a105b70ca427dce9d7aec4,Daniel Hiltgen,2025-05-03T19:01:56Z,"sched: logging improvements (#10550)  This enhances our logging in the scheduler.  The initial ""waiting for server"" log no longer claims an initial error state (now ""not responding"" which better reflects the actual state).  Runners now have slog wiring to report more details about the runner, including PID.",57,14,71
ollama/ollama,dd1d4e99e7e8475d1669f566bb5c0ae30db419f1,aritra saha,2025-05-03T02:45:02Z,readme: add llama 4 models (#10530),2,0,2
ollama/ollama,a6ef73f4f26a22cc605516113625a404bd064250,Jesse Gross,2025-05-02T00:06:53Z,"ggml: Fix race that resulted in ""context canceled"" when loading  Successfully completing processing with an errgroup cancels the associated context. However, we also have a goroutine that is checking for cancelation of the context. As a result, there is a race where the goroutine can pick up the cancelation and report an error, replacing the sucessful error message.  To avoid that, this replaces the goroutine with a cancelation check when we are reading files. This also has the advantage of stopping all reads relatively quickly on error and also ensuring that there are no outstanding I/O operations when we return in this case.  The downside is that if a file read blocks forever (for example, over the network) then cancelation of the context effectively won't be honored. However, this is also true for other smaller files we read and the tensors are read in small chunks (128K), so it's consistent and better on balance overall.",5,8,13
ollama/ollama,c2f5d6662ba696b94455eaaa298a797d56fdca5c,Jesse Gross,2025-05-02T18:24:19Z,"ollamarunner: Re-enable worst case graph preallocation.  Worst case graph preallocation was disabled by a27462b ""ollamarunner: Temporarily disable worst case graph preallocation"" since it caused crashes with large batches when not using the GPU.  This backports upstream llama.cpp commit f057808 ""ggml: Don't assert fail when tensor data changes (#13222)"", which fixes the underlying bug and allows reverting the previous workaround.",46,7,53
ollama/ollama,57fb759f3cb0716eeed5b59b047a7e141d1659d2,Harsh Nevse,2025-05-02T06:08:51Z,readme: update link to langchain in community integrations (#10465),1,1,2
ollama/ollama,8dd12c873d7816f76d5488c8ff879a574dad5e01,Jeffrey Morgan,2025-05-02T01:24:09Z,llama: update to commit e1e8e099 (#10513),3745,1736,5481
ollama/ollama,e6d2d04121fdd89b424ae57cc64511862f50f31a,frob,2025-05-01T23:50:20Z,image: add vision capability for projector-based models (#10509)  Co-authored-by: Richard Lyons <frob@cloudstaff.com>,5,0,5
ollama/ollama,074bac844714b3a011d1cf6ebc40c169b10d1aef,Jesse Gross,2025-05-01T20:45:32Z,"kvcache: Log batch size if we can't find a slot  In some cases, we can't find a cache slot when using sliding window attention. It would be helpful in this (and other cases) to know what the batch size is.  Bug #10127",1,1,2
ollama/ollama,8e8f2c6d67ff3ee758102f197bb1b347ed1e97bd,Jesse Gross,2025-05-01T18:34:02Z,"ollamarunner: Fix memory leak when processing images  The context (and therefore associated input tensors) was not being properly closed when images were being processed. We were trying to close them but in reality we were closing over an empty list, preventing anything from actually being freed.  Fixes #10434",7,15,22
ollama/ollama,938e8447e8b8e107047b5d30515c2e589d185d6d,AliAhmedNada,2025-05-01T21:49:47Z,readme: add Jirapt project to community integrations (#10522),1,0,1
ollama/ollama,d5d5f0c44561bccd26374e99f9cc98c6160e4a99,aritra saha,2025-05-01T21:46:09Z,readme: change granite3.2 to granite3.3 (#10525)  Update the list for readme,1,1,2
ollama/ollama,a7835c671615d71280ca7dba7264bd05a4f90915,Michael Yang,2025-05-01T00:59:31Z,fix: write gguf padding (#10510)  * add gguf_test  * fix padding  padding was being added to offset but not to the running count,65,1,66
ollama/ollama,ad3c7c9bda3b2db9a6887f65e3134c093333d3d5,Devon Rifkin,2025-04-30T20:57:45Z,"strip out thinking tags in message history for qwen3 & r1 (#10490)  * strip out thinking tags in message history for qwen3 & r1  This is in advance of ""proper"" support where we'll make reasoning configurable and we'll parse out thinking/reasoning tags and provide them to the caller. These models expect there to be no thinking tags in the message history, so this should improve quality  * parse model names instead of hacky prefix check",148,0,148
ollama/ollama,415c8fcc3deee73e8a11822a962a87c7cb58d938,Daniel Hiltgen,2025-04-30T18:26:52Z,"Fix ""Stopping..."" scheduler hang (#10487)  * Adjust initial scheduler refCount  Ensure we only set the refCount on success  * sched: fix lock order inversion deadlock  Under certain race conditions, there was a scenario where the scheduler would get into a deadlock while trying to update free space information while a model was trying to unload.",9,8,17
ollama/ollama,718eda1b3ec79b9b4a5f87ac8350d6ed7f63598b,Daniel Hiltgen,2025-04-30T18:25:22Z,Narrow set of paths we load GGML from (#10485)  Users may have other incompatible GGML installs on their systems. This will prevent us from trying to load them from the path.,10,11,21
ollama/ollama,421b7edeb4f466fdd8ea25b03cdc595eee86fa6c,Shahin R,2025-04-30T16:50:47Z,"readme: add link to lumina, a lightweight React frontend client (#10378)",1,0,1
ollama/ollama,7b68e254c29bea48dec9a455a5a46b4ac6c823c2,batuhankadioglu,2025-04-29T23:51:09Z,all: update several golang.org/x packages (#10436),18,18,36
ollama/ollama,7bec2724a56ad990d66b9ca05e2b47191955596c,Daniel Hiltgen,2025-04-29T18:57:54Z,integration: fix embedding tests error handling (#10478)  The cleanup routine from InitServerconnection should run in the defer of the test case to properly detect failures and report the server logs,15,11,26
ollama/ollama,a27462b7085c7ba794f3b8da1553f4f1caa08ed0,Jesse Gross,2025-04-29T17:48:39Z,"ollamarunner: Temporarily disable worst case graph preallocation  When we later have a large batch running purely on a CPU, this results the error: GGML_ASSERT(talloc->buffer_id >= 0)  Disabling this means that we will incrementally reallocate memory as the graph grows.  Fixes #10410",6,4,10
ollama/ollama,6bf0b8193acadc5d595f320796d9876302ae0620,crStiv,2025-04-29T17:30:44Z,readme: fix typos (#10399),17,17,34
ollama/ollama,db428adbb81413d8be3933eb0ec2256ab36880ce,Devon Rifkin,2025-04-29T17:21:36Z,Merge pull request #10468 from ollama/drifkin/num-parallel-1,1,1,2
ollama/ollama,fe5b9bb21b971a9326fc47fe51a4c351ccbc12a9,Devon Rifkin,2025-04-29T09:04:14Z,"lower default num parallel to 2  this is in part to ""pay"" for #10452, which doubled the default context length. The combination isn't fully neutral though, because even though the old 4x2k limit and the new 2x4k limit are memory equivalent, the 1x fallback is larger with 4k",1,1,2
ollama/ollama,6ec71d8fb6705d12c9bc3df3511ce9b255ee375f,Devon Rifkin,2025-04-29T00:13:51Z,Merge pull request #10452 from ollama/drifkin/4096-context-length  config: update default context length to 4096,5,5,10
ollama/ollama,44b466eeb2e42e9ce2852c69d7cddb7ebac5daf8,Devon Rifkin,2025-04-29T00:03:23Z,config: update default context length to 4096,5,5,10
ollama/ollama,a25f3f8260c421413cb821ba8cb338fff6b32280,Devon Rifkin,2025-04-29T00:02:10Z,"Merge pull request #10451 from ollama/revert-10364-drifkin/context-length  Revert ""increase default context length to 4096""",12,49,61
ollama/ollama,dd93e1af85be606c642bcd9da1f69761cfdbaee8,Devon Rifkin,2025-04-28T23:54:11Z,"Revert ""increase default context length to 4096 (#10364)""  This reverts commit 424f648632c925ce14a75018c4dcab395e035993.",12,49,61
ollama/ollama,d2ee599dcf9746a3af0433463f9ce7dbc8cbc693,Devon Rifkin,2025-04-27T20:45:13Z,load arrays with up to 1024 elements when estimating  This mirrors the old behavior before #10382,1,1,2
ollama/ollama,6ed88985903be474ecd59992f7191c2f0fa87e36,Devon Rifkin,2025-04-25T23:16:15Z,"ggml: fix crash for array head counts  If it's an array, it uses the max value in the array  If array values for head counts becomes more popular, we can consider a more invasive change like #10225 to calculate more accurate estimates.  Fixes: #9984",110,30,140
ollama/ollama,5cfc1c39f3d5822b0c0906f863f6df45c141c33b,Michael Yang,2025-04-26T02:24:48Z,model: fix build (#10416),1,1,2
ollama/ollama,f0ad49ea17d587cce7f4b2c6a6ccb3139ec083c8,Michael Yang,2025-04-23T23:20:40Z,memory,5,2,7
ollama/ollama,7ba9fa9c7d0bc73abacca88d6827d973d7ba92cf,Michael Yang,2025-04-21T17:45:56Z,fixes for maverick,53,26,79
ollama/ollama,8bf11b84c1870b88e9e1568041ee837a062aa2c0,Michael Yang,2025-04-11T01:00:43Z,chunked attention,84,4,88
ollama/ollama,470af8ab899aca6a72571f0c1e2ac6f9049aca29,Michael Yang,2025-04-17T22:46:55Z,connect vision to text,80,4,84
ollama/ollama,178761aef3d5c66fecd5bb394b38475c1543a4b5,Michael Yang,2025-04-16T22:25:34Z,image processing  Co-authored-by: Patrick Devine <patrick@infrahq.com>,494,4,498
ollama/ollama,f0c66e6dea7d79c0f6106540d20cea37f93bd97f,Michael Yang,2025-04-03T22:18:29Z,llama4,833,15,848
ollama/ollama,54055a6dae0588d178eb355a8c36051d1b6e98a5,Michael Yang,2025-04-25T23:15:08Z,fix test,1,2,3
ollama/ollama,340448d2d1689f707d7644bc8ca018e0db015c9c,Michael Yang,2025-04-25T23:08:25Z,explicitly decode maxarraysize 1024,6,6,12
ollama/ollama,ced7d0e53df146200c322ccc7f3493aa32f627e1,Michael Yang,2025-04-23T23:07:11Z,fix parameter count,1,1,2
ollama/ollama,a0dba0f8aefad9a843b59040857f4a04021c54e1,Michael Yang,2025-04-23T23:05:57Z,default slice values,63,4,67
ollama/ollama,5e20b170a783c8a476991ca4a7ee32029761fe66,Michael Yang,2025-04-23T22:24:20Z,update comment,1,2,3
ollama/ollama,d26c18e25c493ca55add9713ed151081c8de7ecf,Michael Yang,2025-04-23T19:40:05Z,fix token type,36,25,61
ollama/ollama,8d376acc9b3e6796a12e6462c530b1bff3dc2f90,Michael Yang,2025-04-23T19:23:21Z,zero means zero  use a default of 1024 when asking for zero is confusing since most calls seem to assume 0 means do not ready any data,0,4,4
ollama/ollama,dc1e81f027db50b5d57f9bef620f06ff8d603c18,Michael Yang,2025-04-23T19:22:02Z,convert: use -1 for read all,2,3,5
ollama/ollama,5d0279164c2fcb4a1d3100d30988ba54ace548d1,Michael Yang,2025-04-23T18:22:06Z,generic ggml.array,102,125,227
ollama/ollama,214a7678eab94a6acf88fd1682f8fe6733ba555d,Michael Yang,2025-04-24T20:09:39Z,"fix superfluous call to WriteHeader  the first call to http.ResponseWriter.Write implicitly calls WriteHeader with http.StatusOK if it hasn't already been called. once WriteHeader has been called, subsequent calls has no effect. Write is called when JSON encoding progressUpdateJSON{}. calls to http.ResponseWriter.WriteHeader after the first encode is useless and produces a warning:  http: superfluous response.WriteHeader call from github.com/ollama/ollama/server/internal/registry.(*statusCodeRecorder).WriteHeader (server.go:77)",6,1,7
ollama/ollama,4892872c184a8fa81baaedf9669bf94e4a278964,Michael Yang,2025-04-25T21:45:15Z,convert: change to colmajor,13,16,29
ollama/ollama,0b9198bf47d71b9cc1830beb29123d0d9f302f07,Michael Yang,2025-03-21T22:54:49Z,ci: silence deprecated gpu targets warning,4,2,6
ollama/ollama,e9e5f61c45e13f9b87be985ae735de8c217e9915,Jeffrey Morgan,2025-04-25T00:26:02Z,llama: update to commit 2016f07b (#10352),1956,1742,3698
ollama/ollama,11dde418241a8f3d8a67206ae51229e30fa69695,Parth Sareen,2025-04-24T23:47:57Z,server: improve spacing for JSON grammar (#10131),5,5,10
ollama/ollama,a53d744b01c65de77afb77aed4a576b317a90912,Parth Sareen,2025-04-24T18:51:19Z,llama: remove model loading for grammar (#10096),514,100,614
ollama/ollama,40b10eee6d62a32578ca7e884fb73d4c8bc644a0,Adrien Duermael,2025-04-24T03:13:51Z,api: fix ImageData struct comment to expect raw image bytes (#10386),1,1,2
ollama/ollama,424f648632c925ce14a75018c4dcab395e035993,Devon Rifkin,2025-04-22T23:33:24Z,"increase default context length to 4096 (#10364)  * increase default context length to 4096  We lower the default numParallel from 4 to 2 and use these ""savings"" to double the default context length from 2048 to 4096.  We're memory neutral in cases when we previously would've used numParallel == 4, but we add the following mitigation to handle some cases where we would have previously fallen back to 1x2048 due to low VRAM: we decide between 2048 and 4096 using a runtime check, choosing 2048 if we're on a one GPU system with total VRAM of <= 4 GB. We purposefully don't check the available VRAM because we don't want the context window size to change unexpectedly based on the available VRAM.  We plan on making the default even larger, but this is a relatively low-risk change we can make to quickly double it.  * fix tests  add an explicit context length so they don't get truncated. The code that converts -1 from being a signal for doing a runtime check isn't running as part of these tests.  * tweak small gpu message  * clarify context length default  also make it actually show up in `ollama serve --help`",49,12,61
ollama/ollama,2eb1fb3231063365408155d2fffce9d62ad3c5ee,Richard Shiue,2025-04-20T22:38:06Z,readme: add AppFlowy to community integrations (#10335),2,1,3
ollama/ollama,08065216425ba73828805756118f26b61cd03f28,greengrass821,2025-04-20T22:21:48Z,cmd: add support for escaping ~ in filepath (#10339)  Co-authored-by: tooth paste <tooth_paste91@Poorneshwars-MacBook-Pro.local>,1,0,1
ollama/ollama,88738b357bcd25eea860b59bf7de2f6b94cfc352,Michael Yang,2025-04-18T23:32:48Z,create tempdir in models directory  the models directory should have plenty of storage and also ensure there's no cross-device copy,1,1,2
ollama/ollama,4e535e618846ffb00a2a6714c07847d6d2951453,Blake Mizerany,2025-04-19T01:12:28Z,"server/internal/registry: make pull send errors with Error field (#10326)  Previously, the pull handler would send an error message in the Status field, this prevented the client from using the message as a signal to stop. In the case of the ""run"" command, it would follow the pull with a ""show"" which would print a nearly identical ""not found"" message for unresolved models.  Fixes #10307",9,7,16
ollama/ollama,40b8fdbdcacb41b9cf42869051df765f66750036,Michael Yang,2025-04-03T17:25:23Z,arange,42,20,62
ollama/ollama,1d99451ad705478c0a22262ad38b5a403b61c291,Blake Mizerany,2025-04-17T19:43:09Z,server/internal/client/ollama: handle some network errors gracefully (#10317),115,16,131
ollama/ollama,09bb2e30f69489b2bd5138fa81d9dbb54c1d2f19,Jeffrey Morgan,2025-04-17T02:54:20Z,ml/backend/ggml: use default CUDA compression mode (#10314),1,1,2
ollama/ollama,dc264be6ffa39cc2cb02565fd8674a894b066936,Jeffrey Morgan,2025-04-17T01:56:29Z,ml: add missing cmake property and remove additional CMakeLists.txt (#10310),2,539,541
ollama/ollama,fbe70396181222e2d91ca1d8895b11c5fd464c3f,Devon Rifkin,2025-04-16T22:15:08Z,Merge pull request #10290 from ollama/drifkin/template-highlighting  docs: change more template blocks to have syntax highlighting,2,2,4
ollama/ollama,943464ccb85678069b2ad44051f13203c51e67b0,Jeffrey Morgan,2025-04-16T22:14:01Z,llama: update to commit 71e90e88 (#10192),42991,33852,76843
ollama/ollama,369de832cdca7680c8f50ba196d39172a895fcad,Blake Mizerany,2025-04-16T21:43:07Z,server/internal/registry: remove superfluous progress bar flush (#10303)  This removes the extra flushProgress() at the end of handlePull. It is unnecessary because final progress updates are flushed in all cases of the main select loop.,0,1,1
ollama/ollama,3457a315b241d5d2ada9958d22cc5effb2643a7e,Blake Mizerany,2025-04-16T21:33:40Z,"server/internal/client/ollama: cleanup use of multiple counters (#10304)  The completed and received counters must work in tandem and the code should better reflect that. Previously, the act of updating them was 2-3 lines of code duplicated in multiple places. This consolidates them into a single update closure for easy reading and maintenance.  This also simplifies error handling in places where we can use a return parameter and defer to handle the error case for updates.  Also, remove the old Layer field from the trackingReader struct.",38,41,79
ollama/ollama,ed4e1393149e1ba5e8fbf5b6629d2658342e39d9,Daniel Hiltgen,2025-04-16T21:25:55Z,"Integration test improvements (#9654)  Add some new test coverage for various model architectures, and switch from orca-mini to the small llama model.",709,67,776
ollama/ollama,56dc316a57f07fbed80723d1ecd589da0906018e,Daniel Hiltgen,2025-04-16T20:37:00Z,Give tests more time to run (#10306)  Fix flake failures on windows,1,1,2
ollama/ollama,2fec73eef6e9482f606f185ebb2ae4f75ad1a37c,Michael Yang,2025-04-11T20:39:51Z,fix write gguf padding,13,16,29
ollama/ollama,1e7f62cb429e5a962dd9c448e7b1b3371879e48b,Blake Mizerany,2025-04-16T06:24:44Z,"cmd: add retry/backoff (#10069)  This commit adds retry/backoff to the registry client for pull requests.  Also, revert progress indication to match original client's until we can ""get it right.""  Also, make WithTrace wrap existing traces instead of clobbering them. This allows clients to compose traces.",233,158,391
ollama/ollama,ccb7eb81357c6612705c6b67306616e2bc57acf7,Jesse Gross,2025-04-14T19:12:36Z,"ggml: Free ggml_backend_buffer_t when releasing buffer  When ggml_backend_buffer_free() is called, the device memory is released but not all backends consistently release the actual ggml_backend_buffer_t in system RAM, causing a memory leak.  Bug #10040",224,53,277
ollama/ollama,637fd2123040749ed6af918ac2a00c532d8b4c31,Devon Rifkin,2025-04-15T19:08:11Z,"docs: change more template blocks to have syntax highlighting  In #8215 syntax highlighting was added to most of the blocks, but there were a couple that were still being rendered as plaintext",2,2,4
ollama/ollama,0fe487e7325cd72b8b06e758c2a03842c557dbda,Devon Rifkin,2025-04-15T00:42:51Z,Merge pull request #10276 from ollama/drifkin/cors-headers  server: add `OpenAI-Beta` header to CORS safelist,7,6,13
ollama/ollama,6bfaa6e282e2d137c5e4bc96dfa2bddaab465ee9,Devon Rifkin,2025-04-15T00:11:20Z,Merge pull request #10277 from ollama/drifkin/docs-json-errors  docs: update some response code blocks to json5,2,2,4
ollama/ollama,378d3210dc5393d5155f2143325b1b26d29c88fb,Devon Rifkin,2025-04-15T00:09:06Z,docs: update some response code blocks to json5  This is to prevent rendering bright red comments indicating invalid JSON when the comments are just supposed to be explanatory,2,2,4
ollama/ollama,97fe45e36da2880bef95398fda711f09c3028998,Devon Rifkin,2025-04-14T22:36:10Z,server: add `OpenAI-Beta` header to CORS safelist  alphabetized the compat list and then added a single header  fixes: #9801,7,6,13
ollama/ollama,64a9cc8f05c44a7267c73eaa8bd61ea077c5280a,CYJiang,2025-04-14T11:49:41Z,cmd: add missing file close in tests (#10179),1,0,1
ollama/ollama,f50d691254e671e69975c4e54fc4d0469b538f10,Jesse Gross,2025-04-08T19:11:55Z,"ggml: Fix memory leak on input tensors  For every forward pass through the model, we need to allocate input tensors: tokens, images, positions, outputs and masks. These get allocated in system memory.  However, when we close the context that the tensors were allocated through, the metadata gets freed but the actual backend memory does not. This results in a significant memory leak.  This makes it so that all the memory allocated through a context gets freed when it is closed.  Fixes #10040",23,8,31
ollama/ollama,34c3b68fc8a14eb5a93f6bdd175fa94e2e8fa12b,Jesse Gross,2025-04-09T18:21:57Z,"ggml: Don't allocate CPU buffers as CUDA Host buffers  Allocating (and in particular, freeing) memory from CUDA host buffers is expensive and can cause a significant performance hit if we do it for every token. Using normal system memory avoids this issue and also gives the OS more flexibility to manage it.  There is no performance impact from this patch directly (either positive or negative) but it makes a difference once we start freeing memory correctly.",0,6,6
ollama/ollama,f33ccd5d27f521dac79bba0312371414a0b3bc08,Jesse Gross,2025-03-11T23:06:06Z,ggml: Use pointer receivers for Context  Context is currently mixed between pointer and value receivers. Change this to be all pointer receivers so don't have to reason about whether the things we are updating in the struct will be retained.,12,12,24
ollama/ollama,bc108b9ad61da81a5d170e0f487b7603fbeb768f,Jesse Gross,2025-04-10T18:55:05Z,ggml: Log filesystem errors  Sometimes loading the GGUF file fails with: panic: context canceled  This is probably a filesystem error but it doesn't provide any information about what happened.,2,0,2
ollama/ollama,ef65174df23fb2efb499a18d7071348cc0ec58da,Tom Sheffler,2025-04-10T00:45:49Z,"types: include the 'items' and '$defs' fields to properly handle ""array"" types (#10091)   ---------  Co-authored-by: Parth Sareen <parth.sareen@ollama.com>",16,0,16
ollama/ollama,42ecb9f13896c5329764e1946ec3ab1aad2de0a1,Ire Gaddr,2025-04-09T23:01:02Z,fix(scheduler): make model unload order deterministic (#10185),14,8,22
ollama/ollama,5c0331fd83877a5a91ec216c6d40b7ceaa8ff51e,湛露先生,2025-04-09T20:24:56Z,Fix dockerfile. (#9855)  Signed-off-by: zhanluxianshen <zhanluxianshen@163.com>,2,2,4
ollama/ollama,e7019c94554e6d93bf216cb484c8c1c69df98fcb,CYJiang,2025-04-08T22:17:40Z,fix(integration): move waitgroup Add(1) outside goroutine to avoid potential issue (#10070)  Signed-off-by: googs1025 <googs1025@gmail.com>,2,2,4
ollama/ollama,d98bfe7e7083f54e4c9065d4138f53bd47348761,Michael Yang,2025-04-03T23:54:46Z,kvcache: stub out test structs,6,111,117
ollama/ollama,6747099d715bfc8c9679c3f6365a08ffb950ad32,Parth Sareen,2025-04-08T22:05:38Z,types: add any type and validation for ToolFunction enum (#10166),71,10,81
ollama/ollama,ccc8c6777bc926b4cdb3b0a89ad344418b6710da,frob,2025-04-08T22:01:39Z,cleanup: remove OLLAMA_TMPDIR and references to temporary executables (#10182)  * cleanup: remove OLLAMA_TMPDIR * cleanup: ollama doesn't use temporary executables anymore  ---------  Co-authored-by: Richard Lyons <frob@cloudstaff.com>,0,7,7
ollama/ollama,dbb149e6f78673cc1c84e6527321c740d8d36a9a,Jesse Gross,2025-04-03T19:50:20Z,"ollamarunner: Preallocate worst case graph at startup  Currently, the KV cache and graph are lazily allocated as needed. The cache is fully allocated on first use of the corresponding layer whereas the graph grows with the size of the context.  This can be an issue if another application allocates more VRAM after we do our calculations - Ollama will crash in the middle of inference. If we instead allocate the maximum needed memory at startup of the runner, we will either succeed or fail at that point rather than at some surprising time in the future.  Currently, this only generates a worst case batch for text, which means that vision models may get a partial allocation and continue to lazily allocate the rest.",148,47,195
ollama/ollama,a807985e598c7a905f563023e1f8ee04756e2e36,Jesse Gross,2025-04-04T22:04:25Z,"ggml: Check for OOM and return as Go errors  If there is a CUDA OOM, we currently don't check the return value and will evetually segfault. This checks for the problem and generates a Go error. At the moment, this will still result in a panic but having the error is the first step to being able to handle it more gracefully.",33,8,41
ollama/ollama,8643c4d5bfa8ba131661713029d09b9f1792761a,qwerty108109,2025-04-08T02:42:26Z,readme: fix url for big-AGI in community integrations (#10173),1,1,2
ollama/ollama,b0c3aba5906ebc51b1c4cbb4532173559ba6e6ed,Jonathan Hecl,2025-04-07T23:31:45Z,readme: add GGUF-to-ollama to community integrations (#10156),1,0,1
ollama/ollama,19c0c25de88cacc453e8f608f236ad5bf1891f5a,qwerty108109,2025-04-07T23:27:20Z,readme: rename community integration from Claude Dev to Cline (#10168),1,1,2
ollama/ollama,2f723ac2d6112344944d95efb0287a99c14a05c4,Alex Rozgo,2025-04-07T21:27:01Z,types: allow tool function parameters with a single type or an array of types (#9434),149,27,176
ollama/ollama,249fbbe52fdfb850175bf409eaa515b0c0100dc9,Devon Rifkin,2025-04-07T21:02:35Z,Merge pull request #10169 from ollama/drifkin/fix-contributing-formatting  CONTRIBUTING: fix code block formatting,1,1,2
ollama/ollama,c38680b8a1058b79a98d87aa671361cafed09536,Devon Rifkin,2025-04-07T20:53:33Z,"CONTRIBUTING: fix code block formatting  There were only 3 spaces instead of 4, so the example was being considered to include html elements",1,1,2
ollama/ollama,16fca86c4a6c43e2752062e7f82d20f00d9fd292,Michael Yang,2025-04-05T00:33:07Z,digest files in parallel,21,5,26
ollama/ollama,0f3f9e353df96d4cfc40ac19114c782a57fe30f5,Daniel Hipke,2025-04-05T00:04:24Z,"ml/backend/ggml: create a new file descriptor for tensor (#10133)  improves model loading times on network-based filesystems such as GCS fuse by creating a dedicated file descriptor for each section of the file being read, reducing seeking",8,1,9
ollama/ollama,6bd0a983cd2cf74f27df2e5a5c80f1794a2ed7ef,Bruce MacDonald,2025-03-14T23:56:32Z,"model: support for mistral-small in the ollama runner  Mistral is a popular research lab making open source models. This updates the forward pass of llama architecture models to support both llama models and mistral models by accounting for additional metadata present in mistral models, and finding the correct dimensions for the output projection.",1116,350,1466
ollama/ollama,1861fbdeb51fc3ca306c3f1e1bb820fc3986f508,Michael Yang,2025-04-03T21:05:21Z,Merge pull request #9873 from ollama/mxyng/fs-config  fs: move ml.Config to fs package,55,40,95
ollama/ollama,3b96a93672377129f2a2aafc447e79ef1ca48c5f,Michael Yang,2025-03-18T21:38:44Z,fs: move ml.Config to fs package,55,40,95
ollama/ollama,e53b3cbd0c3f08eb692a318c8eaf687a01c2e8c0,Bruce MacDonald,2025-04-03T17:19:24Z,"llm: set done reason at server level (#9830)  No functional change. Many different done reasons can be set at the runner level, so rather than obsuring them we should return them to the server process and let it choose what to do with the done reason. This separates the API concerns from the runner.",54,42,96
ollama/ollama,b51e0f397ced70bbfa7f22e9b3c94953967cb8e5,Jeffrey Morgan,2025-04-02T20:22:56Z,model: fix issues with spm tokenizer for Gemma 3 (#10081),171,109,280
ollama/ollama,b42970063d8f05c47dd6d9a6b71f1e14cc4805c9,jmorganca,2025-03-30T23:05:40Z,"kvcache: Add check for values that fall out of sliding window cache  The sliding window cache trims entries that are outside the window for the latest token. This works when we are extending the cache, such as when the conversation continues. However, if we have a partial overlap in conversation (including the BOS tokens), then we resume from a past point in the conversation and the needed tokens are no longer stored in memory. This verifies that the new window overlaps with the old one before reusing the cache.  Co-authored-by: Jesse Gross <jesse@ollama.com>",131,2,133
ollama/ollama,493385eb3e811ebbb49c6a23d6db7c39885bbb89,Jesse Gross,2025-04-01T22:01:23Z,"ollamarunner: Don't truncate a SameBatch  When truncating inputs to the the context window at the beginning of a sequence, we remove the minimum amount possible. However, this may cause us to truncate to the middle of a set of inputs that the model specified should not be split up. To avoid this, we need to remove the rest of the partial batch.",31,4,35
ollama/ollama,9876c9faa41c7dd7143fa47727520d353559f81b,Bruce MacDonald,2025-04-02T16:44:27Z,chore(all): replace instances of interface with any (#10067)  Both interface{} and any (which is just an alias for interface{} introduced in Go 1.18) represent the empty interface that all types satisfy.,58,58,116
ollama/ollama,4e415029b30b2dc8a666491fdbe6254536e5d810,IsAurora6,2025-04-02T08:27:16Z,readme: add Casibase to community integrations (#10057),1,0,1
ollama/ollama,e172f095ba4af2c98d7744ce4ffcf4cd3a8e123c,Bruce MacDonald,2025-04-01T22:21:46Z,api: return model capabilities from the show endpoint  (#10066)  With support for multimodal models becoming more varied and common it is important for clients to be able to easily see what capabilities a model has. Retuning these from the show endpoint will allow clients to easily see what a model can do.,521,69,590
ollama/ollama,c001b98087e45b7b60509127d4d2e9d9ba809444,Ilian,2025-04-01T00:28:59Z,docs: add TagSpaces to community integrations (#9983),1,0,1
ollama/ollama,23fc8e92eb01ddd1cf06b34ff270926ec7edd4b8,Abyss-c0re,2025-04-01T00:23:04Z,docs: add DeepShell to community projects (#9955)  Co-authored-by: Bruce MacDonald <brucewmacdonald@gmail.com>,1,0,1
ollama/ollama,4059a297a6d95ce94f3619eac0536fda666d58f1,湛露先生,2025-04-01T00:07:42Z,discover: /proc/cpuinfo file open and close. (#9950)  Signed-off-by: zhanluxianshen <zhanluxianshen@163.com>,2,3,5
ollama/ollama,66b253923891d41a31d28531e9db5efccf53e1d0,Bruce MacDonald,2025-03-31T19:54:45Z,"runner: clear cache when shift is not possible (#9433)  Clear KV cache when shift operation is not supported by model. Added KvCacheCanShift() check to handle models that can't perform cache shifts, falling back to full cache clear while preserving logical token history to maintain expected behavior when context window fills up.",179,13,192
ollama/ollama,ef27d52e7957e00fe664e7dc73cff2714f85468f,Blake Mizerany,2025-03-31T06:54:54Z,"server/internal/client/ollama: cache completed chunks (#9933)  This change adds tracking of download chunks during the pull process so that subsequent pulls can skip downloading already completed chunks. This works across restarts of ollama.  Currently, download state will be lost if a prune is triggered during a pull (e.g. restart or remove). This issue should be addressed in a follow-up PR.",437,297,734
ollama/ollama,b2a465296d7131ca440fd81c1bee888f4103a585,Jesse Gross,2025-03-15T00:24:46Z,"runner: Release semaphore and improve error messages on failures  If we have an error after creating a new sequence but before finding a slot for it, we return without releasing the semaphore. This reduces our parallel sequences and eventually leads to deadlock.  In practice this should never happen because once we have acquired the semaphore, we should always be able to find a slot. However, the code is clearly not correct.",9,3,12
ollama/ollama,5d097277ef8b08c86f354b54596976869998257d,Jesse Gross,2025-03-27T21:00:05Z,"ollamarunner: Ensure batch size limits are not exceeded  With the llama runner, we can generate up to NUM_PARALLEL batches at once, which will then get broken up to into individual batches to get executed by llama.cpp (i.e. we add up to 2048 tokens and this gets split into 4 batches of 512 tokens at default settings).  This splitting can improve parallelism on multi-GPU systems because the individual batches can move though the pipeline without blocking on the first one to fully complete. However, we don't yet support this in the Ollama runner, partially because it makes it hard to enforce model-specified batch constraints, which didn't exist previously.  The result is that we will try to execute the full, unsplit batch. This could result in out of memory or insufficient KV cache space errors.  This triggers batch breaking when the total inputs from all sequences exceeds the batch size, rather than per-sequence. In order to ensure fairness, it also reintroduces round-robinning around sequences so that we don't let one busy sequence starve the others.",27,6,33
ollama/ollama,071a9872cb76f07d09dc8a3c65046d35d921f4e6,Leandro Borges Ferreira,2025-03-31T00:28:06Z,readme: add Writeopia to community integrations (#10042),1,0,1
ollama/ollama,0bd0454ea7f51b1ddf527b4ebdaf1e614df211bc,CYJiang,2025-03-28T18:50:22Z,server: organize error types (#9465)  Co-authored-by: Bruce MacDonald <brucewmacdonald@gmail.com>,10,9,19
ollama/ollama,01aa7887221e7bd286ebcb14a088c94ba1c22a99,Jesse Gross,2025-03-27T18:52:09Z,"ml: Remove Output from Context interface  Model implementations should use Input for all of their tensors supplied to the model. This includes tensors that relate to the outputs, which is confusing since there is also an Output funciton.  Since Output is only used internally in GGML and not used by any model implementations, we can remove it from the interface to reduce confusion.",3,23,26
ollama/ollama,ead27aa9fe85b4a1e1c434080d5e005e3cd68a16,saman-amd,2025-03-27T14:35:19Z,Add gfx1200 & gfx1201 support on linux (#9878),120,11,131
ollama/ollama,b816ff86c923e0290f58f2275e831fc17c29ba37,Parth Sareen,2025-03-27T00:34:18Z,docs: make context length faq readable (#10006),7,1,8
ollama/ollama,e5d84fb90b21d71f8eb816656ca0b34191425216,molbal,2025-03-26T20:39:01Z,docs: add molbal/orca-cli to community integrations (#9909),1,0,1
ollama/ollama,dd66712e3159161c1de9c39a12fb83edf8813d39,Hengky Steen,2025-03-26T20:38:05Z,docs: add ollamb to community projects,1,0,1
ollama/ollama,f66216e3990b73869341c58ac9561b26c468c558,Jesse Gross,2025-03-24T20:39:07Z,"ggml: Support heterogeneous KV cache layer sizes in memory estimation  Gemma3 uses sliding windows for its context on 5/6 layers, significantly reducing memory usage but leading to uneven usage across layers, which makes allocation to the correct GPU difficult. We currently estimate very conservatively by assuming all layers are consistent at the max size.  Llama3.2-vision is also inconsistent between self attention and cross attention layers - at moment, we calculate the correct total size and then average this across layers. In some cases, this may lead to crashes if a large layer is placed on a GPU sized by the average.  This allows memory estimation to calculate per-layer KV cache size and take this account when placing layers onto GPUs. We already do this for weights that vary per-tensor, so this is a logical extension.  Fixes #9730 Fixes #9890",49,28,77
ollama/ollama,f4f0992b6ea5d651eff609461c24ece936bd5708,Jesse Gross,2025-03-25T18:41:26Z,llm: Fix debug logging for memory estimates,1,1,2
ollama/ollama,1feff619779115d76f033eb59a7a896aad6c2e18,Jesse Gross,2025-03-25T04:17:53Z,"kvcache: Sliding window cache only needs a single batch total  When computing the size of the cache for sliding window attention, we don't need to multiple the batch size by the number of parallel sequences - the batch size is constant.  This also simplifies the check for whether to allocate the cache size based on capacity or window size as the batch size is already incorporated into the capacity when handled by the runner.",2,2,4
ollama/ollama,5e0b904e887fc648fb8a3a55283f8f33063a78eb,copeland3300,2025-03-25T16:52:23Z,docs: add flags to example linux log output command (#9852),1,1,2
ollama/ollama,131f0355a59f4840b057fb8f3c2e59e456f91041,Matheus C. França,2025-03-24T16:25:58Z,readme: add ollama-d library (#9907),1,0,1
ollama/ollama,ce929984a33230269905e0e3cfa335cb8d6ba781,Blake Mizerany,2025-03-21T23:16:38Z,"server/internal/client/ollama: fix file descriptor management in Pull (#9931)  Close chunked writers as soon as downloads complete, rather than deferring closure until Pull exits. This prevents exhausting file descriptors when pulling many layers.  Instead of unbounded defers, use a WaitGroup and background goroutine to close each chunked writer as soon as its downloads finish.  Also rename 'total' to 'received' for clarity.",31,25,56
ollama/ollama,4b34930a31ceb9cc10d95b8bcd60c319f47d8043,Michael Yang,2025-03-21T21:47:13Z,Merge pull request #9897 from ollama/mxyng/chunk-load  ml/backend/ggml: load tensors in 128KiB chunks,58,30,88
ollama/ollama,74bd09652d69c77a4bed34b3afda74c87295115b,Michael Yang,2025-03-19T20:03:16Z,ml/backend/ggml: load tensors in 32KiB chunks,58,30,88

sha,author_login,date,message,additions,deletions,total_changes,commit_count,total_prs,merged_prs,pr_acceptance_rate
73869f2e81467db8422cbb4831cce9a7bdc85c4b,Tavish9,2025-07-17 15:47:31+00:00,"Fix typing order (#39467)

* fix type order

* change all Union[str, dict] to Union[dict, str]

* add hf_parser test && fix test order

* add deepspeed dependency

* replace deepspeed with accelerator",32,25,57,1,3,3,1.0
bda75b4011239d065de84aa3e744b67ebfa7b245,hellopahe,2025-07-17 15:07:12+00:00,"Add unified logits_to_keep support to LLMClass (#39472)

* add supports for logits_to_keep for qwen25vl and glm4v

* Update relevant modular files",20,4,24,1,1,1,1.0
bf6c9976851627b545f071d4861c54cda7fdfe6b,gante,2025-07-17 14:29:57+00:00,"[serve] Add speech to text (`/v1/audio/transcriptions`) (#39434)

* Scaffolding

* Explicit content

* Naïve Responses API streaming implementation

* Cleanup

* Scaffolding

* Explicit content

* Naïve Responses API streaming implementation

* Cleanup

* use openai

* validate request, including detecting unused fields

* dict indexing

* dict var access

* tmp commit (tests failing)

* add slow

* use oai output type in completions

* (little rebase errors)

* working spec?

* guard type hint

* type hints. fix state (CB can now load different models)

* type hints; fn names; error type

* add docstrings

* responses + kv cache

* metadata support; fix kv cache; error event

* add output_index and content_index

* docstrings

* add test_build_response_event

* docs/comments

* gate test requirements; terminate cb manager on model switch

* nasty type hints

* more type hints

* disable validation by default; enable force models

* todo

* experiment: base model from typed dict

* audio working

* fix bad rebase

* load audio with librosa

* implement timed models

* almost working

* make fixup

* fix tests

* transcription request type

* tokenizer -> processor

* add example in docs

---------

Co-authored-by: Lysandre <hi@lysand.re>",355,104,459,10,616,539,0.88
8b3de61a65241d3c81902bc47902dc4a6033da94,zhaiji0727,2025-07-17 13:57:49+00:00,"Update integration_utils.py (#39469)

* Update integration_utils.py

sanitize mlflow upload metric

* Update integration_utils.py

change import order to pass CI

* Update integration_utils.py

add comments

* Update integration_utils.py

Remove whitespace from blank line",6,2,8,1,1,1,1.0
7fd60047c8fac34d15cfd9044737dce536afe576,peteryschneider,2025-07-17 13:23:29+00:00,"fix: ImageTextToTextPipeline handles user-defined generation_config (#39374)

fix: ImageTextToTextPipeline handles user-defined generation_config passed to the pipeline

Co-authored-by: Raushan Turganbay <raushan@huggingface.co>",5,0,5,1,1,1,1.0
60b5471da3c3d59b8695b9969817267179120add,cyyever,2025-07-17 13:21:59+00:00,"Enable some ruff checks for performance and readability (#39383)

* Fix inefficient sequence tests

Signed-off-by: cyy <cyyever@outlook.com>

* Enable PERF102

Signed-off-by: cyy <cyyever@outlook.com>

* Enable PLC1802

Signed-off-by: cyy <cyyever@outlook.com>

* Enable PLC0208

Signed-off-by: cyy <cyyever@outlook.com>

---------

Signed-off-by: cyy <cyyever@outlook.com>",40,40,80,5,56,43,0.77
fc700c2a26af9e1b27162d408e8edfa2903c715f,Stonepia,2025-07-17 13:12:32+00:00,"Fix convert_and_export_with_cache failures for GPU models (#38976)

* Add the `device` option for `generate()`

* Add device for default tensors to avoid tensor mismatch

* [test] Enable test_static_cache_exportability for torch_device

* infer device from the prompt_token_ids

* Add device for generated tensor

* [Test] Make `test_export_static_cache` tests to run on devices rather than only CPU

* fix format

* infer device from the model",57,27,84,1,1,1,1.0
54680d75c9ed32ca7d7b0266d0563dcdd6fd40e3,ydshieh,2025-07-17 13:06:23+00:00,"Update `GemmaIntegrationTest::test_model_2b_bf16_dola` (#39362)

fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",1,1,2,32,1020,916,0.9
322400af58794ace3b3f97ecf927570b8c464bfc,klimarissa17,2025-07-17 13:06:04+00:00,fix a comment typo in utils.py (#39459),1,1,2,1,1,1,1.0
43f07018cff3c305e9d3a3782e3a95100800be4c,cyyever,2025-07-17 13:05:21+00:00,"Use newer typing notation (#38934)

Signed-off-by: cyy <cyyever@outlook.com>",2176,2263,4439,5,56,43,0.77
565dd0bad74a46d85c41e2d870f803d9e7a1a94e,SunMarc,2025-07-17 12:51:50+00:00,"Fix tests due to breaking change in accelerate (#39451)

* update values

* fix",6,6,12,4,135,119,0.88
26fed50460a8843399b58a23143a9e3f8a6f5aca,KKZ20,2025-07-17 08:54:23+00:00,fix max_length calculating using cu_seq_lens (#39341),3,1,4,1,1,1,1.0
cdfe6164b32d52f5d2125a29f0762e18ec5ac708,yushi2006,2025-07-17 08:24:30+00:00,"fix(pipelines): QA pipeline returns fewer than top_k results in batch mode (#39193)

* fixing the bug

* Try a simpler approach

* make fixup

---------

Co-authored-by: Matt <rocketknight1@gmail.com>",26,10,36,1,6,4,0.67
b85ed49e0a5f1bd9fd887f497d055b22b9319a12,renet10,2025-07-16 21:13:07+00:00,"Corrections to PR #38642 and enhancements to Wav2Vec2Processor __call__ and pad docstrings (#38822)

* Correcting PR #38642.  The PR removed references to the deprecated method ""as_target_processor()"" in the
__call__ and pad method docstrings, which is correct, but also removed all references to PreTrainedTokenizer,
which is incorrect.  This commit adds back the reference to PreTrainedTokenizer and also takes the
opportunity to enhance the docstrings with the invocation procedure post removal of ""as_target_processor()""
and adds information on return values.

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

---------

Co-authored-by: René Tio <tor@Jammer.local>
Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",23,5,28,1,1,1,1.0
787a0128a92926c615b29abfa8dec40087c1d875,dhruvmalik007,2025-07-16 19:40:22+00:00,"create ijepa modelcard (ref : PR  #36979 ). (#39354)

* wip: adding first version of the IJEPA model card.

* refactor based on the @stevhliu feedbacks

* refactor:
- revert the accidental removal of the autodoc api description and the image reerece architecture

- general context updation.

* - changes of model for example quantization.
- merging the  quantization content.",80,34,114,1,1,1,1.0
48f2233cdfdf49551f689268ad82a9eba7d9345b,ridima11,2025-07-16 19:15:15+00:00,Improve grammar and clarity in perf_hardware.md (#39428),3,3,6,1,2,1,0.5
e68ebb695f9d1d990462397e284e79d8729aafea,hiyouga,2025-07-16 16:02:26+00:00,"fix cached file error when repo type is dataset (#36909)

* fix cached file

* Update hub.py",10,3,13,1,16,13,0.81
35a416c4005ec65c73291d87bc478adedf70e251,Krish0909,2025-07-16 15:59:28+00:00,"Fix indentation bug in SmolVLM image processor causing KeyError (#39452)

Fix indentation bug in Idefics3 image processor

- Fix KeyError when do_image_splitting=False
- Move split_images_grouped assignment inside loop
- Ensures all image shapes are stored, not just the last one
- This fixes the bug in both Idefics3 and generated SmolVLM processors

cc @yonigozlan

Co-authored-by: Krishnan Vignesh <krishnanvignesh@Krishnans-MacBook-Air.local>",2,2,4,1,4,1,0.25
2c58705dc23ce869e82b1a6ca225ad718916e8d5,LckyLke,2025-07-16 15:54:29+00:00,"Updated Megatron conversion script for gpt2 checkpoints  (#38969)

* update script to support new megatron gpt format

* fixed quality failures

---------

Co-authored-by: Luke Friedrichs <LckyLke>",85,13,98,1,1,1,1.0
26be7f717e12241df3d575d825bfa51026da9be8,vasqu,2025-07-16 13:53:43+00:00,"[`CI`] Fix partially red CI (#39448)

fix",1,2,3,5,41,32,0.78
0a88751940a3c2c61ddd8e1dcca21fe9b3106f00,sebastianvlad1,2025-07-16 13:51:30+00:00,"Fixes #39204: add fallback if get_base_model missing (#39226)

* Fixes #39204: add fallback if get_base_model missing

* Inline try_get_base_model logic as suggested in PR review

* Apply style fixes

---------

Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",6,1,7,1,1,1,1.0
ba506f87db36ce916c59ace15cb77d9cdd662c53,winglian,2025-07-16 13:47:24+00:00,make the loss context manager easier to extend (#39321),7,1,8,3,45,27,0.6
9f1ac6f1859a650f8491af864d36f88107279bdf,ArthurZucker,2025-07-16 13:22:44+00:00,"Remove something that should have never been there (#38254)

* what the hell

* update

* style

* style

* typing

* fix init issue

* fix granite moe hybrid as well",102,16,118,4,452,371,0.82
a7ca5b5d67d8627ab40d57874889715fc083d844,zucchini-nlp,2025-07-16 13:01:35+00:00,"Fix processor tests (#39450)

fix",6,0,6,20,298,253,0.85
71818f570bb5346b2431ea9fae580ca3ee359390,kylesayrs,2025-07-16 12:57:42+00:00,"[Bugfix] [Quantization] Remove unused init arg (#39324)

remove unused arg from ct config init

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>",0,1,1,4,15,13,0.87
cc24b0378e6062895a03a077860be28d290c1d07,qubvel,2025-07-16 12:50:35+00:00,"Better typing for model.config (#39132)

* Apply to all models config annotation

* Update modular to preserve order

* Apply modular

* fix define docstring

* fix dinov2 consistency (docs<->modular)

* fix InstructBlipVideoForConditionalGeneration docs<->modular consistency

* fixup

* remove duplicate code

* Delete config_class attribute from the modeling code

* Add config_class attribute in base model

* Update init sub class

* Deprecated models update

* Update new models

* Fix remote code BC issue

* fixup

* fixing more corner cases

* fix new models

* add test

* modular docs update

* fix comment a bit

* fix for py3.9",630,585,1215,5,88,71,0.81
4b258454a7f2fd446b95f51bea7bf303d8313cd1,thisisiron,2025-07-16 12:28:02+00:00,"Fix typo in generation configuration for Janus model weight conversion (#39432)

* Fix typo in generation configuration for Janus model weight conversion

* Fix typo

* Update Janus model generation configuration

* Update Janus model to use generation_kwargs",3,0,3,1,8,4,0.5
de5ca373acefc3c5cfc99aa697e7a073f7a2de23,LysandreJik,2025-07-16 12:16:16+00:00,"Responses API in `transformers serve` (#39155)

* Scaffolding

* Explicit content

* Naïve Responses API streaming implementation

* Cleanup

* Responses API (to be merged into #39155) (#39338)

* Scaffolding

* Explicit content

* Naïve Responses API streaming implementation

* Cleanup

* use openai

* validate request, including detecting unused fields

* dict indexing

* dict var access

* tmp commit (tests failing)

* add slow

* use oai output type in completions

* (little rebase errors)

* working spec?

* guard type hint

* type hints. fix state (CB can now load different models)

* type hints; fn names; error type

* add docstrings

* responses + kv cache

* metadata support; fix kv cache; error event

* add output_index and content_index

* docstrings

* add test_build_response_event

* docs/comments

* gate test requirements; terminate cb manager on model switch

* nasty type hints

* more type hints

* disable validation by default; enable force models

* todo

---------

Co-authored-by: Lysandre <hi@lysand.re>

* Slight bugfixes

* PR comments from #39338

* make fixup

---------

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>
Co-authored-by: Joao Gante <joao@huggingface.co>",948,391,1339,6,555,484,0.87
c8524aeb07f370195733f610760236d22e88cb61,zucchini-nlp,2025-07-16 12:00:17+00:00,"[cache] make all classes cache compatible finally (#38635)

* dump

* push other models

* fix simple greedy generation

* xmod

* add fmst and clean up some mentions of old cache format

* gpt-bigcode now follows standards

* delete tuple cache reference in generation

* fix some models

* fix some models

* fix mambas and support cache in tapas

* fix some more tests

* fix copies

* delete `_reorder_cache`

* another fix copies

* fix typos and delete unnecessary test

* fix rag generate, needs special cache reordering

* fix tapas and superglue

* reformer create special cache

* recurrent gemma `reorder_cache` was a no-op, delete

* fix-copies

* fix blio and musicgen pipeline tests

* fix reformer

* fix reformer, again...

* delete `_supports_cache_class`

* delete `supports_quantized_cache`

* fix failing tests

* fix copies

* some minor clean up

* style

* style

* fix copies

* fix tests

* fix copies

* create causal mask now needs positions?

* fixc copies

* style

* Update tests/test_modeling_common.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* clean-up of non-generative model after merging main

* check `is_decoder` for cache

* delete transpose for scores

* remove tuple cache from docs everywhere

* fix tests

* fix copies

* fix copies once more

* properly deprecate `encoder_attention_mask` in Bert-like models

* import `deprecate_kwarg` where needed

* fix copies again

* fix copies

* delete `nex_decoder_cache`

* fix copies asks to update for PLM

* fix copies

* rebasing had a few new models, fix them and merge asap!

* fix copies once more

* fix slow tests

* fix tests and updare PLM checkpoint

* add read token and revert accidentally removed line

* oh com -on, style

* just skip it, read token has no access to PLM yet

---------

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",5715,6839,12554,20,298,253,0.85
6cb43defd084586df707fb85f6ab72bc800212ad,IliasAarab,2025-07-16 11:57:13+00:00,"docs: add missing numpy import to minimal example (#39444)

docs: add numpy import to minimal example",1,0,1,1,1,1,1.0
61163099f1b95e9f8960f6382bff4aa31db52fc9,cyyever,2025-07-16 11:36:48+00:00,"Remove runtime conditions for type checking (#37340)

Remove dynamic conditions for type checking

Signed-off-by: cyy <cyyever@outlook.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",167,236,403,5,56,43,0.77
bfc9ddf5c6243c8f8a9615051436a70078f73943,SunMarc,2025-07-16 11:35:53+00:00,"Add StableAdamW Optimizer  (#39446)

* Added StableAdamW as an optimizer option for Trainer. Also wrote tests to verify its behaviour.

* Fixed issue with

* Added docs for StableAdamW. Also fixed a typo in schedule free optimizers

---------

Co-authored-by: Gautham Krithiwas <gauthamkrithiwas2003@gmail.com>",187,1,188,4,135,119,0.88
b9ee5282464acdef0cb275bd05456af8dcfa8a62,molbap,2025-07-16 10:45:46+00:00,"add test scanner (#39419)

* add test scanner

* add doc + license

* refactor for only 1 tree traversal

* add back test of only one method

* document single method scan

* format

* fixup generate tests

* minor fix

* fixup

* fixup doc",310,0,310,2,55,43,0.78
79941c61ce754ac2b0e5c74f25c08af4afaef6ed,ahadnagy,2025-07-16 10:09:18+00:00,"Fix missing definition of diff_file_url in notification service (#39445)

Fix missing definition of diff_file_url",1,1,2,2,11,7,0.64
e048d48bd0f87a0331020f55966e715faf0671d4,richardodliu,2025-07-16 10:01:08+00:00,"Add cosine_with_min_lr_schedule_with_warmup_lr_rate scheduler in Trainer (#31870)

* add cosine_with_min_lr_schedule_with_warmup_lr_rate scheduler in trainer

* Update src/transformers/optimization.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update optimization.py

fix the error of the unclosed ""(""

* Update optimization.py

remove whitespace in line 402 in order to pass the quality test

* Update src/transformers/optimization.py

* Update src/transformers/optimization.py

* Apply style fixes

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",111,0,111,1,1,1,1.0
0cf08e90ddf4433a2f7d29053628a61607c2dc01,qgallouedec,2025-07-16 09:54:20+00:00,"Change log level from warning to info for scheduled request logging in `ContinuousBatchProcessor` (#39372)

Change log level from warning to info for scheduled request logging in ContinuousBatchProcessor",1,1,2,1,21,17,0.81
ae4e306a4073f72f4ad0c44aaa4c273006ef5ebe,cyyever,2025-07-16 09:52:33+00:00,"Defaults to adamw_torch_fused for  Pytorch>=2.8 (#37358)

* Defaults to adamw_torch_fused for latest Pytorch

Signed-off-by: cyy <cyyever@outlook.com>

* Fix test

Signed-off-by: cyy <cyyever@outlook.com>

---------

Signed-off-by: cyy <cyyever@outlook.com>",9,7,16,5,56,43,0.77
4524a68c66db404e9dd999931eb687903f63a66d,wjdghks950,2025-07-16 09:45:58+00:00,"Fix L270 - hasattr(""moe_args"") returning False error (#38715)

* Fix L270 - hasattr(""moe_args"") returning False error

* Update src/transformers/models/llama4/convert_llama4_weights_to_hf.py

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",1,1,2,1,1,1,1.0
d33a1c389f737a87471c64138be30bf5c59b5e8a,zucchini-nlp,2025-07-16 09:31:35+00:00,"[chat template] add a testcase for kwargs (#39415)

add a testcase",39,0,39,20,298,253,0.85
99c9763398dde67554e4ae051794c6f27de0a87f,Phoenix-Shen,2025-07-16 09:22:00+00:00,"Fixed a bug calculating cross entropy loss in `JetMoeForCausalLM` (#37830)

fix: :bug: Fixed a bug in calculating Cross Entropy loss in JetMoeForCausalLM

In the original code, we shift the logits and pass shift_logits into the self.loss_function, but in self.loss_function, the shift_logits will be shifted again, so we are actually doing ""next next token prediction"", which is incorrect. I have removed the logits shifting before calling self.loss_function.

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",2,12,14,1,2,1,0.5
667ad023743421be186ab2715e930c226f8fb112,rudolfwilliam,2025-07-16 09:20:23+00:00,"Remove double soft-max in load-balancing loss. Fixes #39055 . (#39056)

Remove double soft-max in load-balancing loss. Fixes #39055",6,8,14,1,1,1,1.0
31d81943c994d11a079223809cfca89bfaaee363,kylesayrs,2025-07-16 08:44:40+00:00,"[Core] [Offloading] Fix saving offloaded submodules (#39280)

* fix counting meta tensors, fix onloading meta tensors

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>

* remove unrelated fix

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>

* remove unrelated change

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>

* add clarifying comment

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>

* add test_save_offloaded_model_with_direct_params

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>

* fix merge conflict, add decorators

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>

---------

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>",52,6,58,4,15,13,0.87
add43c4d09a89e3ad134efc8fd119350f363642e,zucchini-nlp,2025-07-16 07:41:50+00:00,"[autodocstring] add video and audio inputs (#39420)

* add  video and audio inputs in auto docstring

* fix copies",64,437,501,20,298,253,0.85
0dc2df5ddafe3cb5824ad24e85beba13e0aa6726,ahadnagy,2025-07-16 02:20:02+00:00,"CI workflow for performed test regressions (#39198)

* WIP script to compare test runs for models

* Update line normalitzation logic

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",183,13,196,2,11,7,0.64
1bc9ac5107ff32c0115bd0b269924455be79db64,sbucaille,2025-07-15 19:40:50+00:00,"docs: update LightGlue docs (#39407)

* docs: update LightGlue docs

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",81,43,124,6,16,11,0.69
d9574f2fe3b1f365340f8d3a6d30b48d736869b8,sbucaille,2025-07-15 19:40:26+00:00,"docs: update SuperGlue docs (#39406)

* docs: update SuperGlue docs

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",106,80,186,6,16,11,0.69
9f41f67135b0656c428ff2c2b446d8eb15f5a7c5,zucchini-nlp,2025-07-15 15:23:54+00:00,"[vlm] fix loading of retrieval VLMs (#39242)

* fix vlm with retrieval

* we can't use AutoModel because new ColQwen was released after refactor

* no need for colqwen

* tied weight keys are necessary, if using IMageTextToText

* need to apply renaming in tied weights, only for ColPali

* overwrite tied keys in ColPali

* fix copies, modular can't handle if-statements",67,24,91,20,298,253,0.85
b1d14086e4bfb3be4417fcac092936231ab74ec2,winglian,2025-07-15 15:21:15+00:00,"handle training summary when creating modelcard but offline mode is set (#37095)

* handle training summary when creating modelcard but offline mode is set

* chore: lint",7,1,8,3,45,27,0.6
67f42928f0ec97a4635e7ff52a4b5e7879590c1c,DWarez,2025-07-15 15:16:10+00:00,"Remove residual quantization attribute from dequantized models (#39373)

* fix: removing quantization trace attribute from dequantized model

Fixes #39295

* add: test `to(dtype=torch.float16)` after dequantization",28,0,28,1,2,1,0.5
30c508dbcbc2d4e19507da5af46a32a7de73c5e9,jiangwangyi,2025-07-15 14:02:25+00:00,"Remove deprecated audio utils functions (#39330)

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",0,143,143,1,4,2,0.5
d8e05951b8efd4880acca9a3f291e8b65841a86d,HRezaei,2025-07-15 13:37:28+00:00,Fix bugs in pytorch example run_clm when streaming is enabled (#39286),117,41,158,1,2,1,0.5
a989bf8d84bacd00fa13db8e7031b139bcddd622,Rocketknight1,2025-07-15 13:28:59+00:00,"Fix bugs from pipeline preprocessor overhaul (#39425)

* Correct load classes for VideoClassificationPipeline

* Correct load classes for the ASR pipeline",3,2,5,4,398,351,0.88
53c9dcd6fd31cb9e8a10248693a905d0223b8316,McPatate,2025-07-15 12:22:12+00:00,refactor: remove `set_tracer_provider` and `set_meter_provider` calls (#39422),1,19,20,1,16,12,0.75
f03b3841494575b92a5350b4150e4f25faeab8d2,cyyever,2025-07-15 12:11:37+00:00,"Fix invalid property (#39384)

Signed-off-by: cyy <cyyever@outlook.com>",10,10,20,5,56,43,0.77
c4d41567fa4ed03123580d4168c25a0a328119d1,jiqing-feng,2025-07-15 12:05:49+00:00,"set document_question_answering pipeline _load_tokenizer to True (#39411)

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>",1,1,2,6,67,54,0.81
f56b49f48f58ae0dbbcb164a4b4ebb137f015473,Rocketknight1,2025-07-15 11:57:32+00:00,"Ignore extra position embeddings weights for ESM (#39063)

* Ignore extra position embeddings weights

* Slight name fix",1,0,1,4,398,351,0.88
2b79f143756bf10a2d43e5ab6d413374b633d2ce,44670,2025-07-15 09:53:41+00:00,"support loading qwen3 gguf (#38645)

* support loading qwen3 gguf

* Add qwen3 into GGUF_TO_FAST_CONVERTERS for tokenizer conversion

* Add testcase

* Fix formatting",31,0,31,1,1,1,1.0
0e4b7938d0e965362973797f47ad2b85f605a96a,orionw,2025-07-15 08:40:41+00:00,"Add ModernBERT Decoder Models - ModernBERT, but trained with CLM! (#38967)

* working locally; need to style and test

* added docs and initial tests; need to debug and flesh out

* fixed tests

* working long context; batches

* working fa2 and eager

* update tests

* add missing confnigs

* remove default autoset

* fix spacing

* fix most tests

* fixed tests

* fix to init

* refactor to match new transformers updates

* remove static cache option

* fa2 fix

* fix docs

* in progress

* working on tests

* fixed issue with attn outputs

* remove debug

* fix local config attr

* update doc string

* fix docstring

* add docs to toc

* correct typo in toc

* add new updates from main w.r.t. ModernBERT RoPE

* fix local param

---------

Co-authored-by: oweller2 <oweller2@dsailogin.mgmt.ai.cluster>
Co-authored-by: oweller2 <oweller2@l07.mgmt.ai.cluster>
Co-authored-by: oweller2 <oweller2@n02.mgmt.ai.cluster>
Co-authored-by: oweller2 <oweller2@l08.mgmt.ai.cluster>
Co-authored-by: oweller2 <oweller2@l01.mgmt.ai.cluster>
Co-authored-by: oweller2 <oweller2@l02.mgmt.ai.cluster>",2020,0,2020,1,3,1,0.33
0b724114cf8475f146ca2fd644c4e31f395441eb,alvarobartt,2025-07-15 07:59:25+00:00,Fix typo in `/v1/models` output payload (#39414),1,1,2,1,5,5,1.0
8d6259b0b8290c2406949ce6342051b1f09a074c,zucchini-nlp,2025-07-15 07:34:06+00:00,"[refactor] set attention implementation (#38974)

* update

* fix some tests

* init from config, changes it in-place, add deepcopy in tests

* fix modernbert

* don't delete thsi config attr

* update

* style and copies

* skip tests in generation

* fix style

* accidentally removed flash-attn-3, revert

* docs

* forgot about flags set to False

* fix copies

* address a few comments

* fix copies

* custom code BC",451,776,1227,20,298,253,0.85
6017f5e8ed33d48096cdf8630d1cc7cbf2550c90,sameerajashyam,2025-07-14 17:47:19+00:00,"[siglip] fix pooling comment (#39378)

* feat(siglip2): add forward pass with pooled output logic in Siglip2TextModel

* test(siglip2): add test_text_model.py to verify pooled output behavior

* style(siglip2): fix formatting in test_text_model.py using Ruff

* fix(siglip2): remove misleading 'sticky EOS' comment and sync modular-classic files

* fix(siglip2): remove misleading 'sticky EOS' comment and sync modular-classic files

* chore(siglip2): regenerate classic model after modular change

* Update",2,2,4,1,6,1,0.17
8d40ca5749dff9e0dcdc18eba4165691fe951f42,Tanuj-rai,2025-07-14 17:35:17+00:00,"Update phi4_multimodal.md (#38830)

* Update phi4_multimodal.md

* Update docs/source/en/model_doc/phi4_multimodal.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/phi4_multimodal.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/phi4_multimodal.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/phi4_multimodal.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/phi4_multimodal.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update phi4_multimodal.md

* Update phi4_multimodal.md

* Update phi4_multimodal.md

* Update phi4_multimodal.md

* Update phi4_multimodal.md

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",48,22,70,2,10,4,0.4
3635415af2b6f6e0d6345849f07ba6da9af4206a,MilkClouds,2025-07-14 16:25:06+00:00,"[Docs] Fix typo in CustomTrainer compute_loss method and adjust loss reduction logic (#39391)

Fix typo in CustomTrainer compute_loss method and adjust loss reduction logic",2,2,4,1,2,2,1.0
3a48e9534cefb5937da33bba1fdc9d8b1b9a0d91,rasmi,2025-07-14 16:05:28+00:00,"Use np.pad instead of np.lib.pad. (#39346)

* Use np.pad instead of np.lib.pad.

* Update audio_utils.py

Formatting",1,3,4,2,5,4,0.8
3d8be20cd2f76aa03d3f42808f2c9b36c94608b3,Rocketknight1,2025-07-14 15:40:04+00:00,"Totally rewrite how pipelines load preprocessors (#38947)

* Totally rewrite how pipelines load preprocessors

* Delete more mappings

* Fix conditionals, thanks Cyril!",286,226,512,4,398,351,0.88
903944a411c35b8f7b2b51ada77a7c2a80e7fb88,eromomon,2025-07-14 10:16:49+00:00,"[examples] fix do_reduce_labels argument for run_semantic_segmentation_no_trainer (#39322)

* no use do_reduce_labels argument in model

* use do_reducer_labels in AutoImageProcessor",1,2,3,2,2,2,1.0
8165c703ab4284a189cef18dd7c8bad767318e75,Cyrilvallez,2025-07-14 10:02:59+00:00,"Fix Lfm2 and common tests (#39398)

* fix

* better fix

* typo",7,0,7,14,132,118,0.89
878d60a3cb072b60895eac8fcfd35798393420d9,zucchini-nlp,2025-07-14 09:42:06+00:00,"Deprecate AutoModelForVision2Seq (#38900)

deprecate vision2seq",23,2,25,20,298,253,0.85
ad333d4852b7b2f85fabe5dde1ce615816221b03,dsnsabari,2025-07-14 07:47:39+00:00,"[Qwen2.5-VL] Fix torch.finfo() TypeError for integer attention_mask_tensor (#39333)

* Update modeling_qwen2_5_vl.py

### 🐛 Bug Description

When using Unsloth’s Qwen2.5-VL vision models (both 3B and 7B) with the latest HuggingFace Transformers (commit: 520b9dcb42cef21662c304583368ff6645116a45), the model crashes due to a type mismatch in the attention mask handling.

---

### 🔥 Error Traceback

* Fix dtype compatibility in attention mask processing

Replace hardcoded torch.finfo() usage with dtype-aware function selection to handle both integer and floating-point attention mask tensors.
Technical Details:

Problem: Line 1292 assumes floating-point dtype for attention_mask_tensor
Solution: Add dtype check to use torch.iinfo() for integer types and torch.finfo() for float types
Files Modified: transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py

* Update modeling_qwen2_5_vl.py

* Update modeling_qwen2_5_vl.py

* Fix: Cast to float before applying torch.finfo

* # Fix: Use appropriate function based on dtype

* Update modular_qwen2_5_vl.py

* Fix: Cast to float before applying torch.finfo

* Fix: Use appropriate function based on dtype

* Fix: Use appropriate function based on dtype

* Updatet modeling_glm4v.py

* Only apply conversion for floating point tensors (inverted masks)

* corrected the format issue

reformatted modeling_glm4v.py

All done! ✨ 🍰 ✨
1 file reformatted

* Fix: Cast to float before applying torch.finfo

Corrected the format issue

* Fix torch.finfo() for integer attention mask

#39333

* Run make fix-copies and make style for CI compliance

- Updated dependency versions table
- Fixed code formatting and style issues
- Sorted auto mappings
- Updated documentation TOC

* Fix torch.finfo() TypeError for

Fix torch.finfo() TypeError for integer attention_mask_tensor #39333

* Fix torch.finfo() TypeError for integer",20,10,30,1,1,1,1.0
c30af65521e806eac11552d1fcd83d8e1461c07f,zucchini-nlp,2025-07-14 07:20:01+00:00,"[BLIP] remove cache from Qformer (#39335)

* remove cache from Qformer

* fix

* this was never correct...",82,165,247,20,298,253,0.85
66cd9956184f22ba43b975e5360e793eee87c146,zucchini-nlp,2025-07-14 06:34:58+00:00,"[shieldgemma] fix checkpoint loading (#39348)

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",11,7,18,20,298,253,0.85
a1ad9197c5756858e9014a0e01fe5fb1791efdf2,yonigozlan,2025-07-12 23:39:06+00:00,"Fix overriding Fast Image/Video Processors instance attributes affect other instances (#39363)

* fix and add tests

* nit",81,5,86,2,88,74,0.84
dc98fb3e5e1439896c03265ad84335b122ce4b27,ydshieh,2025-07-12 21:19:37+00:00,"update docker file to use latest `timm` (for `perception_lm`) (#39380)

update docker file for timm

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",2,0,2,32,1020,916,0.9
5c30f7e390429904ecf0749c2e9fd9a3f29cc714,ParagEkbote,2025-07-11 18:23:08+00:00,"Update Model Card for Encoder Decoder Model (#39272)

* update model card.

* add back the model contributors for mamba and mamba2.

* update the model card.

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* update batches with correct alignment.

* update examples and remove quantization example.

* update the examples.

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* update example.

* correct the example.

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",90,84,174,1,6,6,1.0
0d7efe3e4b7e057d105d90862cffc4f8cc125d1a,Xiang-cd,2025-07-11 17:59:41+00:00,"fix gpt2 usage doc (#39351)

fix typo of gpt2 doc usage",1,1,2,1,1,1,1.0
a646fd55fdd97427ad33c4ee17d41758b7cafa99,MShaheerMalik77,2025-07-11 17:59:09+00:00,"Updated CamemBERT model card to new standardized format (#39227)

* Updated CamemBERT model card to new standardized format

* Applied review suggestions for CamemBERT: restored API refs, added examples, badges, and attribution

* Updated CamemBERT usage examples, quantization, badges, and format

* Updated CamemBERT badges

* Fixed CLI Section",88,33,121,1,1,1,1.0
af74ec65a7d5a1fbe220164f0c3ece601c091114,eromomon,2025-07-11 17:58:26+00:00,"Update Readme to Run Multiple Choice Script from Example Directory (#39323)

* Update Readme to run in current place

* Update Readme files to execute PyTorch examples from their respective folders",11,11,22,2,2,2,1.0
70e57e4710d8a617a6f0ea73183d9bc4c91063c9,juliendenize,2025-07-11 16:26:58+00:00,"Add mistral common support (#38906)

* wip: correct docstrings

* Add mistral-common support.

* quality

* wip: add requested methods

* wip: fix tests

* wip: add internally some methods not being supported in mistral-common

* wip

* wip: add opencv dependency and update test list

* wip: add mistral-common to testing dependencies

* wip: revert some test changes

* wip: ci

* wip: ci

* clean

* check

* check

* check

* wip: add hf image format to apply_chat_template and return pixel_values

* wip: make mistral-common non-installed safe

* wip: clean zip

* fix: from_pretrained

* fix: path and base64

* fix: path and import root

* wip: add docs

* clean

* clean

* revert

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",3573,8,3581,1,2,2,1.0
665418dacc199d0c0825cdb0442de46b04e40e15,learning-chip,2025-07-11 14:59:51+00:00,"Remove device check in HQQ quantizer (#39299)

* Remove device check in HQQ quantizer

Fix https://github.com/huggingface/transformers/issues/38439

* Apply style fixes

---------

Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",1,4,5,1,1,1,1.0
601bea2c4efd58440f9a7399b0dfae164703338b,manueldeprada,2025-07-11 14:36:10+00:00,"Verbose error in fix mode for utils/check_docstrings.py (#38915)

* fix ast deprecations for python 3.14: replace node.n by node.value and use `ast.Constant`

More verbose exceptions in `fix_docstring` on docstring formatting issues.",19,4,23,2,20,12,0.6
24f771a043871a109d8a969bf92730746e085f3b,ydshieh,2025-07-11 14:30:56+00:00,"fix failing `test_sdpa_can_dispatch_on_flash` (#39259)

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",14,7,21,32,1020,916,0.9
ee74397d207cb6e5e698a558c2952f54935bd942,ArthurZucker,2025-07-11 13:54:25+00:00,"update cb TP (#39361)

* update cb TP

* safety",12,1,13,4,452,371,0.82
9bc675b3b6e6faddb90619a0f1fdc43cce943190,Cyrilvallez,2025-07-11 13:34:01+00:00,"Fix link for testpypi (#39360)

fix link",1,1,2,14,132,118,0.89
bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29,shuminghu,2025-07-11 09:07:32+00:00,"PerceptionLM (#37878)

* plm template

* A working plm with fixed image features

* hacked processor

* First version that reproduced PLM output using PE from timm.

* Simplify and fix tie_word_embeddings

* Use PIL resize. Simplify converstion.

* First version that works with video input.

* simplifed image preprocessing (not batched)

* Minor fixes after rebasing on main.

* Video processor based on new API.

* Revert to use _preprocess for image processor.

* refactor with modular

* fix tie_word_embedding

* Testing with timm PE

* check in missed converstion from modular to model.py

* First working version of PLM with Eva PE. PLM-1B and 3B outputs are exactly the same as before. PLM-8B output has some differences.

* address review comments

* Fixed batching if video and image examples mixed.

* Simplify PE configuration.

* Enable AutoModel for PerceptionEncoder.

* Update PE config style.

* update all headers

* Minor fixes.

* Move lm_head to PerceptionLMForConditionalGeneration.
Fix vit_G model specification.

* Fix for testing_modeling_perception_lm.py

* Image processing refactoring to use more common parts.

* Fix processor test.

* update tests to use model from hub

* More test fixes.

* integration test GT update after rebasing; probably due to video preprocessing

* update test media path to hub

* Stop tracking local scripts

* address some review comments

* refactor image processing.

* small fixes

* update documentation and minor fixes

* remove scripts

* Minor fix for CI

* Fix image processing

* CI and doc fix

* CI formatting fix

* ruff fix

* ruff formatting

* ran utils/sort_auto_mappings.py

* update docstring

* more docstring udpates

* add vision_input_type default fallback for image processing

* more verbose variable naming

* test update

* Remove PE and PEConfig use AutoModel(TimmWrapper) instead

* Minor cleanup.

* Minor Fix: remove any ref to PE. Ruff format and check.

* fix docstring

* Fix modular/model consistency.Improvex docstringfor  .

* Fix PerceptionLMForConditionalGenerationModelTest

* ruff fix

* fix for check_repo

* minor formatting

* dummy size arg to fix for processor test.

* Update docstring for PerceptionLMConfig

* Minor fixes from review feedback.

* Revert some minor changes per reviewer feedback.

* update base_model_prefix

* address reviewer feedback

* fix comment in modeling file

* address reviewer feedback

* ruff format

* Pre-merge test update.

* reapply modular and fix checkpoint name

* processor test path

* use modular a bit more

* remove dead code

* add token decorator

---------

Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>
Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",3262,1,3263,1,1,1,1.0
4b47b2b8ea9252e36669397243b770505f953d69,giuseppeCoccia,2025-07-10 22:34:10+00:00,"Updated Switch Transformers model card with standardized format (Issue #36979) (#39305)

* Updated Switch Transformers model card with standardized format (Issue #36979)

* Apply reviewer suggestions to the new standardised Switch Transformer's model card

* Update switch_transformers.md

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",73,18,91,1,1,1,1.0
fe1a5b73e62c54adafcedb47ac2430e058dc4312,qubvel,2025-07-10 18:07:59+00:00,"[modular] speedup check_modular_conversion with multiprocessing (#37456)

* Change topological sort to return level-based output (lists of lists)

* Update main for modular converter

* Update test

* update check_modular_conversion

* Update gitignore

* Fix missing conversion for glm4

* Update

* Fix error msg

* Fixup

* fix docstring

* update docs

* Add comment

* delete qwen3_moe",127,47,174,5,88,71,0.81
571a8c21313ef734e77cf9874ea0334d25bd7ff5,Cyrilvallez,2025-07-10 16:53:40+00:00,"Add a default value for `position_ids` in masking_utils (#39310)

* set default

* Update masking_utils.py

* add small test",11,6,17,14,132,118,0.89
bdc8028cb3efe6e33982eb8d297ffbb695606e84,kylesayrs,2025-07-10 16:33:30+00:00,"[Core] [Offloading] Enable saving offloaded models with multiple shared tensor groups (#39263)

* fix counting meta tensors, fix onloading meta tensors

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>

* remove unrelated fix

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>

* add test

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>

---------

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>",39,18,57,4,15,13,0.87
df49b399dc02a2375f0e9bd0e74c544247ab3976,gante,2025-07-10 15:40:08+00:00,"[tests] tag serve tests as slow  (#39343)

* maybe they need more cpu resources?

* add todo",2,0,2,10,616,539,0.88
36e80a18da4cde6efe2d9d43875b7cd2d50f18c7,paulpak58,2025-07-10 15:27:55+00:00,"[modeling][lfm2] LFM2: Remove deprecated seen_tokens (#39342)

* [modeling][lfm2] remove deprecated seen_tokens

* [modular][lfm2] remove deprecated seen_tokens from modular file",0,8,8,2,3,2,0.67
9682d07f92bffcc2d091a32cfbb3692884e7cacd,paulpak58,2025-07-10 14:07:33+00:00,"LFM2 (#39340)

* [modeling][lfm2] LFM2 model on 4.53.0 interface

* [configuration] hook in LFM2 keys

* [modeling][lfm2] update modeling interface for 4.53.1

* [modeling][lfm2] apply mask to hidden conv states

* [misc] ruff format/lint

* [modeling][lfm2] minor: NotImplemented legacy cache conversion

* Create lfm2.md

* create nice modular

* style

* Update modeling_auto.py

* clean and start adding tests

* style

* Update test_modeling_lfm2.py

* Update __init__.py

* small test model size

* config

* small fix

* fix

* remove useless config attrs -> block_dim and conv_dim are hiden_size

* fix prepare inputs

* fix config

* test

* typo

* skip tests accordingly

* config docstrings

* add doc to .md

* skip config docstring check

---------

Co-authored-by: Maxime Labonne <81252890+mlabonne@users.noreply.github.com>
Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",1645,6,1651,2,3,2,0.67
38c3931362e4d6548c586d217452a19b279d9bbe,gante,2025-07-10 13:41:38+00:00,"[server] add tests and fix passing a custom `generation_config` (#39230)

* add tests; fix passing a custom generation_config

* tool integration test

* add install step

* add accelerate as dep to serving

* add todo",320,53,373,10,616,539,0.88
6b09c8eab05820d480f4da97d23456428d410082,edwko,2025-07-10 10:36:58+00:00,"Handle DAC conversion when using weight_norm with newer PyTorch versions (#36393)

* Update convert_dac_checkpoint.py

* Update convert_dac_checkpoint.py

* Apply style fixes

---------

Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Co-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",34,1,35,1,1,1,1.0
92043bde294da59207816c40c7601fbc49e10d8d,ydshieh,2025-07-10 09:51:55+00:00,"fix `phi3` tests (#39312)

fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",12,2,14,32,1020,916,0.9
520b9dcb42cef21662c304583368ff6645116a45,Kuangdd01,2025-07-10 08:44:28+00:00,"fix Glm4v batch videos forward (#39172)

* changes for video

* update modular

* change get_video_features

* update video token replacement

* update modular

* add test and fix typo

* lint

* fix order

* lint

* fix

* remove dependency

* lint

* lint

* remove todo

* resize video for test

* lint..

* fix test

* new a processor for video_test

* fix test",127,23,150,1,2,2,1.0
bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2,zucchini-nlp,2025-07-10 05:18:44+00:00,"Delete deprecated stuff (#38838)

* delete deprecated stuff

* fix copies

* remove unused tests

* fix modernbert and fuyu

* Update src/transformers/cache_utils.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* bye bye `seen_tokens`

* address comments

* update typings

* ecnoder decoder models follow same pattern as whisper

* fix copies

* why is it set to False?

* fix switch transformers

* fix encoder decoder models shared weight

* fix copies and RAG

* remove `next_cache`

* fix gptj/git

* fix copies

* fix copies

* style...

* another forgotten docsrting

---------

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",914,2164,3078,20,298,253,0.85
c6ee0b1da8ff57102548430e18480fa78a106022,yonigozlan,2025-07-09 21:46:22+00:00,"Fix broken SAM after #39120 (#39289)

fix",2,2,4,2,88,74,0.84
aff7df8436dde04762170d3d0fbe906c7216d6f2,jiqing-feng,2025-07-09 21:14:45+00:00,"enable static cache on TP model (#39164)

* enable static cache on TP model

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* check tp size before init kv cache

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* fix docstring

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* add tp tests

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* fix comment

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* fix other cache head size

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

---------

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>",84,1,85,6,67,54,0.81
2ef59646b8466a6e47cbf42754637a9f4a82484f,HollowMan6,2025-07-09 21:12:39+00:00,"Fix `max_length_q` and `max_length_k` types to `flash_attn_varlen_func` (#37206)

Also add notes asking users to set `TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1`
or call `torch._dynamo.config.capture_scalar_outputs = True`, as currently
this will cause a graph break.

Signed-off-by: Hollow Man <hollowman@opensuse.org>",9,1,10,1,2,2,1.0
2d600a4363b401f155fe6336994b50b2047982e8,avihu111,2025-07-09 21:09:50+00:00,"Granite speech speedups (#39197)

* ensure the query is updated during training

avoid unused parameters that DDP does not like

* avoid a crash when `kwargs` contain `padding=True`

trainers often pass this argument automatically

* minor

* Remove mel_spec lazy init, and rename to mel_filters.
this ensures save_pretrained will not crash when saving the processor during training
https://github.com/huggingface/transformers/blob/d5d007a1a0f0c11a726a54c8f00bd71825f84d02/src/transformers/feature_extraction_utils.py#L595

* minor - most feature extractors has a `sampling_rate` property

* speedup relative position embeddings

* fix several issues in model saving/loading:
- avoid modifying `self._hf_peft_config_loaded` when saving
- adapter_config automatically points to the original base model - a finetuned version should point to the model save dir.
- fixing model weights names, that are changed by adding an adapter.

* minor

* minor

* minor

* fixing a crash without peft active

* add todo to replace einsum

* granite speech speedups:
1. register attention_dist to avoid cpu-to-gpu transfer every layer.
2. pad_sequence is much faster than per-sample-padding + concat.
3. avoid returning audio back to cpu when using a compute device.

* support audio.shape=(1,L)",5,11,16,3,3,3,1.0
5111c8ea2f3eb918fc090f7dd4393d4204940e10,tomaarsen,2025-07-09 19:06:46+00:00,Fix typo: langauge -> language (#39317),1,1,2,1,30,27,0.9
2781ad092dad77ff554cb70ec130b97e44cfba78,Bpriya42,2025-07-09 18:32:40+00:00,"docs: update LLaVA-NeXT model card (#38894)

* docs: update LLaVA-NeXT model card

* Update docs/source/en/model_doc/llava_next.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/llava_next.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/llava_next.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/llava_next.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/llava_next.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/llava_next.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/llava_next.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/llava_next.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* [docs] Updated llava_next model card

* Update docs/source/en/model_doc/llava_next.md remove image sources

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* [fix] Change Flash Attention to SDPA badge

* [doc] fixed quantization example

* docs: updated contribution details and badges

* Update llava_next.md

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",125,234,359,1,1,1,1.0
16dd7f48d00fbb2c1991ad90634b14856133f2d3,ydshieh,2025-07-09 17:36:48+00:00,"skip files in `src/` for doctest (for now) (#39316)

fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",7,0,7,32,1020,916,0.9
d61c0d087cedbfdbbee8c75b210d5837c35addb8,emanrissha,2025-07-09 17:23:03+00:00,"Updated the Model docs - for the MARIAN model (#39138)

* Update marian.md

This update improves the Marian model card to follow the Hugging Face standardized model card format. The changes include:

- Added a clear description of MarianMT, its architecture, and how it differs from other models.
- Provided usage examples for Pipeline and AutoModel.
- Added a quantization example for optimizing model inference.
- Included instructions and examples for multilingual translation with language codes.
- Added an Attention Mask Visualizer example.
- Added a Resources section with relevant links to papers, the Marian framework, language codes, tokenizer guides, and quantization documentation.
- Fixed formatting issues in the code blocks for correct rendering.

This update improves the readability, usability, and consistency of the Marian model documentation for users.

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update marian.md

* Update marian.md

* Update marian.md

* Update marian.md

* Update docs/source/en/model_doc/marian.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update marian.md

* Update marian.md

* Update marian.md

* Update marian.md

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",99,120,219,1,1,1,1.0
161cf3415ed5b0caee38ce42d8805744d13c1b50,ydshieh,2025-07-09 17:07:44+00:00,"add `stevhliu` to the list in `self-comment-ci.yml` (#39315)

add

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",1,1,2,32,1020,916,0.9
3be10c6d19fafc55d96d52e8bf30715058373543,Cyrilvallez,2025-07-09 16:40:37+00:00,"Fix consistency and a few docstrings warnings (#39314)

* Update modeling_deepseek_v2.py

* fix docstrings

* fix

* fix",43,6,49,14,132,118,0.89
4652677c89ecb664ee06bf141d1b7b648798e122,maximizemaxwell,2025-07-09 16:29:51+00:00,"🌐 [i18n-KO] Translated quark.md to Korean (#39268)

* initial translation

* removed english parts

* maintain consistency

* Update docs/source/ko/quantization/quark.md

Co-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>

* Update docs/source/ko/quantization/quark.md

Co-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>

* Update docs/source/ko/quantization/quark.md

Co-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>

* Update docs/source/ko/quantization/quark.md

Co-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>

* add toctree

* fixed indentation

---------

Co-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>",87,0,87,1,7,6,0.86
c98090420431e49d5cab8f41b1cb5426a8b87e5a,VladOS95-cyber,2025-07-09 15:04:28+00:00,"Add DeepSeek V2 Model into Transformers (#36400)

* add initial structure

* doc fixes, add model base logic

* update init files

* some fixes to config and modular

* some improvements for attention

* format

* remove unused attn

* some fixes for moe layer and for decoder

* adapt _compute_yarn_parameters for deepseek

* format

* small fix

* fix for decoder forward

* add tests, small refactoring

* fix dummies

* fix init

* fix doc

* fix config docs

* add sequce doc, fix init for gate

* fix issues in tests

* fix config doc

* remove unused args

* some fixes and refactoring after review

* fix doc for config

* small fixes for config args

* revert config refactoring

* small refactoring

* minor fixes after rebase

* small fix after merge

* fix modular

* remove rotaryembd from public init

* small test fix

* some rotary pos calculation improvement

* fix format

* some improvements and fixes

* fix config

* some refactoring

* adjust some unit tests

* skip test

* small fixes and tests adjustment

* reapply modular

* fix all tests except Integration

* fix integration testzs

* cleanup BC stuff

* rope

* fix integrations tests based on a10

* style

---------

Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>
Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",1913,0,1913,1,16,13,0.81
accbd8e0fe7e535321fa6fa8de82c4260b500ddf,zucchini-nlp,2025-07-09 14:10:38+00:00,"[sliding window] revert and deprecate (#39301)

* bring back and deprecate

* oops

---------

Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",71,18,89,20,298,253,0.85
1cefb5d788f5e1a5b59fd57394ed93cbe71f0d86,Cyrilvallez,2025-07-09 13:46:53+00:00,"[modular] Allow method with the same name in case of @property decorator (#39308)

* fix

* add example

* fix

* Update modular_model_converter.py",264,8,272,14,132,118,0.89
4798c05c64ddcca574fdce962a72466bdcb55a9e,ydshieh,2025-07-09 13:35:48+00:00,"skip `test_torchscript_*` for now until the majority of the community ask for it (#39307)

fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",4,0,4,32,1020,916,0.9
fe5f3c85d292e34bed52e02a53edd5fa2acfc010,ydshieh,2025-07-09 11:49:33+00:00,"fix `aria` tests (#39277)

* fix

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",16,0,16,32,1020,916,0.9
0687d481e2c71544501ef9cb3eef795a6e79b1de,zucchini-nlp,2025-07-09 07:45:01+00:00,"[flash attn 3] bring back flags (#39294)

* flash attn 3 flag

* fix copies",195,4,199,20,298,253,0.85
25343aafee10da8a13b217a39f3825f88c6d8dbe,JJJYmmm,2025-07-09 05:03:44+00:00,"Fix SDPA attention precision issue in Qwen2.5-VL (#37363)

* solve conflicts and remove  redundant attention_mask in qwenvit

* update decoded text check

* remove trailing whitespace",201,245,446,1,6,3,0.5
0e1c2817455602d182bd8ebf5fba212e14fb187e,yaswanth19,2025-07-08 19:46:32+00:00,"[Tests] Update model_id in AIMv2 Tests (#39281)

* Update model_id in tests

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",5,5,10,5,18,13,0.72
7ef592c96cbf616492ac4181a390d465189abec6,bzhangGo,2025-07-08 17:08:48+00:00,"Update T5gemma (#39210)

* bug fix: add vocab_size to t5gemmaconfig for pipeline.

* Update checkpoint placeholder

* minor change

* minor change

* minor change: update example.

* fix: add vocab_size as an explict arg.

* buf fix:

remove vocab_size verification; instead, re-set encoder/decoder vocab size.

Note, in t5gemma, vocab size of encoder/decoder shoud be always the same.

* add `add_generation_prompt` for message preprocessing.",51,20,71,2,2,2,1.0
1ecd52e50a31e7c344c32564e0484d7e9a0f2256,lhoestq,2025-07-08 15:06:12+00:00,"Add torchcodec in docstrings/tests for `datasets` 4.0 (#39156)

* fix dataset run_object_detection

* bump version

* keep same dataset actually

* torchcodec in docstrings and testing utils

* torchcodec in dockerfiles and requirements

* remove duplicate

* add torchocodec to all the remaining docker files

* fix tests

* support torchcodec in audio classification and ASR

* [commit to revert] build ci-dev images

* [commit to revert] trigger circleci

* [commit to revert] build ci-dev images

* fix

* fix modeling_hubert

* backward compatible run_object_detection

* revert ci trigger commits

* fix mono conversion and support torch tensor as input

* revert map_to_array docs + fix it

* revert mono

* nit in docstring

* style

* fix modular

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",448,350,798,2,25,20,0.8
1255480fd226129075e10c20842efd444f5b0e36,sbucaille,2025-07-08 15:03:04+00:00,"[lightglue] add support for remote code DISK keypoint detector (#39253)

* feat: add trust_remote_code in LightGlueConfig

* fix: made sure trust_remote_code is provided only when necessary

* fix: make style

* docs: added missing trust_remote_code docstring

* refactor: refactored LightGlue config init

* fix: removed unnecessary argument",40,10,50,6,16,11,0.69
838a0268b88b6e56783b401f9cacae9cb0cbb120,ydshieh,2025-07-08 13:36:05+00:00,"fix flaky `test_generate_compile_model_forward` (#39276)

fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",3,0,3,32,1020,916,0.9
29d0030e237485a97ea568120d79a7988535ea63,qubvel,2025-07-08 13:24:39+00:00,"Refactor `PretrainedConfig.__init__` method to make it more explicit (#39158)

* cleanup

* fix no `__init__` test

* fix missing inits",135,102,237,5,88,71,0.81
1580f6465347df4e8d7c12cd4d3dc603b8a689ad,gante,2025-07-08 10:44:01+00:00,"[smollm3] add tokenizer mapping for `smollm3` (#39271)

add tok mapping to smollm3",1,0,1,10,616,539,0.88
db05e4ff33cbb6b08ad882cfe47d50d4071a1daa,kashif,2025-07-08 10:34:22+00:00,"[pagged-attention] fix off-by-1 error in pagged attention generation (#39258)

* fix off-by-1 error in pagged attention generation

* formatting

* use update_with_token",5,1,6,1,29,18,0.62
6f1a43896ce970d316aa1dff79ca33281fee244b,gante,2025-07-08 10:31:03+00:00,"[CI] fix docs (#39273)

* fix docs

* add ko gloassary file to toctree",1,5,6,10,616,539,0.88
fbdaa7b099e4253be4175e0201cd477e9de05363,yaswanth19,2025-07-08 09:53:21+00:00,"Add Aimv2 model (#36625)

* Model skelton

* changes

* temp push

* changes

* Added support for aimv2-native

* More changes

* More changes

* Stupid mistake correction

* Added config and refactor

* Added vison model

* update

* Refactor for lit variant

* Added Text Model

* Minor fixes

* nits

* update

* Preliminary tests

* More fixes

* Updated tests 🤗

* Refactor

* Updated testcase

* Updated config

* make fixup

* more fixes

* Bug fix and updates

* deadcode

* Fixes

* nit

* up

* Happy CI ✅

* Reduce LOC

* nit

* nit

* make style

* return_dict refactor

* bug fix

* fix

* doc update

* nit

* make fixup

* Minor update

* _init_weigths modifcation

* update tests

* Minor fixes post review

* Update w.r.t GradientCheckpointingLayer

* docs update

* update

* nit

* Use more Modular 😉

* Change name from AIMv2 to Aimv2

* Nit

* make style

* Add model doc pointer

* make style

* Update model doc section

* updates

* Modify attn mask and interface

* update test

* Final change

* Utilize flash and flex attn

* keep attn mask

* camelcase model name in test file

* Fix docstring

* Fix config warning finally and create_causal_mask

* disable torchscript

* remove unused arg

* remove from tests

* balance model size for tests

* fix device

* tests

* tests

* flaky test

* fix import

---------

Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>
Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",2977,2,2979,5,18,13,0.72
d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1,LoserCheems,2025-07-08 09:44:29+00:00,"Add Doge model (#35891)

* Add Doge Model

* Fix code quality

* Rollback an error commit

* Fix config for open-source weights

* Revert ""Fix config for open-source weights""

This reverts commit 229cdcac10a6a4274d1dd13b729bc14c98eb0c76.

* Add modular_doge

* Update Doge inherits from Llama

* Fix import bug

* [docs] Add usage of doge model

* Fix Doge import pretrainedconfig from modeling_utils to configuration_utils

* [docs] remove trust remote code from doge

* Fix dynamo bug in doge model

* Update docstrings

* Import apply_rotary_pos_emb and repeat_kv from Llama

* Fix all nits

* Fix code quality

* Fix some bugs

* Fix code quality

* Remove inherited `_update_causal_mask` from Llama
This leads to incorrect weight initialization.

* Fix the wrong tensor orderings in DogeCDMoE

* Fix attention mask bug
We have to provide attention_mask for dynamic mask computation

* Modify most implementations to inherit from Llama
But there are two problems:
1. `flex_attention_forward` is not updated properly
2. `Example` error in the forward method of DogeForCausalLM

* Modify CDMoE for batch efficient implementation

* Uniform MoE configuration names, just like QwenMoE

* Fix code quality

* Fix code quality

* Fix code quality

* Add tp plan of CDMoE Module

* Hybird DMA with sliding window

* Update valid tokens greater than window size

* Fix code quality

* Add `convert_doge_weights_to_hf`

* Fix STATE_DICT_MAPPING in convert_doge_weights_to_hf.py

* Fix nits in modular_doge

* Fix code quality

* Fix all nits

* Fix all nits

* Make sure the attention function is updated inside the class

* Fix code quality issues in the Doge model and add a test for it

* Fix `test_generate`

* Fix code quality

* Fix nits fllowing suggestions

* Fix code quality

* Fix code quality issues

* Fix nits

* Fix code quality nits

* Fix the missing parameters in the configuration.

* Fix the missing parameters in the configuration.

* Fix nits

* Add initialization of attention

* Fix last nits

* Simplify dynamic mask generation logic

* Rename router_logits to gate_logits for matching latest changes of MixtralModel

* Rename typings for matching latest changes of MixtralModel

* Fixes typo in comment

* Update src/transformers/models/doge/modular_doge.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Fix code quality issues to match other modular

* Fix code quality issues to match other modular

* Fix the static compilation errors

* Update model weights link

* Fix code quality issues to match other modular

* reapply modular and support for new outputs

* style

* simplify a lot

* fix import location

* reapply modular

* fix

* fix integration test

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>
Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",2624,0,2624,1,4,2,0.5
d370bc64c63563ba610c08763828be71f4dc561c,kaln27,2025-07-08 09:39:31+00:00,"Fix errors when use verl to train GLM4.1v model (#39199)

* Fix errors when use verl to train GLM4.1v model

* Support glm4v load from AutoModelForVision2Seq
* Set glm4v model _checkpoint_conversion_mapping attr from None to {}

* Update modeling_auto.py",4,4,8,1,1,1,1.0
5fb8bb3e1a897bb46a709e51fb393412e9a15ea8,ArthurZucker,2025-07-08 09:38:11+00:00,"fix recompiles due to instance key, and deepcopy issues (#39270)

* fix recompiles due to instance key, and deepcopy issues

* dict",3,4,7,4,452,371,0.82
356fd681098f4b33a6f95660a5a0252eae313348,guang-yng,2025-07-08 08:59:37+00:00,"fix(generation): stop beam search per-instance when heuristic satisfied (#38778)

* fix(decoding): stop beam search per-instance when heuristic satisfied

Previously, when early_stopping is set to `False`, the early-stopping heuristic only halted generation when **all** batch instances reached the criterion. This caused instances that are impossible (suggested by the heuristic) to improve keep generating, leading to inconsistent and overlong outputs across the batch.

Now we apply the heuristic **per-instance**: once a certain instance of batch has its all beams impossibe to improve, we mark that instance finished while letting others continue. This restores expected behavior and ensures consistency in batched generation.

* Add test case GenerationIntegrationTests.test_beam_search_early_stop_heuristic

* Update naming improvement_possibility -> is_early_stop_heuristic_unsatisfied

* Add comments for early stop heuristic

* Update src/transformers/generation/utils.py

---------

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",109,30,139,1,1,1,1.0
0b0ede8b2bc30d8f0125ab9a57eb60b94950fb3a,molbap,2025-07-08 08:41:44+00:00,"remove broken block (#39255)

* remove broken block

* fixup",0,15,15,2,55,43,0.78
a21557fa3e7f5b0723dd871909dc3917a0c35871,ydshieh,2025-07-08 08:38:25+00:00,"Skip `test_eager_matches sdpa generate` and update an integration test for blip-like models (#39248)

* skip

* skip

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",26,3,29,32,1020,916,0.9
ea3c2c027769980c0501dc615ef7e755d206af62,gudwls215,2025-07-08 08:20:52+00:00,"Fix license text, duplicate assignment, and typo in constant names (#39250)

- Complete Apache License text in Italian documentation
- Remove duplicate variable assignment in Perceiver converter
- Fix typo in MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES constant",3,2,5,1,1,1,1.0
b2816da8021b4e7568cb1e840a5d9aa1357c26a7,yao-matrix,2025-07-08 08:18:26+00:00,"fix xpu failures on PT 2.7 and 2.8 w/o IPEX and enable hqq cases on XPU (#39187)

* chameleon xpu bnb groundtruth update on bnb triton backend since we are
deprecating ipex backend

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

* enable hqq uts on XPU, all passed

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

* fix style

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

* fix comment

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

---------

Signed-off-by: YAO Matrix <matrix.yao@intel.com>",15,14,29,4,53,51,0.96
17b3c96c00cd8421bff85282aec32422bdfebd31,zRzRzRzRzRzRzR,2025-07-08 06:22:04+00:00,"Glm 4 doc (#39247)

* update the glm4 model readme

* update test

* update GLM-4.1V model

* update as format

* update

* fix some tests

* fix the rest

* fix on a10, not t4

* nit: dummy import

---------

Co-authored-by: raushan <raushan@huggingface.co>",154,76,230,3,8,4,0.5
bbca9782ca1b8b358cc832a1b821aa1b450850da,dross20,2025-07-07 22:56:57+00:00,"Update LED model card (#39233)

* Update LED model card

* Remove extra arguments

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",124,51,175,3,6,6,1.0
41e865bb8dd373451a4db1874cf25252bdb0a1c6,ydshieh,2025-07-07 17:49:41+00:00,"fix some flaky tests in `tests/generation/test_utils.py` (#39254)

fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",8,1,9,32,1020,916,0.9
93747d89eab1441af877064d0313bac536d60109,Cyrilvallez,2025-07-07 17:40:41+00:00,"Simplify Mixtral and its modular children (#39252)

* simplify mixtral a lot

* fix

* other moes

* mixtral

* qwen3

* back

* Update modular_qwen3_moe.py",77,467,544,14,132,118,0.89
3993ee1e988482d46384408c097aac28babad794,simonreise,2025-07-07 17:34:59+00:00,"Add `segmentation_maps` support to MobileNetV2ImageProcessor (#37312)

* Add `segmentation_maps` support to mobilenet_v2 image processor and `reduce_labels` to mobilevit

* Changed mobilenetv2 tests to support fastimageprocessor

* added `segmentation_maps` support to fast image processor

* reverted to upstream/main

* Add optional

* Use autodocstring

* Changed docs

* Docs fix

* Changed fp to match beit fp

* Change typing imports

* Fixed repo inconsistency

* Added fast-slow equivalence tests

* Removed unnecessary call

* Add `reduce_labels` to Mobilevit fast processor

---------

Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",722,51,773,1,4,4,1.0
b96f213fcfdaf8ccd18ad8864f70b39553cea331,Shohail-Ismail,2025-07-07 16:57:42+00:00,"Clarify per_device_train_batch_size scaling in TrainingArguments (#38… (#38857)

Clarify global batch size calculation in TrainingArguments (#38484)",2,1,3,1,1,1,1.0
969805256034d62965ca83e5aa4abbcbba9313ad,JoosunH,2025-07-07 16:12:55+00:00,"Add Korean translation for glossary.md (#38804)

* Add Korean translation for glossary.md

* Update docs/source/ko/glossary.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/ko/glossary.md

Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>

* Update docs/source/ko/glossary.md

Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>

* Update docs/source/ko/glossary.md

Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>

* Update docs/source/ko/glossary.md

Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>

* Update docs/source/ko/glossary.md

Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>

* Update docs/source/ko/glossary.md

Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>

* Update docs/source/ko/glossary.md

Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>

* Update docs/source/ko/glossary.md

Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>

* Update docs/source/ko/glossary.md

Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>

* Update docs/source/ko/glossary.md

Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>

* Update docs/source/ko/glossary.md

Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>

* Update docs/source/ko/glossary.md

Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>

---------

Co-authored-by: Joosun40 <77312900+Joosun40@users.noreply.github.com>
Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
Co-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>",454,0,454,1,1,1,1.0
bf203aa9da5af35e57cc78333537a2032f913692,Wauplin,2025-07-07 13:58:36+00:00,Update tiny-agents example (#39245),1,3,4,1,37,33,0.89
c4e39ee59c7ccc552e67889c1b81a574d5badf2e,kaixuanliu,2025-07-07 13:13:25+00:00,"adjust input and output texts for test_modeling_recurrent_gemma.py (#39190)

* adjust input and output texts for test_modeling_recurrent_gemma.py

Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>

* fix bug

Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>

* adjust

Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>

* update Expectation match

Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>

* fix

---------

Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",15,4,19,2,7,3,0.43
14cba7ad33279d18e42857251e56c944560dbe18,jiqing-feng,2025-07-07 13:12:02+00:00,"enable xpu on kv-cache and hqq doc (#39246)

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>",19,16,35,6,67,54,0.81
32db48db730e7a41e58ef043e50310275f7c629c,Cyrilvallez,2025-07-07 13:11:48+00:00,"Fix patch helper (#39216)

remove -1",0,2,2,14,132,118,0.89
a3618d485a321ab9389a250ca712c67775edf1cc,qubvel,2025-07-07 13:05:28+00:00,"RotaryEmbeddings change `is not None` -> `isinstance(..., dict)` (#39145)

is None -> isinstance dict",53,53,106,5,88,71,0.81
9b09fe479feb6ebf9d1e8ec0f84f009ebce7f36c,ydshieh,2025-07-07 13:04:26+00:00,"fix `fastspeech2_conformer` tests (#39229)

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",14,16,30,32,1020,916,0.9
00e9efceab0958105aa1e64d63530e51daf6cda7,FightingZhen,2025-07-07 13:03:39+00:00,"[bugfix] fix flash attention 2 unavailable error on Ascend NPU (#39166)

[bugfix] fix flash attention 2 error on Ascend NPU",10,16,26,1,9,6,0.67
056fa73fae97f0db277939d89859139566dc4f81,Cyrilvallez,2025-07-07 12:52:57+00:00,"[modular] Simplify logic and docstring handling (#39185)

* simplify a lot

* Update modular_model_converter.py

* finalize

* remove outdated functions

* apply it

* and examples",381,466,847,14,132,118,0.89
f16fbfb89ad2c310ed998c3c9f8c9125dae6ae32,xadupre,2025-07-07 12:48:31+00:00,"Make _compute_dynamic_ntk_parameters exportable (#39171)

* Make _compute_dynamic_ntk_parameters exportable

* add unit test",12,1,13,1,6,3,0.5
4243bb844da660b387b3c409487e549754f7acc3,kaixuanliu,2025-07-07 12:47:04+00:00,"fix bug using FSDP V1 will lead to model device not properly set (#39177)

* fix bug using FSDP V1 will lead to model device not properly set

Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>

* update the code

Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>

---------

Signed-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>",4,5,9,2,7,3,0.43
34c16167eb4f5733a357ac62f2e06c2a5d95bb0b,ydshieh,2025-07-07 12:43:50+00:00,"Don't send new comment if the previous one is less than 30 minutes (unless the content is changed) (#39170)

fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",51,15,66,32,1020,916,0.9
b8f397e456251882c9f011596504a9c388065230,davanstrien,2025-07-07 12:41:33+00:00,fix typo in Gemma3n notes (#39196),1,1,2,1,2,2,1.0
5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5,Cyrilvallez,2025-07-07 12:36:43+00:00,"[modular] Follow global indexing and attribute setting, and their dependencies (#39180)

* export global indexing statements

* add example

* style

* examples",254,85,339,14,132,118,0.89
8570bc29f3d994f0d96538987aedbe8ff383ea4a,Isotr0py,2025-07-07 11:54:18+00:00,"Fix missing fast tokenizer/image_processor in whisper/qwen2.5-omni processor (#39244)

* fix missing fast tokenizer in whisper processor

Signed-off-by: Isotr0py <2037008807@qq.com>

* fix processor test

Signed-off-by: Isotr0py <2037008807@qq.com>

* fix qwen2.5 omni processor

Signed-off-by: Isotr0py <2037008807@qq.com>

---------

Signed-off-by: Isotr0py <2037008807@qq.com>",6,6,12,2,25,23,0.92
b283d52f7f89d9cf3c77cfef233c4cbf700959ff,xenova,2025-07-07 10:14:08+00:00,[vjepa2] replace einsum with unsqueeze (#39234),1,1,2,1,16,13,0.81
a325409a5051d68879030214e9c33180505f0d81,remi-or,2025-07-07 09:42:33+00:00,"Expectations re-order and corrected FA3 skip (#39195)

* Fix Expectations and a FA3 skip

* Fixed docstring

* Added context for Default expectation",16,8,24,4,21,20,0.95
b0a8e0b8d7eba2af7346331dd1d48c50892867b2,zrohyun,2025-07-07 03:43:43+00:00,"[video processors] Support float fps for precise frame sampling (#39134)

* [video processors] Support float fps for precise frame sampling

Enable fractional fps values (e.g., 1.5, 29.97) in video processors
for more precise frame sampling control.

- Change fps type from int to float across all video processors
- Maintain backward compatibility with integer values

Extends: #38105

* [video processors] Refine fps typing to Union[int, float]

Change fps type from Optional[float] to Optional[Union[int, float]]
for more explicit type information about supporting both integer
and floating-point frame rates.

- Update type hints and docstrings across 8 files
- Maintain backward compatibility
- Clarify support for both int and float values

Extends: #38105

* Revert ""[video processors] Support float fps for precise frame sampling""

This reverts commit 7360d6e661b413ca0239e5ef61f9b1abbeab8e65.",20,20,40,1,1,1,1.0
ca7e1a3756c022bf31429c452b2f313f043f32de,ArthurZucker,2025-07-05 09:34:28+00:00,"Refactor the way we handle outputs for new llamas and new models (#39120)

* just update 2 files

* update other models as well just making fix-copies

* also add the changes needed to modeling utils

* put this on the pretrained model instead

* nits and fixes

* update generic, fix to use config value

* update other modelings

* use transformers kwargs instead

* update

* update

* update other models

* update

* updates

* update

* update

* update

* fix

* finally

* very small nits

* this fixes more tests

* fix other models as well!

* update modularqwen2

* update models based on qwen2

* update

* update

* remove the **flash stuff in favor of noraml kwargs

* update

* propagate gemma?

* remove output attentions

* propagate

* support cross attention edge case

* same

* test this

* fixes

* more fix

* update

* update

* fix conflicts

* update

* fix emu3

* fix emu3

* move the fix a bit

* quel enfer

* some fixes, loss_kwargs should never had been

* finish fixing gemma3n

* fix small lm3

* fix another one

* fix csm now

* fux csm and mistral

* fix mistral now

* small fixes

* fix janusss

* only for some models

* fixup

* phix phi3

* more fixes?

* dose this fix it?

* update

* holy shit it was just graph breaks

* protect torch

* updates

* fix samhq?

* fix moonshine

* more moonshine fixes, 3 failures left!

* nits

* generic needs to support more

* more fixes to moonshine!

* fix cross attention outputs!

* fix csm!

* nits

* fix stupid kosmos2

* current updates

* fixes

* use output recorder?

* nicer!

* a little bit of magic

* update

* fix protect

* fix

* small fixes

* protect import

* fix a bunch of more models

* fix fixups

* fix some of the last ones

* nit

* partly fix phi

* update

* fix import path

* make something that is fullgraph compatible just to be sure

* typing was wrong on llama so the rest was wrong as well

* fucking ugly but at least it is still exportable

* syle

* supposed to fix moonshine, it still breaks

* fix some default

* fix the last bits of sam

* update samhq

* more fixes to am hq

* nit

* fix all output+hidden states and output_attentions!

* fix?

* fix diffllama

* updates to fix initialization on the sam pips

* ups there was a bug

* fix the last sam hq test

* fix gotocr

* fix gotocr2!

* fixes

* skip stupid tests

* there was one left :)

* fixup

* fix fix copies issues with this test file

* fix copies for sam_hq

* rm some comments

* skip 2 more failing tests

* fix

* fix everything

* Apply suggestions from code review

Co-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>
Co-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>

* add more doc!

* fix public init

* fix modular qwen3

---------

Co-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>
Co-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>",2005,5896,7901,4,452,371,0.82
e6a8063ef1af16df964b644b07e1d17e96555d23,ydshieh,2025-07-04 11:35:53+00:00,"Update expected values (after switching to A10) - part 8 - Final (#39220)

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",20,16,36,32,1020,916,0.9
cd8a041a4f6ecd8887bbf895493327edc82fc1b8,ydshieh,2025-07-04 10:48:10+00:00,"Update expected values (after switching to A10) - part 7 (#39218)

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",60,14,74,32,1020,916,0.9
0cf27916f09a1a99af55ef4f2f3e8675372f38b6,Cyrilvallez,2025-07-04 07:01:56+00:00,"Add packed tensor format support for flex/sdpa/eager through the mask! (#39194)

* Add the necesary logic to mask_utils

* add it everywhere

* Update masking_utils.py

* style

* Update masking_utils.py

* Update modeling_mimi.py

* Update masking_utils.py

* add support for more than batch size 1

* Update masking_utils.py

* add test

* style

* Update test_masking_utils.py

* Update masking_utils.py

* add require_token

* fix tests

* fix",303,9,312,14,132,118,0.89
037755ed54208eefa77673b0af2a0b13e51f2fb1,ydshieh,2025-07-03 20:45:30+00:00,"Update expected values (after switching to A10) - part 6 (#39207)

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",85,21,106,32,1020,916,0.9
1168f57abffd077d7d2687087aa10ba644a76a0d,ydshieh,2025-07-03 17:56:02+00:00,"Update expected values (after switching to A10) - part 5 (#39205)

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",111,23,134,32,1020,916,0.9
7d9e52f376ad4b351ae696b0a62280cb9c63f70b,LysandreJik,2025-07-03 16:15:31+00:00,"Fix continuous batching in `transformers serve` (#39149)

* Fix CB

* Nit

* Update src/transformers/commands/serving.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Add todos

---------

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",38,28,66,6,555,484,0.87
85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7,gante,2025-07-03 16:04:16+00:00,"[serve] Cursor support, move docs into separate page, add more examples (#39133)

* jan docs

* rm

* [cursor] tmp commit

* Cursor working :D

* Update docs/source/en/serving.md

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* Update docs/source/en/serving.md

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* Update docs/source/en/serving.md

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* Update docs/source/en/serving.md

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* Update docs/source/en/serving.md

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* Update docs/source/en/serving.md

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* Update docs/source/en/serving.md

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* Update docs/source/en/serving.md

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* Update src/transformers/commands/serving.py

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* cursor docs

* try to fix agents/tools docs?

* try to fix agents/tools docs?

* Update docs/source/en/serving.md

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* add transformers chat example with transformers serve

---------

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>",263,591,854,10,616,539,0.88
e15b06d8dc6fa132550311d63c9758b580f39bcc,qubvel,2025-07-03 14:22:47+00:00,"[typing] better return typehints for `from_pretrained` (#39184)

* config

* processor

* feature-extractor

* jukebox

* fixup

* update other methods in config

* remove ""PretrainedConfig"" annotations",36,27,63,5,88,71,0.81
a25fc3592eec7a18aa20fe5d85bd335477896cbc,ydshieh,2025-07-03 13:13:06+00:00,"Update expected values (after switching to A10) - part 4 (#39189)

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",294,134,428,32,1020,916,0.9
b31e9d19a6607aafdd921bc592897900712ba61d,vasqu,2025-07-03 10:02:58+00:00,"[`Dia`] Change ckpt path in docs (#39181)

fix ckpt path",3,3,6,5,41,32,0.78
18e0cae207a38d2c430b5fa08f9597312d1c1ab3,IlyasMoutawwakil,2025-07-03 09:17:27+00:00,"Fix many HPU failures in the CI (#39066)

* more torch.hpu patches

* increase top_k because it results in flaky behavior when Tempreture, TopP and TopK are used together, which ends up killing beams early.

* remove temporal fix

* fix scatter operation when input and src are the same

* trigger

* fix and reduce

* skip finding batch size as it makes the hpu go loco

* fix fsdp (yay all are passing)

* fix checking equal nan values

* style

* remove models list

* order

* rename to cuda_extensions

* Update src/transformers/trainer.py",71,54,125,1,13,12,0.92
bff964c429a5bfc8ca85789f20f37d6bfb60b294,SunMarc,2025-07-03 09:07:11+00:00,"Decouple device_map='auto' and tp_plan='auto'  (#38942)

* dissociate

* better place

* fix",6,4,10,4,135,119,0.88
8178c43112295bf8c4ef04c667efbbbfd34b8bca,winglian,2025-07-03 07:04:16+00:00,when delaying optimizer creation only prepare the model (#39152),1,1,2,3,45,27,0.6
91221da2f1f68df9eb97c980a7206b14c4d3a9b0,zucchini-nlp,2025-07-03 05:20:41+00:00,"[glm4v] fix video inference (#39174)

fix video inference",6,6,12,20,298,253,0.85
ebfbcd42da327b4a9f2d73c93a962be0a581faaa,remi-or,2025-07-02 21:41:14+00:00,"Test fixes for Aria (and some Expectation for llava_next_video) (#39131)

* Expectations for llava_next_video

* Updated image src in aria

* Fix test_small_model_integration_test

* Fix small model integration llama

* Fix a bunch of tests

* Style

* Shortened generation in test from 900 to 90",148,69,217,4,21,20,0.95
37a239ca50885443a3216f56110a03f959509c80,ydshieh,2025-07-02 20:48:30+00:00,"Update expected values (after switching to A10) - part 3 (#39179)

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",181,68,249,32,1020,916,0.9
9326fc332d4b8477fb1b990a5de486c70a94696d,ydshieh,2025-07-02 20:47:55+00:00,"Update expected values (after switching to A10) - part 2 (#39165)

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* empty

* [skip ci]

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",420,193,613,32,1020,916,0.9
25cd65ac43ee1a96cef4692bda0b110d1e3c6903,pcuenca,2025-07-02 20:09:58+00:00,"Random serve fixes (#39176)

* Fix index out of bounds exception on wrong kv reuse

* Prevent loading same model twice

---------

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>
Co-authored-by: Lysandre Debut <hi@lysand.re>",10,5,15,1,31,23,0.74
548794b886a2186e9904ce6a90819eb2d0dfe266,LysandreJik,2025-07-02 20:06:47+00:00,"[serve] Model name or path should be required (#39178)

* Model name or path should be required

* Fix + add tests

* Change print to log so it doesn't display in transformers chat",29,2,31,6,555,484,0.87
2d561713f8e3eb3fcd219b44c9ea8f51db69c96c,gante,2025-07-02 17:29:16+00:00,[generate] document non-canonical beam search default behavior (#39000),15,3,18,10,616,539,0.88
df12d87d184db59aed00b6b22c2daff7bac95204,stevhliu,2025-07-02 14:56:29+00:00,"[docs] ViTPose (#38630)

* vitpose

* fix?

* fix?

* feedback

* fix

* feedback

* feedback

* update sample image",187,177,364,5,145,131,0.9
2b4a12b5bf5b5fa609c64268c600e81fe8623afc,Cyrilvallez,2025-07-02 13:55:05+00:00,"Reduce Glm4v model test size significantly (#39173)

* fix test size

* Update test_modeling_glm4v.py",8,14,22,14,132,118,0.89
e355c0a11c927d9e8f22409559c0fae76ccc598c,bvantuan,2025-07-02 13:03:57+00:00,"Fix missing initializations for models created in 2024 (#38987)

* fix GroundingDino

* fix SuperGlue

* fix GroundingDino

* fix MambaModel

* fix OmDetTurbo

* fix SegGpt

* fix Qwen2Audio

* fix Mamba2

* fix DabDetr

* fix Dac

* fix FalconMamba

* skip timm initialization

* fix Encodec and MusicgenMelody

* fix Musicgen

* skip timm initialization test

* fix OmDetTurbo

* clean the code

Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>

* add reviewed changes

* add back timm

* style

* better check for parametrizations

---------

Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",229,98,327,3,6,3,0.5
1125513a8da80d16e26cecfcbb508efc9038b5a7,remi-or,2025-07-02 12:39:39+00:00,"Blip2 fixes (#39080)

* Fixed some devices errors

* Fixed other device issues and more expectations

* Reverted support flags

* style

* More granular support

* Fixed some rebase stuff

* add a not None check before .to",50,18,68,4,21,20,0.95
28df7f854ac4ec650c4a5057cc95a072d5efa5a8,Isotr0py,2025-07-02 11:57:15+00:00,"Fix multimodal processor get duplicate arguments when receive kwargs for initialization (#39125)

* fix processor tokenizer override

Signed-off-by: Isotr0py <2037008807@qq.com>

* code format

Signed-off-by: Isotr0py <2037008807@qq.com>

* add regression test

Signed-off-by: Isotr0py <2037008807@qq.com>

* fix

Signed-off-by: Isotr0py <2037008807@qq.com>

* check image processor same

Signed-off-by: Isotr0py <2037008807@qq.com>

---------

Signed-off-by: Isotr0py <2037008807@qq.com>",19,3,22,2,25,23,0.92
b61023a1b760b207d99b699dafc1fbfde992c12c,yaswanth19,2025-07-02 11:25:26+00:00,"🚨🚨🚨 [eomt] make EoMT compatible with pipeline (#39122)

* Make EoMT compatible with pipeline

* Implicit patch offsets

* remove patch offsets from arg

* Modify tests

* Update example

* fix proc testcase

* Add few more args

* add pipeline test suite

* fix

* docstring fixes

* add pipeline test

* changes w.r.t review

* 🙈 MB

* should fix device mismatch

* debug

* Fixes device mismatch

* use decorator

* we can split mlp

* expected values update

---------

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>",113,92,205,5,18,13,0.72
4d5822e65daa1ea14d199a8e7b893a01787cfcc1,zucchini-nlp,2025-07-02 10:05:10+00:00,"[smolvlm] fix video inference (#39147)

* fix smolvlm

* better do as before, set sampling params in overwritten `apply_chat_template`

* style

* update with `setdefault`",40,12,52,20,298,253,0.85
9b2f5b66d83e1b15dd1430f887f2037fd4039992,ved1beta,2025-07-02 09:45:50+00:00,fix default value of config to match checkpionts in LLaVa-OV models (#39163),1,1,2,1,5,2,0.4
e8e0c76162263840661fc0ca0da3952861754759,ChongYou,2025-07-02 02:11:03+00:00,"Add activation sparsity reference in gemma3n doc (#39160)

Add activation sparsity reference in the description of gemma3n",2,1,3,1,1,1,1.0
8e87adc45f20ba88360afbc29ab3f7a0063bf720,ydshieh,2025-07-01 21:27:22+00:00,"fix `llama` tests (#39161)

* fix

* fix

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",13,19,32,32,1020,916,0.9
4c1715b6109184b062198793c3922ae1cffa79f9,ydshieh,2025-07-01 18:54:31+00:00,"Update expected values (after switching to A10) (#39157)

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* empty

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",138,66,204,32,1020,916,0.9
ab59cc27fe1e166095f1b53e050a718fa7e86f34,ydshieh,2025-07-01 18:19:06+00:00,"Suggest jobs to use in `run-slow` (#39100)

* pr

* pr

* pr

* pr

* pr

* pr

* pr

* pr

* pr

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",489,0,489,32,1020,916,0.9
db2f5354439f887f4ae0a46fb3f4a6dd4bec3b45,jiqing-feng,2025-07-01 18:06:37+00:00,"update bnb ground truth (#39117)

* update bnb resulte

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* set seed to avoid sampling different results

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* fix int8 tests

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* fix typo

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* add comments

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

---------

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>",10,0,10,6,67,54,0.81
260846efadb9b03472427a46c30ba8f717d182c4,ybkurt,2025-07-01 17:10:29+00:00,fix: remove undefined variable (#39146),2,2,4,1,1,1,1.0
cdfe49a4d0a364c4bdf2b828b67403994f00b092,rasmi,2025-07-01 16:29:16+00:00,"Change `@lru_cache()` to `@lru_cache` to match styles from #38883. (#39093)

Match styles in #38883",1,1,2,2,5,4,0.8
f46798193ecd617752e54099983598a912982b64,DavidS2106,2025-07-01 16:17:58+00:00,"Fix: Ensure wandb logs config in offline mode (#38992)

* Fix: Ensure wandb logs config in offline mode

* Apply style fixes

---------

Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Co-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",5,1,6,1,1,1,1.0
fe838d6631badce944d77c3822f09200d01951bb,ydshieh,2025-07-01 16:10:30+00:00,"Fix missing fsdp & trainer jobs in daily CI (#39153)

fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",1,0,1,32,1020,916,0.9
128387757105c7c0b57b519ac2aaff217a20e3f0,sbucaille,2025-07-01 12:14:44+00:00,[superglue] fix wrong concatenation which made batching results wrong (#38850),2,2,4,6,16,11,0.69
f8b88866f552e0eeb21059c90e2c30dba058c8e9,zucchini-nlp,2025-07-01 11:33:20+00:00,"[VLMs] support passing embeds along with pixels (#38467)

* VLMs can work with embeds now

* update more models

* fix tests

* fix copies

* fixup

* fix

* style

* unskip tests

* fix copies

* fix tests

* style

* omni modality models

* qwen models had extra indentation

* fix some other tests

* fix copies

* fix test last time

* unrelated changes revert

* we can't rely only on embeds

* delete file

* de-flake mistral3

* fix qwen models

* fix style

* fix tests

* fix copies

* deflake the test

* modular reverted by fixes, fix again

* flaky test, overwritten

* fix copies

* style",1136,1710,2846,20,298,253,0.85
20901f1d681669fa402f47edce49873432c3212e,ArkVex,2025-07-01 10:29:52+00:00,"[typing] LlamaAttention return typehint  (#38998)

* helo llama

* helo llama

* helo llama

* apply modular

* fix dia

---------

Co-authored-by: qubvel <qubvel@gmail.com>",12,12,24,1,4,1,0.25
7a25f8dfdba4c710d278d8312ef2522c5996a894,zucchini-nlp,2025-07-01 10:18:37+00:00,"[qwen2-vl] fix FA2 inference (#39121)

* fix FA2

* update is causal flag and remove mask for FA2

* update for FA2 with varlen path

* how the tests were passing with different devices?

* add comment and ref to the PR

* move mask preparation to base pretrained model

* seq len is the first dim, not second

* fix copies to fix GLM4V",363,199,562,20,298,253,0.85
def96632394fae03689019d2ed552f4790eb7d21,kmehant,2025-07-01 10:03:22+00:00,"feat: support indivisible shards for TP model loading and TPlizing. (#37220)

* feat: support uneven loading and sharding
resolve merge conflicts
Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>

* fix: allow for empty tensor computations

Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>

* test: add llama1b test case

Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>

* due to q_proj colwise it has to be multi of 2

Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>

* refactor: use slice API

Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>

* refactor: use slice API

Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>

* refactor: use slice API

Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>

* refactor: use slice API

Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>

---------

Signed-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>",58,8,66,1,9,8,0.89
06c4a4d499aeb213c558d6fb59adf864a6062dad,jiqing-feng,2025-07-01 09:32:20+00:00,"fix caching_allocator_warmup with tie weights (#39070)

* fix caching_allocator_warmup with tie weights

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* fix comment

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

---------

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>",5,0,5,6,67,54,0.81
e4355747213c32885ddc4128d46708f8fcf847be,zucchini-nlp,2025-07-01 09:08:21+00:00,"🚨 Don't use cache in non-generative models (#38751)

* deprecate for 1 version

* style

* fix some tests

* fix esm

* skip for now, GC requires positional args but we have keyword args

* remove transpose for scores in modified models only

* skip fx trace tests",991,2350,3341,20,298,253,0.85
dbc98328da2cabe7938423c51569252f2b49a5b3,Cyrilvallez,2025-07-01 08:34:53+00:00,"Several fixes for Gemma3n (#39135)

* remove the skips

* fix the epsilon to a small value (does not make sense otherwise)

* safeguard

* overload test_eager_matches_sdpa

* Update test_modeling_common.py

* skip appropriate tests

* correct no_split_layer

* fix all devices issue

* fix backward

* fix",491,390,881,14,132,118,0.89
d53518c5f2dd7ada022ff5b725c684c9ed89cb44,bvantuan,2025-07-01 07:47:53+00:00,"Fix key mapping for VLMs (#39029)

* fix key mapping for VLMs

* use __mro__ instead

* update key mapping in save_pretrained",8,2,10,3,6,3,0.5
3457e8e73e4f5532cc69059682b1ba4484d7e7e8,eustlb,2025-06-30 19:55:36+00:00,"[Whisper] update token timestamps tests (#39126)

* fixes

* update comment

* update for A10

* all a10

* all a10

* all a10

* all a10

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",18,14,32,8,42,29,0.69
fe35eca7bded3e6190f2d760849712d3031f6319,dross20,2025-06-30 17:42:56+00:00,"Update BigBirdPegasus model card (#39104)

* Update igbird_pegasus.md

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",112,48,160,3,6,6,1.0
29a3f5ed8c5588151419012408f394b4644d4aa6,yao-matrix,2025-06-30 15:54:05+00:00,"switch default xpu tp backend to pytorch built-in XCCL from pytorch 2.8 (#39024)

* switch default xpu tp backend to pytorch built-in XCCL from pytorch 2.8

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

* Update docs/source/en/perf_infer_gpu_multi.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update perf_infer_gpu_multi.md

* Update perf_infer_gpu_multi.md

* Update perf_infer_gpu_multi.md

---------

Signed-off-by: YAO Matrix <matrix.yao@intel.com>
Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",6,4,10,4,53,51,0.96
9e0c865b8be73cfe7aac1b5aa146cd7839784c8a,VladimirGutuev,2025-06-30 15:53:43+00:00,"docs: correct two typos in awesome-transformers.md (#39102)

* docs(awesome-projects): fix typo “Itt leverages” → “It leverages” (#39101)

closes #39101

* docs(awesome-projects): fix grammar “We provides” → “We provide” (#39101)

closes #39101",2,2,4,1,1,1,1.0
03db2700abf84971351c7374a548a9d4fc156916,jiqing-feng,2025-06-30 14:56:55+00:00,"Enable XPU doc (#38929)

* fix example with dataset

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* update torchao doc

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* update torchao doc

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* fix device type

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* revert torchao change

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* fix torchao doc

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* revert torchao change

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* update xpu torchao doc

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* update chat_templating_multimodal.md

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* use full name for int8

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

* revert int8 title

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>

---------

Signed-off-by: jiqing-feng <jiqing.feng@intel.com>
Co-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",75,8,83,6,67,54,0.81
ea0ea392e57f8816f9ab8e5f740577a0343a1594,gante,2025-06-30 13:47:48+00:00,Fix chat (#39128),6,3,9,10,616,539,0.88
ed36f8490eb3748f2424d854936cc6f816aeb486,LysandreJik,2025-06-30 13:25:36+00:00,"Licenses (#39127)

* Licenses

* Licenses",27,1,28,6,555,484,0.87
e8f90b5397df419f211498ef399f9255790b8428,LysandreJik,2025-06-30 13:10:53+00:00,"Split `transformers chat` and `transformers serve`  (#38443)

* Next token

* Split chat and serve

* Support both generation methods

* Style

* Generation Config

* temp

* temp

* Finalize serving.py

Co-authored-by: =?UTF-8?q?c=C3=A9lina?= <hanouticelina@gmail.com>

* Finalize chat.py

* Update src/transformers/commands/serving.py

Co-authored-by: célina <hanouticelina@gmail.com>

* Lucain's comments

Co-authored-by: Lucain <lucain@huggingface.co>

* Update

* Last comments on PR

* Better error handling

* Better error handling

* CI errors

* CI errors

* Add tests

* Fix tests

* Fix tests

* [chat] Split chat/serve (built on top of lysandre's PR) (#39031)

* Next token

* Split chat and serve

* Support both generation methods

* Style

* Generation Config

* temp

* temp

* Finalize serving.py

Co-authored-by: =?UTF-8?q?c=C3=A9lina?= <hanouticelina@gmail.com>

* Finalize chat.py

* Update src/transformers/commands/serving.py

Co-authored-by: célina <hanouticelina@gmail.com>

* Lucain's comments

Co-authored-by: Lucain <lucain@huggingface.co>

* Update

* Last comments on PR

* Better error handling

* Better error handling

* CI errors

* CI errors

* Add tests

* Fix tests

* Fix tests

* streaming tool call

* abstract tool state; set tool start as eos

* todos

* server working on models without tools

* rm chat's deprecated flags

* chat defaults

* kv cache persists across calls

* add server docs

* link

* Update src/transformers/commands/serving.py

* Apply suggestions from code review

* i love merge conflicts

* solve multi turn with tiny-agents

* On the fly switching of the models

* Remove required positional arg

---------

Co-authored-by: Lysandre <hi@lysand.re>
Co-authored-by: =?UTF-8?q?c=C3=A9lina?= <hanouticelina@gmail.com>
Co-authored-by: Lucain <lucain@huggingface.co>

* Protect names

* Fix tests

---------

Co-authored-by: =?UTF-8?q?c=C3=A9lina?= <hanouticelina@gmail.com>
Co-authored-by: Lucain <lucain@huggingface.co>
Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",915,310,1225,6,555,484,0.87
539c6c2fa8abc9bb97218ac0b3c3d143ba800e05,ydshieh,2025-06-30 12:23:27+00:00,"All CI jobs with A10 (#39119)

all a10

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",38,38,76,32,1020,916,0.9
ed9f252608389c4e5cb2c5a94f5cc47d76855842,RyanMullins,2025-06-30 12:10:51+00:00,"docs: Gemma 3n audio encoder (#39087)

Updating Gemma 3n docs and docstrings to clarify the relationship
between the newly trained audio encoder used in Gemma 3n and the USM
model from the original paper.",12,12,24,2,9,8,0.89
4a79bf947d0614d2a023b9137a32cf754ac241fe,zRzRzRzRzRzRzR,2025-06-30 10:16:22+00:00,"Fix some bug for finetune and batch infer For GLM-4.1V (#39090)

* update

* 1",13,14,27,3,8,4,0.5
2100ee654569d323bfb77266cd3a75070abfda97,yao-matrix,2025-06-30 09:49:03+00:00,"fix UT failures on XPU w/ stock PyTorch 2.7 & 2.8 (#39116)

* fix UT failures on XPU w/ stock PyTorch 2.7 & 2.8

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

* zamba2

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

* xx

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

* internvl

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

* tp cases

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

---------

Signed-off-by: YAO Matrix <matrix.yao@intel.com>",120,52,172,4,53,51,0.96
ccf2ca162e33f381e454cdb74bf4b41a51ab976d,ydshieh,2025-06-27 21:08:14+00:00,"skip some `test_sdpa_can_dispatch_on_flash` (#39092)

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",18,1,19,32,1020,916,0.9
a11f69289572955f4be9d2bc7b7c5dd949722fc1,st81,2025-06-27 18:25:32+00:00,"Fixes the failing test `test_is_split_into_words` in `test_pipelines_token_classification.py` (#39079)

* Fix test pipelines token classification for is_split_into_words

* Fix incorrect import format",6,4,10,1,3,2,0.67
18143c76bfa86792d293d646bb795935c2266967,sandeepyadav1478,2025-06-27 17:35:30+00:00,"Sandeepyadav1478/2025 06 19 deberta v2 model card update (#38895)

* [docs]: update deberta-v2.md model card

* chore: req updates

* chore: address code review feedback and update docs

* chore: review feedback and updates

* chore: model selection updates

* chores: quantizations review updates",102,57,159,1,1,1,1.0
02a769b05860d2390e837309c3b41e99218b6555,stevhliu,2025-06-27 16:38:21+00:00,"[fix] Add FastSpeech2ConformerWithHifiGan (#38207)

* add to mapping

* oops

* oops

* add to config_mapping_names

* revert

* fix?

* config-mapping-names

* fix?

* fix?",7,1,8,5,145,131,0.9
c2dc72bb5f15fcfbba061a8b243997bf424d67df,BenjaminBossan,2025-06-27 16:33:11+00:00,"TST Fix PEFT integration test bitsandbytes config (#39082)

TST Fix PEFT integration test bitsandbytes config

The PEFT integration tests still used load_in_{4,8}_bit, which is
deprecated, moving to properly setting BitsAndBytesConfig. For 4bit,
also ensure that nf4 is being used to prevent

> RuntimeError: quant_type must be nf4 on CPU, got fp4",27,5,32,2,11,11,1.0
c8064bea9a2482b741de87e2b7e4faa93181da72,S1ro1,2025-06-27 15:28:05+00:00,Fix: unprotected import of tp plugin (#39083),0,1,1,1,9,6,0.67
dd7dc4a4a2281c4a3eda1247fc05e34149a55786,farrosalferro,2025-06-27 15:26:57+00:00,"Add Fast Image Processor for Chameleon (#37140)

* Add Fast Image Processor for Chameleon

* add warning to resize and move blend_rgba to convert_to_rgb

* Remove unrelated files

* Update image_processing_chameleon_fast to use auto_docstring

* fix equivalence test

---------

Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>
Co-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",223,79,302,1,2,2,1.0
6d773fc3bc936b4dfa9b97d46cc9250dddfa2e1f,ydshieh,2025-06-27 14:54:11+00:00,"fix `dots1` tests (#39088)

fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",4,0,4,32,1020,916,0.9
c8764ab9353f7cd822f1184a0e9848cef5c04a6f,tvukovic-amd,2025-06-27 14:49:47+00:00,"guard torch distributed check (#39057)

* guard torch distributed check

* Update src/transformers/pipelines/base.py

---------

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>",1,1,2,1,1,1,1.0
49d9fd49bd3d58853d461295bc2fd4f2c808de87,MinJu-Ha,2025-06-27 14:40:24+00:00,"Add Fast Image Processor for mobileViT (#37143)

* Add image_processing_mobilevit_fast.py

* Fix copies

* update _preprocess for channel_flip

* Update for batched image processing

* Resolve merge conflicts with main

* Fix import order and remove trailing whitespace (ruff clean-up)

* Fix copy inconsistencies

* Add NotImplementedError for post_process_semantic_segmentation to satisfy repo checks

* Add auto_docstring

* Adjust style

* Update docs/source/en/model_doc/mobilevit.md

Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>

* Update src/transformers/models/mobilevit/image_processing_mobilevit_fast.py

Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>

* Update src/transformers/models/mobilevit/image_processing_mobilevit_fast.py

Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>

* Delete not used function

* test: add missing tests for  and

* Add post_process_semantic_segmentation to mobilevit_fast.py

* Add preprocess function to image_processing_mobilebit_fast.py

* ruff check for formatting

* fix: modify preprocess method to handle BatchFeature correctly

* Remove logic for default value assignment

Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>

* Remove normalization adn RGB conversion logic not used in slow processor

Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>

* Simplify return_tensors logic using one-liner conditional expression

Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>

* Remove unused normalization and format parameters

Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>

* add **kwargs and remove default values in _preprocess

* add slow_fast equivalence tests for segmentation

* style: autoformat code with ruff

* Fix slow_fast equivalence test

* merge + remove skipped test

---------

Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>
Co-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",397,115,512,1,3,2,0.67
4336ecd1eaae778a24633dea6c62b3a90fb8afd1,NahieliV,2025-06-27 14:39:43+00:00,"add fast image processor nougat (#37661)

* add fast image processor nougat

* test fixes

* docstring white space

* last fixes

* docstring_type

* tolerance unit test

* fix tolerance

* fix rtol

* remove traling white space

* remove white space

* note for tolerance unit test

* fix tests

* remove print

---------

Co-authored-by: yonigozlan <yoni.gozlan@huggingface.co>
Co-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",499,47,546,1,2,2,1.0
0c35280e58ea4a297c1a62f22523bc454301276b,BenjaminBossan,2025-06-27 13:58:10+00:00,"TST PEFT integration tests with pipeline generate (#39086)

Some PEFT integration tests involving text generation pipelines were
failing since #38129 because the base model is too small to generate
longer sequences. Setting max_new_tokens fixes this.",2,2,4,2,11,11,1.0
993665a5ffc9bb985c2adb1a51b94d8bad9b040a,JINO-ROHIT,2025-06-27 13:57:56+00:00,fixed typo for docstring in prepare_inputs method (#39071),1,1,2,1,5,3,0.6
839893c86bf372ee35b2c8dd750d3cdc21a995f5,ydshieh,2025-06-27 13:44:10+00:00,"fix `mistral3` tests (#38989)

* fix

* fix

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",9,14,23,32,1020,916,0.9
2b85b6ce1978c776585cc20bdb013334f1c91e6c,eustlb,2025-06-27 12:51:43+00:00,"[Whisper] 🚨 Fix pipeline word timestamp: timestamp token is end of token time !!! (#36632)

* timestamp token is end of token time !!!

* ensure correct alignment between tokens and timestamp tokens

* ignore input tokens for DTW computation

* use num_frames to avoid token timestamp hallucinations

* token timestamps test updates !

* num_frames: deprecate and use attention_mask instead

* avoid breaking change

* fix the pipeline usage for chunk approach

* make style

* better logging

* better logging

* make style

* update tests with correct values",83,60,143,8,42,29,0.69
9c8d3a70b8bf359150c960c4281aaa853498fe8c,eustlb,2025-06-27 12:32:03+00:00,"Pipeline: fix unnecessary warnings (#35753)

* return attention mask

* use correct model input name

* fix

* make",13,5,18,8,42,29,0.69
1750c518dda15a8b81cff276292674d61152dbf5,yaswanth19,2025-06-27 12:18:18+00:00,"✨ Add EoMT Model ||  🚨 Fix Mask2Former loss calculation (#37610)

* Initial Commit

* up

* More changes

* up

* Only mask_logits mismatch

* close enough logits debug later

* fixes

* format

* Add dummy loss

* Close enough processing for semantic seg

* nit

* Added panoptic postprocessor

* refactor

* refactor

* finally fixed panoptic postprocessor

* temp update

* Refactor ForUniversalSegmentation class

* nits and config update

* Few fixes and inference matches

* change mapping

* Added training support but loss slightly off 🥲

* Loss is matching 😀

* update

* Initial tests skelton

* changes

* tests update

* more modular

* initial tests

* updates

* better docstrings

* changes

* proc tests passing :)

* Image processor update

* tiny change

* QOL changes

* Update test w.r.t latest attn refactor

* repo-consistency fixes

* up

* Image proc fix and integration tests :)

* docs update

* integration tests

* fix

* docs update 🥰

* minor fix

* Happy CI

* fix

* obvious refactoring

* refactoring w.r.t review

* Add fask image proc skelton

* Fast Image proc and cleanups

* Use more modular

* tests update

* Add more tests

* Nit

* QOL updates

* change init_weights to torch default

* add eager func coz of make style

* up

* changes

* typo fix

* Updates

* More deterministic tests

* More modular

* go more modular 🚀

* up

* dump

* add supprot for giant ckpts

* overhaul

* modular

* refactor

* instace seg is ready

* cleanup

* forgot this

* docs cleanup

* minor changes

* EoMT - > Eomt

* Happy CI

* remove redundant comment

* Change model references

* final change

* check annealing per block

* My other PR changes 😂

---------

Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",4923,1,4924,5,18,13,0.72
0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3,yao-matrix,2025-06-27 12:01:53+00:00,"fix a bunch of XPU UT failures on stock PyTorch 2.7 and 2.8 (#39069)

* fix a bunch of XPU UT failures on stock PyTorch 2.7 and 2.8

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

* qwen3

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

* quanto

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

* models

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

* fix style

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

* idefics2

Signed-off-by: YAO Matrix <matrix.yao@intel.com>

---------

Signed-off-by: YAO Matrix <matrix.yao@intel.com>",53,31,84,4,53,51,0.96
cb17103bd5e31373e090f2f37602dcc992c017e4,MekkCyber,2025-06-27 11:51:46+00:00,"Uninstallling Flash attention from quantization docker (#39078)

* update

* revert",3,0,3,2,82,73,0.89
371c4711136386075bfb272692860c1d4ee9c1d2,bvantuan,2025-06-27 10:39:37+00:00,"Fix initialization of OneFormer (#38901)

* fix initialization of OneFormer

* remove redundant initializations

* remove redundant initializations

* remove redundant initializations

* keep BC",57,52,109,3,6,3,0.5
540a10848c26ebec9a0e749d3808333bdae08167,ydshieh,2025-06-27 10:28:10+00:00,"fix `Gemma3nProcessorTest` (#39068)

* fix

* fix

* oups forgot style

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",7,1,8,32,1020,916,0.9
0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0,yaswanth19,2025-06-27 10:14:09+00:00,"Cleanup Attention class for Siglip and dependent models (#39040)

* cleanup attention class

* More models

* more models

* Changes

* make style

* Should fix CI

* This should work 🙏",23,97,120,5,18,13,0.72
1ccc73dee9018dad5dcbadff31851d7c663b8b51,eustlb,2025-06-27 09:27:42+00:00,"[Whisper] fix shape mismatch in tests (#39074)

fix shape mismatch",1,1,2,8,42,29,0.69
a52478253bbe522a420e88ea3940d4d98a935300,stevhliu,2025-06-26 21:40:45+00:00,"[docs] Tensor parallelism (#38241)

* updates

* feedback

* badges

* fix?

* fix?

* fix?

* fix?",212,209,421,5,145,131,0.9
84e8696caebea4cc8afb16a62d5eaae29f01fdd9,stevhliu,2025-06-26 21:21:54+00:00,"[docs] @auto_docstring (#39011)

* refactor

* feedback",107,103,210,5,145,131,0.9
018855de636538aeaf9f49c596f9682431d87f53,dross20,2025-06-26 20:54:48+00:00,"Update PEGASUS-X model card (#38971)

* Update PEGASUS-X model card

* Add cache_implementation argument in quantization code example

* Update CLI example

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Remove TensorFlow and Flax badges

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",106,26,132,3,6,6,1.0
757c26fb40cbeeef3a1288219503acd23febd034,stevhliu,2025-06-26 19:25:14+00:00,"[docs] Model contribution (#38995)

improve",5,5,10,5,145,131,0.9
b372bb5ed1ef618739ee205e629204a866dd755e,ydshieh,2025-06-26 18:07:17+00:00,"fix `layoutlmv3` tests (#39050)

* fix

* fix

* fix

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",4,2,6,32,1020,916,0.9
f171e7e884f4435a372b0690a50db251bc4302a8,sbucaille,2025-06-26 17:13:06+00:00,"Update SuperPoint model card (#38896)

* docs: first draft to more standard SuperPoint documentation

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* docs: reverted changes on Auto classes

* docs: addressed the rest of the comments

* docs: remove outdated reference to keypoint detection task guide in SuperPoint documentation

* Update superpoint.md

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",85,85,170,6,16,11,0.69
2f50230c59ec9f17431236ed6625082cc385c76c,ydshieh,2025-06-26 16:48:14+00:00,"fix `t5gemma` tests (#39052)

* fix

* fix

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",27,6,33,32,1020,916,0.9
23b7e73f0581a880370477597dc948e07c2f064b,ydshieh,2025-06-26 16:36:56+00:00,"fix `test_compare_unprocessed_logit_scores` (#39053)

fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",1,1,2,32,1020,916,0.9
58c768922618cf11ce769fb8368c26c6db54c535,vasqu,2025-06-26 16:23:55+00:00,"[`Flex Attn`] Fix torch 2.5.1 incompatibilities (#37406)

* remove compile on mask creation, ensure kv blocks do not explode on indices

* trigger ci

* switch dynamic compilation to false

* patch new masking functions as well

* add len check

* i was wrong

* last comment",40,6,46,5,41,32,0.78
5154497607970fbd8a03f89a767dffb65619b5ce,LysandreJik,2025-06-26 16:04:36+00:00,Dev version,53,53,106,6,555,484,0.87
0a8081b03d118da9a8c3fa143a03afe54a5c624e,kylesayrs,2025-06-26 15:56:33+00:00,"[Modeling] Fix encoder CPU offloading for whisper (#38994)

* fix cpu offloading for whisper

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>

* unskip offloading tests

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>

* revert small change

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>

* remove tests

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>

---------

Signed-off-by: Kyle Sayers <kylesayrs@gmail.com>",2,18,20,4,15,13,0.87
c63cfd6a833d629a74c098933017c61dd755969d,RyanMullins,2025-06-26 15:55:47+00:00,"Gemma 3n (#39059)

* Gemma 3n

* initial commit of Gemma 3n scaffold

* Fixing param pass through on Gemm3p5RMSNorm

* Adds Einsum layer to Gemma 3n

* Updating EinsumLayer API

* Undoing erroneous force push

* Reverting RMSNorm to with_scale by default

* Adds LAuReL to Gemma 3n

* Adds AltUp to Gemma 3n

* Adding Gemma3p5 overall and text config with vision and audio config placeholders (#3)

* Adding gemma3p5 text configs

* Adding audio config placeholders

* Adding a placeholder for vision configs

* Updating MobileNetVisionConfig, inheriting TimmWrapperConfig

* Updating text configs

* Update src/transformers/models/gemma3p5/modular_gemma3p5.py

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Removing altup configs to accept the suggested configs

* Update src/transformers/models/gemma3p5/modular_gemma3p5.py

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Updating altup config

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Addressing review comments and updating text configs

* Adding a config for activation sparsity

* Updating configs to pass through options to super class init and adjust some name prefixes

* Updating laurel and altup with corrected config values

* Normalizing sub_config initializers

---------

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Updating MLP with activation sparsity (#2)

* Updating DecoderBlock for Gemma 3n (#3)

* Initial Gemm3nTextModel (#4)

NOTE: This implementation WILL CHANGE in the coming weeks, however, changes will be strictly additive and this will remain a suitable baseline for downstream implementations to reference.

* Adding KV Cache Sharing

* Adds Einsum layer to Gemma 3n

* Updating EinsumLayer API

* Refactored kv cache sharing in attention

* Adding KVStore for cache sharing

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update src/transformers/cache_utils.py

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Undoing erroneous force push

* Reverting RMSNorm to with_scale by default

* Adds LAuReL to Gemma 3n

* Updating KV Cache Sharing implementation

* Updating the q and k norm definitions in the attention module

* Fixing name error for q,k,v RMS norm to use the right 3n module

* Updating MLP with activation sparsity

* Updating DecoderBlock for Gemma 3.5

* Updating kv cache sharing implementation with the use of a cache buffer and refactoring some lines of code

* Isolating KV Cache logic to relevant components

* Fixing logic error in Gemma3nAttention.forward

* Refactoring caching contributions and fixing kv_store initialization

* Simplifying Configs

* Remove errant self from super init call

* Bug fix in the Attention module - changing self.head_dim to config.head_dim

* Bug fixes in the LaurelBlock and RMS Norm super init call

* removing redundant code from a merge

* Adding per_layer_inputs to TextModel

* Adding preprocess embeddings with altup

* Adds per-layer-to-single output and a host of TODOs

* Integrating altup predict with the model workflow and other minor bug fixes

* Using nn.Embedding temporarily for text model

* It goes forward

* Minor refactor of attention sparsity and RoPE initialization

* Fixing duplicate rope_scaling param bug when loading from pretrained

---------

Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>
Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>

* Normalizing on altup_num_inputs config option

* regenerating modeling file after syncing to HEAD

* Use torch.std(..., unbiased=False) for activation sparsity (#8)

* Refactoring to a single QVK Norm (#13)

* AltUp: support scale_corrected_output (#14)

* Converts einsums to nn.Linear (#7)

* Converts einsums to nn.Linear

* Removing unused variables

* Aligning SharedKVCache with HybridCache (#11)

* Alinging SharedKVStore with HybridCache

* Remove KVStore. Refactor apply_rotary_pos_emb for sharing

* Addressing review comments

* Supporting split modality embeddings in Gemma3n (#10)

* Adding the Embedder class

* Update modular

Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>

* Update modular

Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>

* Update modular

Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>

* Update modular

Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>

* Update modular

Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>

* Update modular

Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>

* Addressing review comments, adding audio embedding layers, integrating embedder with the remaining architecture, adding a forward method for conditional generation

* Apply suggestions from code review

Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>

* Update modular

Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>

* Addressing review comments, prop drilling audio and vision configs to the text config

* Removing TODO's that have been addressed

* Simplify Embedder init and add audio embeddings

* Embeddings refactor. Adds Gemma3nAudioEmbedder and Gemma3nVisionEmbedder

* Refactoring vision and audio embeddings into ConditionalGeneration model

---------

Co-authored-by: Ryan Mullins <ryan@ryanmullins.org>
Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Updating attention mask for Gemma 3.5 (#15)

* xxx_token_index to xxx_token_id

* remvoing deprecated last_cache_position

* Removing references to SigLIP

* Always init per-layer inputs

* Using torch.finfo().min for epsilon_tensor

* Gemma3nDecoderLayer inherits from Gemma3DecoderLayer. Remove gating lambdas

* fix modular GEMMA3N_INPUTS_DOCSTRING

* Gemma3nAttention inherits from Gemma3Attention

* Modular inheritance fixes

* CausalLM conversion script for 4B model (#16)

* Add Gemma3n Audio Encoder (#6)

* initial commit of Gemma 3.5 scaffold

* Fixing param pass through on Gemm3nRMSNorm

* Adds Einsum layer to Gemma 3.5

* Updating EinsumLayer API

* Undoing erroneous force push

* Reverting RMSNorm to with_scale by default

* Adds LAuReL to Gemma 3n

* Adds AltUp to Gemma 3n

* Adding Gemma3n overall and text config with vision and audio config placeholders (#3)

* Adding gemma3n text configs

* Adding audio config placeholders

* Adding a placeholder for vision configs

* Updating MobileNetVisionConfig, inheriting TimmWrapperConfig

* Updating text configs

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Removing altup configs to accept the suggested configs

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Updating altup config

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Addressing review comments and updating text configs

* Adding a config for activation sparsity

* Updating configs to pass through options to super class init and adjust some name prefixes

* Updating laurel and altup with corrected config values

* Normalizing sub_config initializers

---------

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Updating MLP with activation sparsity (#2)

* Updating DecoderBlock for Gemma 3.5 (#3)

* Initial Gemm3nTextModel (#4)

NOTE: This implementation WILL CHANGE in the coming weeks, however, changes will be strictly additive and this will remain a suitable baseline for downstream implementations to reference.

* Adding KV Cache Sharing

* Adds Einsum layer to Gemma 3.5

* Updating EinsumLayer API

* Refactored kv cache sharing in attention

* Adding KVStore for cache sharing

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update src/transformers/cache_utils.py

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Undoing erroneous force push

* Reverting RMSNorm to with_scale by default

* Adds LAuReL to Gemma 3n

* Updating KV Cache Sharing implementation

* Updating the q and k norm definitions in the attention module

* Fixing name error for q,k,v RMS norm to use the right Gemma 3n module

* Updating MLP with activation sparsity

* Updating DecoderBlock for Gemma 3.5

* Updating kv cache sharing implementation with the use of a cache buffer and refactoring some lines of code

* Isolating KV Cache logic to relevant components

* Fixing logic error in Gemma3nAttention.forward

* Refactoring caching contributions and fixing kv_store initialization

* Simplifying Configs

* Remove errant self from super init call

* Bug fix in the Attention module - changing self.head_dim to config.head_dim

* Bug fixes in the LaurelBlock and RMS Norm super init call

* removing redundant code from a merge

* Adding per_layer_inputs to TextModel

* Adding preprocess embeddings with altup

* Adds per-layer-to-single output and a host of TODOs

* Integrating altup predict with the model workflow and other minor bug fixes

* Using nn.Embedding temporarily for text model

* It goes forward

* Minor refactor of attention sparsity and RoPE initialization

* Fixing duplicate rope_scaling param bug when loading from pretrained

---------

Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>
Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>

* Normalizing on altup_num_inputs config option

* Adding audio encoder config

* Adds high-level components for Audio Encoder

* Implement uniform reducer for Audio Encoder

* Adding placeholders for Conformer components in Audio Encoder

* Adding placeholders for SubSampleConvProjection components in Audio Encoder

* Adding SequenceLayer component placeholders

* Implementing Gemma3nAudioEncoder with nn.Sequential

* Implementing Gemma3nAudioSubSampleConvProjection with nn.Sequential

* Implementing Conformer model with SequenceLayers

* Use OrderedDict in nn.Sequential initializers

* Implements sl.Residual in Torch with nn.Sequential and OrderedDict

* Adopting a base SequenceLayer class with default forward() method

* Implementing sl.GatedLinearUnit in Torch

* Implementing sl.Swish in Torch

* Implementing sl.ReLU in Torch

* Implementing sl.Scale in Torch

* Removing sl.Dropout after tree-shaking

* Implementing sl.RMSNorm in Torch with fake shape

* Implementing sl.GroupNorm in Torch

* Implementing sl.Conv2d in Torch

* Implementing sl.Dense in Torch

* Removing sl.Delay layers, which act as pass-throughs

* Connecting shapes to configs in initializers

* Removing sl.Emit

* Implementing sl.ExpandDims in Torch

* Adding sl.GradientClipping to Torch

* Implementing sl.DenseShaped in Torch

* Implementing sl.LDPA in Torch

* Removing unused sl.CombinedQKVProj class

* Fixing erroneous type hint

* Implemnenting sl.DepthwiseConv1D in Torch

* Implementing sl.MaskInvalid in Torch

* Fixes for initialization

* Fixes for saving weights

* Removing einsums per feedback from HF staff

* Removing Sequence Layers idioms from audio encoder

* Fixes for reviewer comments

* CausalLM conversion script for 4B model

* inv_timescales to non-persistent buffer

* Addressing audio encoder Attention feedback

* Addressing Gemma3nAudioSSCPConvBlock feedback

* Addressing Gemma3nAudioConformerAttention feedback

* Addressing padding feedback

* Weights conversion loads audio state dict

* Always use vision_config so saving works

* Token id updates for configs

* Stubs for interleaving audio embs

* Addressing reviewer feedback

---------

Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>
Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>

* Fixing cache access error

* Removing duplicate code from a bad merge

* Gemma 3n Text + Vision Part 1 (#17)

* testing utilities for numerics comparisons

* Corrected einsum to nn.Linear weights conversion

* Inherit scaled word embs from Gemma3 not Bart

* Fixing transposes for collapsed linears

* More transpose fixes

* numpy api fix

* RMSNorm: Explicit kwargs, scale_shift=0.0 when with_scale=True

* Force AltUp  to float32

* Updating debugging script for AudioEncoder debugging

* Support divide_weight_by_sqrt_fan_in from JAX for per-layer inputs

* Correcting attention einsum conversions

* RMSNorm in type of x

* Fixing douplicate laurel norm/gating

* KV sharing using the right previous indices

* Refactor kv shared index computation. Correct frac_shared_layers

* Use num_shared_layers instead of inferring from a fraction

* fixing a bug for logging

* Fix shared data_ptrs in altup inits

* rope: adjust proj -> norm -> rope to preserve computation (#20)

* rope: adjust proj -> norm -> rope to preserve computation

* Removing some breaking language model fluff in ConditionalGeneration

* Consolidate query_states transforms

---------

Co-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>
Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Vectorize the loops in AltUp (#19)

* Vectorize the loops in AltUp

* fix typo

* Expanding to support batched inputs

* remove extra debug script

* Fix AltUp.forward

---------

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Add 'scale_shift=0.0, with_scale=True' to the final norm in TextModel

* Convert norm to 1/sqrt (#21)

* Convert norm to 1/sqrt

* Scale shift change per Phil's rec

* Adding default activation sparsity

* Fixing 2B config in weights conversion script

* Fixing RMSNorm parameters - adding scale_shift and with_scale

* Correcting query pre-attention scaling

* Adding query_rescale_scalar to text config

* Adding layer_idx to MLP

* Permafix for input_layernorm

* Use 1/sqrt instead of rsqrt in DecoderLayer

* Fix o_proj conversion

* Conversion script update for vision encoder

* Removing logging for debugging timm model

* Fixing bugs in Gemma3nForConditionalGeneration for text generation

* Generating the modeling_gemma3n.py file

* Removing the addition of an erroneous line in the modeling file

* Adding gemma3n text model to modeling_auto

* Bugfix: Updating the interleaving of inputs_embeds and vision_embeds

* Updating the modeling file with the latest bugfix changes

* Updating models/auto for Gemma 3n

* using AutoTokenizer in forward test

* Adding processing_gemma3n.py

* Gemma 3n configured for AutoModel. Conversion script updated.

* Removing errant merge artifacts

---------

Co-authored-by: Mayank Chaturvedi <imayank@google.com>
Co-authored-by: Douglas Reid <douglas-reid@users.noreply.github.com>
Co-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>
Co-authored-by: Xuan-Son Nguyen <thichthat@gmail.com>
Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>

* Removing errant debugging statements from Gemma 3

* Gemma3n audio model (#18)

* testing utilities for numerics comparisons

* Implement CumulativeGroupNorm and add to SubSampleConvProjection and SSCPConvBlock

* Add audio version of forward script based on RyanMullins' implementation

* Updating to match encoder tests. WIP: config question needs resolving

* Updates to audio classes to enable end-to-end running

* Removing vestigial classes, cleaning up print statements

* Adding SiLU / Swish to audio conformer feed forward block

* Shifted Gemma3p5Audio naming prefix to Gemma3NanoAudio

* Adding outputs to audio test

* Fixes to padding in SSCP and 1D convolution, align RMS Norm with wider model

* Update forward test to load from local weights

* Update conversion to process / output audio layers

* Update __all__ to export audio encoder

* AutoModel registration for Gemma 3n Audio

* Use AutoModel for ConditionalGeneration.audio_tower

* Fixing input_proj_linear transpose

* Fixing Gemma3NanoAudioConformerAttention.post conversion

* Fixing Gemma3NanoAudioSSCPConvBlock.conv weights conversion

* Correcting indentation issue on Gemma3p5RMSNorm

---------

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Text + Vision Part 2 (#23)

* Updates for ConditionalGeneration.get_image_features

* Adding a WIP draft of image_processing_gemma3p5.py

* Update src/transformers/models/gemma3p5/modular_gemma3p5.py

Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>

* Modular conversion after github suggested change

* Text + image gives good results

* Fixing image size preset

* Updating configs for the 2B variant in the conversion script

* Using final generation config in conversion script

---------

Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>
Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>

* Audio Integration (#12)

* initial commit of Gemma 3n scaffold

* Fixing param pass through on Gemm3nRMSNorm

* Adds Einsum layer to Gemma 3n

* Updating EinsumLayer API

* Undoing erroneous force push

* Reverting RMSNorm to with_scale by default

* Adds LAuReL to Gemma 3n

* Adds AltUp to Gemma 3n

* Adding Gemma 3n overall and text config with vision and audio config placeholders (#3)

* Adding Gemma 3n text configs

* Adding audio config placeholders

* Adding a placeholder for vision configs

* Updating MobileNetVisionConfig, inheriting TimmWrapperConfig

* Updating text configs

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Removing altup configs to accept the suggested configs

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Updating altup config

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Addressing review comments and updating text configs

* Adding a config for activation sparsity

* Updating configs to pass through options to super class init and adjust some name prefixes

* Updating laurel and altup with corrected config values

* Normalizing sub_config initializers

---------

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Updating MLP with activation sparsity (#2)

* Updating DecoderBlock for Gemma 3n (#3)

* Initial Gemma3nTextModel (#4)

NOTE: This implementation WILL CHANGE in the coming weeks, however, changes will be strictly additive and this will remain a suitable baseline for downstream implementations to reference.

* Adding KV Cache Sharing

* Adds Einsum layer to Gemma 3n

* Updating EinsumLayer API

* Refactored kv cache sharing in attention

* Adding KVStore for cache sharing

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update modular

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Update src/transformers/cache_utils.py

Co-authored-by: Ryan Mullins <ryanmullins@google.com>

* Undoing erroneous force push

* Reverting RMSNorm to with_scale by default

* Adds LAuReL to Gemma 3n

* Updating KV Cache Sharing implementation

* Updating the q and k norm definitions in the attention module

* Fixing name error for q,k,v RMS norm to use the right 3n module

* Updating MLP with activation sparsity

* Updating DecoderBlock for Gemma 3n

* Updating kv cache sharing implementation with the use of a cache buffer and refactoring some lines of code

* Isolating KV Cache logic to relevant components

* Fixing logic error in Gemma3nAttention.forward

* Refactoring caching contributions and fixing kv_store initialization

* Simplifying Configs

* Remove errant self from super init call

* Bug fix in the Attention module - changing self.head_dim to config.head_dim

* Bug fixes in the LaurelBlock and RMS Norm super init call

* removing redundant code from a merge

* Adding per_layer_inputs to TextModel

* Adding preprocess embeddings with altup

* Adds per-layer-to-single output and a host of TODOs

* Integrating altup predict with the model workflow and other minor bug fixes

* Using nn.Embedding temporarily for text model

* It goes forward

* Minor refactor of attention sparsity and RoPE initialization

* Fixing duplicate rope_scaling param bug when loading from pretrained

---------

Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>
Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>

* Normalizing on altup_num_inputs config option

* Adding audio encoder config

* Adds high-level components for Audio Encoder

* Implement uniform reducer for Audio Encoder

* Adding placeholders for Conformer components in Audio Encoder

* Adding placeholders for SubSampleConvProjection components in Audio Encoder

* Adding SequenceLayer component placeholders

* Implementing Gemma3nAudioEncoder with nn.Sequential

* Implementing Gemma3nAudioSubSampleConvProjection with nn.Sequential

* Implementing Conformer model with SequenceLayers

* Use OrderedDict in nn.Sequential initializers

* Implements sl.Residual in Torch with nn.Sequential and OrderedDict

* Adopting a base SequenceLayer class with default forward() method

* Implementing sl.GatedLinearUnit in Torch

* Implementing sl.Swish in Torch

* Implementing sl.ReLU in Torch

* Implementing sl.Scale in Torch

* Removing sl.Dropout after tree-shaking

* Implementing sl.RMSNorm in Torch with fake shape

* Implementing sl.GroupNorm in Torch

* Implementing sl.Conv2d in Torch

* Implementing sl.Dense in Torch

* Removing sl.Delay layers, which act as pass-throughs

* Connecting shapes to configs in initializers

* Removing sl.Emit

* Implementing sl.ExpandDims in Torch

* Adding sl.GradientClipping to Torch

* Implementing sl.DenseShaped in Torch

* Implementing sl.LDPA in Torch

* Removing unused sl.CombinedQKVProj class

* Fixing erroneous type hint

* Implemnenting sl.DepthwiseConv1D in Torch

* Implementing sl.MaskInvalid in Torch

* Fixes for initialization

* Fixes for saving weights

* Removing einsums per feedback from HF staff

* Removing Sequence Layers idioms from audio encoder

* Fixes for reviewer comments

* Converting sl.Frontend to FeatureExtractor

* Updates for ConditionalGeneration.get_image_features

* Adding a WIP draft of image_processing_gemma3n.py

* Update modular

Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>

* Modular conversion after github suggested change

* Text + image gives good results

* Fixing image size preset

* Draft of audio data in chat template

* Removing image processing. Using SigLIP instead.

* Audio input going end-to-end

* Fixing dtype issues in audio encoder

* x-lib formatting consistency

* Adding example data

* Save preprocessor_config.json from conversion script

* Instrumentaiton for debugging

* Additional instrumentation for preprocessing debugging

* Updates to preprocessor, padding; produces correct end-to-end results on sample

* Tackling configuraiton TODOs

* Start of feature extractor refatcor

* Adds Numpy version of USM extractor, removes Torch version and dependencies

* Fixing AltUp.correct coef permute

* Supporting batches of single audio segment inputs

* Docstrings updates for config

* In-lining audio feature extraction

* Adjustments to conversion script and smoke test script

---------

Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>
Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>
Co-authored-by: pculliton <phillipculliton@gmail.com>

* Gemma 3n renaming

* Removing test data and utilities

* Renaming test files

* Gemma 3n refactor

* Fix tokenizer config in conversion script

* Address reviewer feedback

* FeatureExtractor returns float32 by default

* Adding basic tests for audio, and input name for audio encoder

* Audio integration test, updates to model_id for other integration tests

* Use scales for q and k norms (#26)

* Update audio integration test to use HF dataset

* Reviewer feedback

* Expand embedding table to full vocab size in weights conversion

* Mix-n-match MatFormers for Gemma 3n (#25)

* Remove in-place operations (#30)

* chore: removing inplace ops

* remove [tensor] * n pattern

* chore: reviewer feedback in AudioEncoder and AltUp

* More grad clipping

* Dynamo compatibility

* fix: cache slicing error

* chore: simplify shared kv cache slicing

* chore: vision encoder rename in timm

* fix: image processor do_normalize=False

* fixup: style

* chore: model_doc

* fix: docs for code quality

* chore: repo consistency

* fix: RMSNorm in float as in prior Gemmas

* fix: per_layer_inputs = None

* chore: Gemma3nForCausalLM from Gemma3nForConditionalGeneration checkpoint

* chore: repo consistency

* Add initial unit tests for Gemma3nAudioFeatureExtractor (#27)

* Add initial unit tests for Gemma3nAudioFeatureExtractor

* Add basic unit tests for Gemma3nProcessor (#28)

Co-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>

* parameterize tests

---------

Co-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>

* chore: code style

* fix: test cases

* style and consistency

* fix config in the test to be coherent with layer cache sharing

* fix hidden states in tests and code

* inits and mappings

* fix modality prefixes

* test order and prefixes

* fix test exception

* fix class order and reduce model size for faster tests

* restore _checkpoint_conversion_mapping to load Caual from Conditional

* fix config mapping!

* fix: reviewer feedback

---------

Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>
Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>
Co-authored-by: raushan <raushan@huggingface.co>
Co-authored-by: Mayank Chaturvedi <imayank@google.com>
Co-authored-by: Douglas Reid <douglas-reid@users.noreply.github.com>
Co-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>
Co-authored-by: Xuan-Son Nguyen <thichthat@gmail.com>
Co-authored-by: pculliton <phillipculliton@gmail.com>
Co-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com>
Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>

* fix import test

* add model args

* auto_docstring

* replace test path

* consistency

* skip tests for now

* fix docstring for doc builder

* skip unused attr

---------

Co-authored-by: SindhuRaghuram97 <114270661+SindhuRaghuram97@users.noreply.github.com>
Co-authored-by: Sindhu Raghuram <sindhuraghuram@google.com>
Co-authored-by: raushan <raushan@huggingface.co>
Co-authored-by: Mayank Chaturvedi <imayank@google.com>
Co-authored-by: Douglas Reid <douglas-reid@users.noreply.github.com>
Co-authored-by: Douglas Reid <21148125+douglas-reid@users.noreply.github.com>
Co-authored-by: Xuan-Son Nguyen <thichthat@gmail.com>
Co-authored-by: pculliton <phillipculliton@gmail.com>
Co-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com>
Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>
Co-authored-by: Arthur <arthur.zucker@gmail.com>",8723,0,8723,2,9,8,0.89
3e5cc1285503bbdb6a0a3e173b5ae90566862215,gante,2025-06-26 15:25:00+00:00,"[tests] remove tests from libraries with deprecated support (flax, tensorflow_text, ...) (#39051)

* rm tf/flax tests

* more flax deletions

* revert fixture change

* reverted test that should not be deleted; rm tf/flax test

* revert

* fix a few add-model-like tests

* fix add-model-like checkpoint source

* a few more

* test_get_model_files_only_pt fix

* fix test_retrieve_info_for_model_with_xxx

* fix test_retrieve_model_classes

* relative paths are the devil

* add todo",156,691,847,10,616,539,0.88
cfff7ca9a27280338c6a57dfa7722dcf44f51a87,eustlb,2025-06-26 14:33:31+00:00,"[Whisper] Pipeline: handle long form generation (#35750)

* handle long form generation

* add warning

* correct incorrect in place token change

* update test to catch edge case

* make style

* update warning

* add doc",64,17,81,8,42,29,0.69
02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254,eustlb,2025-06-26 13:55:28+00:00,"add _keep_in_fp32_modules_strict (#39058)

* add _keep_in_fp32_modules_strict

* complete test",111,17,128,8,42,29,0.69
d973e62fdd86d64259f87debc46bbcbf6c7e5de2,Vaibhavs10,2025-06-26 12:52:57+00:00,"fix condition where torch_dtype auto collides with model_kwargs. (#39054)

* fix condition where torch_dtype auto collides with model_kwargs.

* update tests

* update comment

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",19,11,30,2,8,6,0.75
44b231671db25974cfebcdae34402ad5099bf37a,zucchini-nlp,2025-06-26 12:06:52+00:00,"[qwen2-vl] fix vision attention scaling (#39043)

scale lost its `-` when refactoring",4,6,10,20,298,253,0.85
ae15715df138949328d18e1dd95fd9cb4efb8e09,eeemmmmmm,2025-06-26 11:56:31+00:00,"polishing docs: error fixes for clarity (#39042)

* fix duplicate deprecate_models.py

* fix duplicate modular_model_converter.py",2,2,4,1,0,0,
3abeaba7e53512ef9c1314163dd7e462ab405ce6,manueldeprada,2025-06-26 11:54:36+00:00,"Create test for #38916 (custom generate from local dir with imports) (#39015)

* create test for #38916 (custom generate from local dir with imports)",22,0,22,2,20,12,0.6
25c44d4b68d4a0feafb3a5a3fc640d04cf59d5a9,remi-or,2025-06-26 11:44:59+00:00,"Internvl fix (#38946)

* Image processor compile fix (#38540)

* Added a compile-friendly versiom of resize to BaseImgProcessorFast

* Changed qwen2 processor to use its parent class .resize

* Style

* underlined issue only happens on AMD w/ comment and bool check

* Fixed some utils functions

* Fixed the same issue for bridgetower

* Fixed the same issue for llava_next

* Repo consistency for llava onevision

* Update src/transformers/image_processing_utils_fast.py

Co-authored-by: Mohit Sharma <mohit21sharma.ms@gmail.com>

---------

Co-authored-by: Mohit Sharma <mohit21sharma.ms@gmail.com>

* Added an Expectation to an internvl test

* Made qwen2_vl use the resize method of its parent clas

* Changed to torch.where

---------

Co-authored-by: Mohit Sharma <mohit21sharma.ms@gmail.com>",53,8,61,4,21,20,0.95
f85b47d1b8820fefc8fbe2704a2fd67e908f9614,vasqu,2025-06-26 11:06:09+00:00,"[`Generate`] Fix no grad on some models (#39008)

fixes on torch no grad for generate",5,0,5,5,41,32,0.78
583db52bc6d5415a205724776136d094ff70c9a4,buttercrab,2025-06-26 11:04:23+00:00,"Add Dia model (#38405)

* add dia model

* add tokenizer files

* cleanup some stuff

* brut copy paste code

* rough cleanup of the modeling code

* nuke some stuff

* more nuking

* more cleanups

* updates

* add mulitLayerEmbedding vectorization

* nits

* more modeling simplifications

* updates

* update rope

* update rope

* just fixup

* update configuration files

* more cleanup!

* default config values

* update

* forgotten comma

* another comma!

* update, more cleanups

* just more nits

* more config cleanups

* time for the encoder

* fix

* sa=mall nit

* nits

* n

* refacto a bit

* cleanup

* update cv scipt

* fix last issues

* fix last nits

* styling

* small fixes

* just run 1 generation

* fixes

* nits

* fix conversion

* fix

* more fixes

* full generate

* ouf!

* fixes!

* updates

* fix

* fix cvrt

* fixup

* nits

* delete wrong test

* update

* update

* test tokenization

* let's start changing things bit by bit - fix encoder step

* removing custom generation, moving to GenerationMixin

* add encoder decoder attention masks for generation

* mask changes, correctness checked against ad29837 in dia repo

* refactor a bit already --> next cache

* too important not to push :)

* minimal cleanup + more todos

* make main overwrite modeling utils

* add cfg filter & eos filter

* add eos countdown & delay pattern

* update eos countdown

* add max step eos countdown

* fix tests

* fix some things

* fix generation with testing

* move cfg & eos stuff to logits processor

* make RepetitionPenaltyLogitsProcessor flexible

- can accept 3D scores like (batch_size, channel, vocab)

* fix input_ids concatenation dimension in GenerationMixin for flexibility

* Add DiaHangoverLogitsProcessor and DiaExponentialDecayLengthPenalty classes; refactor logits processing in DiaForConditionalGeneration to utilize new configurations and improve flexibility.

* Add stopping criteria

* refactor

* move delay pattern from processor to modeling like musicgen.

- add docs
- change eos countdown to eos delay pattern

* fix processor & fix tests

* refactor types

* refactor imports

* format code

* fix docstring to pass ci

* add docstring to DiaConfig & add DiaModel to test

* fix docstring

* add docstring

* fix some bugs

* check

* porting / merging results from other branch - IMPORTANT: it very likely breaks generation, the goal is to have a proper forward path first

* experimental testing of left padding for first channel

* whoops

* Fix merge to make generation work

* fix cfg filter

* add position ids

* add todos, break things

* revert changes to generation --> we will force 2d but go 3d on custom stuff

* refactor a lot, change prepare decoder ids to work with left padding (needs testing), add todos

* some first fixes to get to 10. in generation

* some more generation fixes / adjustment

* style + rope fixes

* move cfg out, simplify a few things, more todos

* nit

* start working on custom logit processors

* nit

* quick fixes

* cfg top k

* more refactor of logits processing, needs a decision if gen config gets the new attributes or if we move it to config or similar

* lets keep changes to core code minimal, only eos scaling is questionable atm

* simpler eos delay logits processor

* that was for debugging :D

* proof of concept rope

* small fix on device mismatch

* cfg fixes + delay logits max len

* transformers rope

* modular dia

* more cleanup

* keep modeling consistently 3D, generate handles 2D internally

* decoder starts with bos if nothing

* post processing prototype

* style

* lol

* force sample / greedy + fixes on padding

* style

* fixup tokenization

* nits

* revert

* start working on dia tests

* fix a lot of tests

* more test fixes

* nit

* more test fixes + some features to simplify code more

* more cleanup

* forgot that one

* autodocs

* small consistency fixes

* fix regression

* small fixes

* dia feature extraction

* docs

* wip processor

* fix processor order

* processing goes brrr

* transpose before

* small fix

* fix major bug but needs now a closer look into the custom processors esp cfg

* small thing on logits

* nits

* simplify indices and shifts

* add simpler version of padding tests back (temporarily)

* add logit processor tests

* starting tests on processor

* fix mask application during generation

* some fixes on the weights conversion

* style + fixup logits order

* simplify conversion

* nit

* remove padding tests

* nits on modeling

* hmm

* fix tests

* trigger

* probably gonna be reverted, just a quick design around audio tokenizer

* fixup typing

* post merge + more typing

* initial design for audio tokenizer

* more design changes

* nit

* more processor tests and style related things

* add to init

* protect import

* not sure why tbh

* add another protect

* more fixes

* wow

* it aint stopping :D

* another missed type issue

* ...

* change design around audio tokenizer to prioritize init and go for auto - in regards to the review

* change to new causal mask function + docstrings

* change ternary

* docs

* remove todo, i dont think its essential tbh

* remove pipeline as current pipelines do not fit in the current scheme, same as csm

* closer to wrapping up the processor

* text to audio, just for demo purposes (will likely be reverted)

* check if it's this

* save audio function

* ensure no grad

* fixes on prefixed audio, hop length is used via preprocess dac, device fixes

* integration tests (tested locally on a100) + some processor utils / fixes

* style

* nits

* another round of smaller things

* docs + some fixes (generate one might be big)

* msytery solved

* small fix on conversion

* add abstract audio tokenizer, change init check to abstract class

* nits

* update docs + fix some processing :D

* change inheritance scheme for audio tokenizer

* delete dead / unnecessary code in copied generate loop

* last nits on new pipeline behavior (+ todo on tests) + style

* trigger

---------

Co-authored-by: Arthur Zucker <arthur.zucker@gmail.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: Vasqu <antonprogamer@gmail.com>",5732,28,5760,1,2,1,0.5
5995cfa0a07de86e3c53fe1f57378c956a5d03db,alex-jw-brooks,2025-06-26 07:45:57+00:00,"Fix Bad Outputs in Fast Path for GraniteMoeHybrid (#39033)

Fix bug in previous state setting",6,3,9,1,14,9,0.64
22b0a898787f9e34c2b9b4ac1e53d2497c44ff39,avihu111,2025-06-26 07:44:17+00:00,"Granite speech speedup + model saving bugfix (#39028)

* ensure the query is updated during training

avoid unused parameters that DDP does not like

* avoid a crash when `kwargs` contain `padding=True`

trainers often pass this argument automatically

* minor

* Remove mel_spec lazy init, and rename to mel_filters.
this ensures save_pretrained will not crash when saving the processor during training
https://github.com/huggingface/transformers/blob/d5d007a1a0f0c11a726a54c8f00bd71825f84d02/src/transformers/feature_extraction_utils.py#L595

* minor - most feature extractors has a `sampling_rate` property

* speedup relative position embeddings

* fix several issues in model saving/loading:
- avoid modifying `self._hf_peft_config_loaded` when saving
- adapter_config automatically points to the original base model - a finetuned version should point to the model save dir.
- fixing model weights names, that are changed by adding an adapter.

* minor

* minor

* minor

* fixing a crash without peft active

* add todo to replace einsum",30,9,39,3,3,3,1.0
1d45d90e5d1552eccb6d8cc9b7bba283ccefb808,gante,2025-06-25 17:29:10+00:00,"[tests] remove TF tests (uses of `require_tf`) (#38944)

* remove uses of require_tf

* remove redundant import guards

* this class has no tests

* nits

* del tf rng comment",21,2504,2525,10,616,539,0.88
d37f7517972f67e3f2194c000ed0f87f064e5099,Rocketknight1,2025-06-25 16:31:26+00:00,"Two ReDOS fixes (#39013)

* two_redos_fixes

* Fix two redos issues

* Just don't use RE at all",7,8,15,4,398,351,0.88
551e48f182673cacd8ae91d839dd6962558d7b9e,eustlb,2025-06-25 16:09:00+00:00,"[Kyutai-STT] correct model type + model id (#39035)

* correct model type + model id

* udpate doc

* init fix

* style !!!",29,23,52,8,42,29,0.69
dad0e87c79d338f41176166b2e1e0591a87a81a1,anton-l,2025-06-25 15:12:15+00:00,"Add SmolLM3 (#38755)

* init smollm3

* integration tests

* config quirks

* docs stub

* rests round 2

* tests round 3

* tests round 4

* bring SWA back

* config checker pls

* final checkpoint

* style and copies

* Update src/transformers/models/smollm3/modular_smollm3.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/models/smollm3/modular_smollm3.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",1879,0,1879,1,47,41,0.87
3233e9b7c3745705c7047a823a19a6ac889239aa,eginhard,2025-06-25 15:07:52+00:00,"refactor: remove custom BarkLayerNorm (#39003)

`nn.LayerNorm` supports `bias=False` since Pytorch 2.1",5,18,23,1,2,1,0.5
3c1d4dfbac964dfc98c83cb30835e9058edecd63,marcndo,2025-06-25 14:55:22+00:00,Fix grammatical error in models documentation (#39019),1,1,2,2,7,3,0.43
858f9b71a8bc39b8ba64f9ca88194b195215aae9,lhoestq,2025-06-25 14:31:20+00:00,"Remove script datasets in tests (#38940)

* remove trust_remote_code

* again

* Revert ""Skip some tests for now (#38931)""

This reverts commit 31d30b72245aacfdf70249165964b53790d9c4d8.

* again

* style

* again

* again

* style

* fix integration test

* fix tests

* style

* fix

* fix

* fix the last ones

* style

* last one

* fix last

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",154,293,447,2,25,20,0.8
3c322c9cdf7d950ae54e0fa737de8435967aa01c,SunMarc,2025-06-25 14:28:44+00:00,"fix gemma3 grad acc (#37208)

* fix gemma3 grad acc

* fix

* fix

* fix

* fix

* rmv print

* rm

* Update setup.py

* Apply style fixes

* propagate the changes

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Co-authored-by: Arthur <arthur.zucker@gmail.com>",13,8,21,4,135,119,0.88
860b898d038f55c866d7ae07ba69bba69aa346de,umarbutler,2025-06-25 14:11:18+00:00,"fix: astronomical loss with ModernBERT when using gradient checkpointing (#38982) (#38983)

* fix: astronomical loss with ModernBERT when using gradient checkpointing

* update the modling fix

---------

Co-authored-by: Arthur <arthur.zucker@gmail.com>",2,2,4,1,3,3,1.0
a2eb75c891f6866cc9aeb66896be59f6c4ce100e,EduardDurech,2025-06-25 12:39:27+00:00,"Support for Flash Attention 3 (#38972)

* Support `flash_attn_3`
Implements fwd and tests for Flash Attention 3 https://github.com/Dao-AILab/flash-attention/commits/main/hopper

- Includes checks for dropout>0 and ALiBi in `modeling_utils.PreTrainedModel._check_and_enable_flash_attn_3` (Dropout will likely be supported soon, so this will need to be updated and `modeling_flash_attention_utils._flash_attention_forward` at the `if _IS_FLASH_ATTN_3_AVAILABLE: ...`

An example Llama implementation is included in `modeling_llama.py` but other models would still need to be updated

Based on https://github.com/huggingface/transformers/pull/36190 which has model implementations and examples which could be merged

* Add tests for Flash Attention 2 and 3 parity

* ci fix

* FA2 compatibiity
- `_prepare_flash_attention_from_position_ids` ->`prepare_fa2_from_position_ids`
- Remove bettertransformer check in Flash Attention 3
- Merge tests
- Add licensing

* ci fix

* Test naming consistency

* ci fix

* Deprecation warning for `prepare_fa2_from_position_ids`

* ci fix",697,261,958,1,3,1,0.33
de98fb25a3772b8fc4a31e55cb0b0560d97353af,yuanwu2017,2025-06-25 10:40:01+00:00,"Fix the seamless_m4t cannot work on Gaudi (#38363)

* Fix the seamless_m4t cannot work on Gaudi

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Refine the patch

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Fix seamless_m4t_v2 crash

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Use the patched_gather

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Remove debug logs

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Remove useless modifications

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Add hpu check

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Add comments

Signed-off-by: yuanwu <yuan.wu@intel.com>

---------

Signed-off-by: yuanwu <yuan.wu@intel.com>
Co-authored-by: Ilyas Moutawwakil <57442720+IlyasMoutawwakil@users.noreply.github.com>",22,0,22,1,10,8,0.8
7503cb911356abce1fc3b614193bd4384fee89cc,redmoe-moutain,2025-06-25 09:38:25+00:00,"[Model] add dots1 (#38143)

* add dots1

* address comments

* fix

* add link to dots1 doc

* format

---------

Co-authored-by: taishan <rgtjf1@163.com>",1239,0,1239,1,1,1,1.0
3ef889690649c082849c667be17b757c32955229,bzhangGo,2025-06-25 09:05:10+00:00,"Encoder-Decoder Gemma (#38332)

* Initial submit

* Fix bugs:
1. add __init__ file
2. tied word embedding
3. support flash/flex attention
4. model saving and loading

* Code refactor:
* Rename encdecgemma to t5gemma.
* Split attention into self- and cross-attention
* Split stack into encoder and decoder
* Add test cases
* Add auto configuration

* Update configurations.

* Fix bugs related to copy and attribute checks

* Fix type union

* Fix merge errors

* run ruff format

* Run make style and update tests.

* Add t5gemma model doc.

* ruff and style formatting.

* Add missed module config.

* Add dummy checkpoint link to pass tests (need updated when real checkpoints are uplioaded.).

* Update model doc.

* Minor updates following Arthur's comments:
* replace docstrings with auto_docstrings
* remove checkpoint layers
* remove deprecate_kwargs

* fix rebase errors

* Fix docstring issues.

* fix t5gemma doc issue.

* run ruff format

* Updates:
* split encoder-only model out
* make t5gemmamodel encoder-decoder only
* update token and sequence classification
* update tests",5148,0,5148,2,2,2,1.0
af9870265e817e57541d90c1797cb68959eb7b1e,zRzRzRzRzRzRzR,2025-06-25 08:43:05+00:00,"GLM-4.1V Model support (#38431)

* 20250508 Model Architecture

* Update modeling_glm4v.py

* Update modeling_glm4v.py

* Update modeling_glm4v.py

* update 1447

* 0526

* update

* format

* problem

* update

* update with only image embed diff

* Final

* upload

* update

* 1

* upload with ruff

* update

* update

* work

* 1

* 1

* update with new note

* 2

* Update convert_glm4v_mgt_weights_to_hf.py

* Update tokenization_auto.py

* update with new format

* remove rmsnrom

* draft with videos

* draft

* update

* update

* fix for review problem

* try to remove min_pixel

* update

* for test

* remove timestamps

* remove item

* update with remove

* change

* update 2200

* update

* Delete app.py

* format

* update

* Update test_video_processing_glm4v.py

* 1

* 2

* use new name

* Update test_video_processing_glm4v.py

* remove docs

* change

* update for image processors update

* 2108

* 2128

* Update modular_glm4v.py

* 1

* update some

* update

* rename

* 1

* remove tests output

* 2

* add configuration

* update

* Update test_video_processing_glm4v.py

* fix simple forward tests

* update with modular

* 1

* fix more tests

* fix generation test

* fix beam search and init

* modular changed

* fix beam search in case of single-image/video. Fails if multiple visuals per text

* update processor

* update test

* pass

* fix beam search

* update

* param correct

* Update convert_glm4v_mgt_weights_to_hf.py

* 1

* Update test_modeling_glm4v.py

* 4

* 2

* 2123 video process

* 2

* revert

* 1

* 2

* revert processing

* update preprocesor

* changed

* 1

* update

* update

* 6

* update

* update

* update

* Delete tmp.txt

* config

* Update video_processing_glm4v.py

* apply modular correctly

* move functions

* fix order

* update the longest_edge

* style

* simplify a lot

* fix random order of classes

* skip integration tests

* correctly fix the tests

* fix TP plan

---------

Co-authored-by: raushan <raushan@huggingface.co>
Co-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>
Co-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",6848,1,6849,3,8,4,0.5
7b3807387b5b24a98fc66101268972ac8e25d7ed,null-pointer-access,2025-06-25 08:29:00+00:00,"Drop unnecessary tokens in GPT2Model generation (#39016)

Drop unnecessary tokens in GPT2Model generation.

Co-authored-by: Yi Pan <conlesspan@outlook.com>",6,4,10,1,1,1,1.0
e212ff9e6aec58fc76086a1c6f5448b0c259dd18,zucchini-nlp,2025-06-25 08:23:37+00:00,"[video processor] support torchcodec and decrease cuda memory usage (#38880)

* don't move the whole video to GPU

* add torchcodec

* add tests

* make style

* instrucblip as well

* consistency

* Update src/transformers/utils/import_utils.py

Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>

* Update src/transformers/utils/import_utils.py

Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>

* Update src/transformers/video_utils.py

Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>

---------

Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",129,9,138,20,298,253,0.85
11d0feacce679f6931f5d032c74cd8167abb0db7,NielsRogge,2025-06-25 08:00:13+00:00,"[AutoModelForMaskGeneration] Remove duplicate code (#38622)

Remove duplicate code",0,0,0,1,336,277,0.82
3ee72af6b6133be5280a1abcf2cb7b497555f537,efsotr,2025-06-25 07:58:34+00:00,"Fix graph break in torch.compile when using FA2 with attention_mask=None and batch size > 1 (#37332)

* Fix graph break in torch.compile when using FA2 with attention_mask=None and batch size > 1

* fix code format

* add test; replace position_ids with query_states becasue position_ids.shape[0] is always 1

* add assert loss is not nan",43,2,45,1,6,4,0.67
ae32f1ad1102fbce259382dec7dd86e39ee23337,ranzhejiang,2025-06-25 07:48:50+00:00,"Add zero dim tensor check when using flash_attention (#38280)

* Add zero dim tensor check when using flash_attention

Signed-off-by: ranzhejiang <zhejiang.ran@intel.com>

* Add zero dim tensor check when using flash_attention

Signed-off-by: ranzhejiang <zhejiang.ran@intel.com>

---------

Signed-off-by: ranzhejiang <zhejiang.ran@intel.com>",7,0,7,1,1,1,1.0
ca402e2116f5917ce0a03659b779a02a555b285f,sbucaille,2025-06-24 22:32:07+00:00,"[LightGlue] Fixed attribute usage from descriptor_dim to keypoint_detector_descriptor_dim (#39021)

fix: fix descriptor dimension handling in LightGlue model",10,12,22,6,16,11,0.69
48b6ef02380f993a6e8dfa0c355f722c2b7b96ed,marcndo,2025-06-24 18:48:15+00:00,"Add Hugging Face authentication procedure for IDEs (PyCharm, VS Code,… (#38954)

* Add Hugging Face authentication procedure for IDEs (PyCharm, VS Code, etc.)

* Update quicktour.md

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",17,0,17,2,7,3,0.43
ea9a30923e5ea4d4afb02c41b1ab34093af3a700,dsmertin,2025-06-24 18:24:50+00:00,"[HPU][Critical Issue Fix] ThreadPool instead of Pool for parallel pre-processing (#39002)

* ThreadPool instead of Pool for parallel pre-processing

* ThreadPool only if hpu available",5,24,29,1,0,0,
995666edb5e9760e163567ee0dccba9a4394cbcd,ivarflakstad,2025-06-24 18:16:56+00:00,Skip sdpa dispatch on flash test due to unsupported head dims (#39010),4,0,4,2,43,34,0.79
f367c6337db43015d41a893e4338c2dd2963bd8a,ivarflakstad,2025-06-24 18:13:36+00:00,"Update self-comment-ci.yml user list (#39014)

add ivarflakstad to self-comment-ci.yml",1,1,2,2,43,34,0.79
67d36dc1d727d887b0ec91cc8e296ef1d216a792,tugsbayasgalan,2025-06-24 17:43:40+00:00,"Fix bugs in DynamicCache (#37880)

* Fix bugs in DynamicCache

* Updarte

* Update

* Lint

* lint

* Rename test

* update

* update",97,1,98,1,8,4,0.5
6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7,eustlb,2025-06-24 16:01:15+00:00,"Add kyutai stt (#38909)

* first draft

* cleaner version

* udpate tests + modeling

* add tests

* init

* udpate test_modeling_common

* fix tests

* csm Processor draft

* convertion update

* mimi cache padding convolutions draft

* mimi streaming udpates

* update mimi padding cache test

* udpate cache padding mimi test

* make style mimi

* updates generate moshi asr

* moshi asr integration tests (single + batched)

* update tests

* update conversion script

* good default sliding window value

* udpdate generate

* update test checkpoint

* nit

* fix mimi

* fix codec prefix

* revert

* revert

* update config

* update config

* unnecessary mimi input restriction

* remove delay in tokens

* remove _prepare_4d_causal_attention_mask_with_cache_position and _update_causal_mask

* test update

* modular update

* make style

* nit

* rename

* create codec model generation config at init

* remove delay

* max_new_tokens/length warning

* correct conv1 padding cache import for modular

* nit

* fix on encoder_past_key_values

* convert modular

* move frame_size to config

* move frame_size to config

* update test name

* handle first token is bos

* better handling of max_new_tokens

* fix

* fix batch size in test input prep

* update docstring

* convert modular

* make style

* make style

* add feature extractor

* correct modular convention name for feature_extraction file

* update convertion script

* doc processor

* update doc

* udpate init

* update model type

* fixes

* update tests

* fix

* make

* add doc

* nit

* fix

* doc

* auto mappings

* doc

* nit

* convert modular

* doc

* nit

* extend _keep_in_fp32_modules to enforce fp32

* renaming to stt

* doc update + test update

* doc fixes

* doc fix

* doc fix

* fix musicgen tests

* fix musicgen tests

* make style

* fix musicgen tests

* correct frame_rate config param for mimi

* update mimi test

* revert update mimi test

* enforce cpu test

* move cache init in cache class

* convert modular

* docstring update

* update model id

* feature_extractor -> feature_extraction (SEW)

* convert modular

* update model id",3999,199,4198,8,42,29,0.69
08bf7f1afee8c1127a28053cf452c44cf7e04d9c,MekkCyber,2025-06-24 15:38:54+00:00,"Add kernelize to transformers (#38205)

* fix

* fix

* fix flow

* remove non compiling path

* change

* style

* fix

* update

* update pin

* revert",13,43,56,2,82,73,0.89
be10d4df60bec044ac0c1ab6fd326479874baafc,avihu111,2025-06-24 15:06:52+00:00,"Granite speech - minor fixes to support training with the HF trainer (#38833)

* ensure the query is updated during training

avoid unused parameters that DDP does not like

* avoid a crash when `kwargs` contain `padding=True`

trainers often pass this argument automatically

* minor

* Remove mel_spec lazy init, and rename to mel_filters.
this ensures save_pretrained will not crash when saving the processor during training
https://github.com/huggingface/transformers/blob/d5d007a1a0f0c11a726a54c8f00bd71825f84d02/src/transformers/feature_extraction_utils.py#L595

* minor - most feature extractors has a `sampling_rate` property",9,24,33,3,3,3,1.0
e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5,Cyrilvallez,2025-06-24 15:04:33+00:00,"Fix undeterministic order in modular dependencies (#39005)

* sort correctly

* Update modeling_minimax.py

* Update modular_model_converter.py",238,1602,1840,14,132,118,0.89
bdf5fb70aa11782cce22027d76879f71f4e41c1e,seven-mile,2025-06-24 14:33:48+00:00,"Skip non-selected experts for qwen3_moe (#38133)

* fix(qwen3moe): skip experts with no workload

* avoid tolist and also update other moe models

* fix: should squeeze 0-dim only",12,10,22,1,2,2,1.0
719058c6255aa877eabd4e0e1fb69460a1680e30,Tanuj-rai,2025-06-24 14:21:36+00:00,Update attention_visualizer.py (#37860),1,1,2,2,10,4,0.4
9f42c1f192cf2dcd9f05a2d8374e298aba1ef576,mylonjones,2025-06-24 13:24:02+00:00,"Added scikit-learn to the example image-classification requirements.txt (#37506)

Co-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",2,1,3,1,1,1,1.0
1636a7bcb942370bb4098c8e67e4c3d3fd6a1740,Cyrilvallez,2025-06-24 13:23:52+00:00,"Fixes for Arcee model (#39001)

* fix modular

* Update modular_arcee.py

* fix",32,99,131,14,132,118,0.89
71de20b818c3aa9715fb3d0e26f448ec534b03d2,Crystalcareai,2025-06-24 13:05:29+00:00,"Add Arcee model support (#38621)

* Add Arcee model support to transformers

- Add ArceeConfig and model mappings for all task types (CausalLM, SequenceClassification, QuestionAnswering, TokenClassification)
- Add auto-loading support through AutoModel, AutoConfig, and AutoTokenizer
- Use LlamaTokenizer for tokenization
- Add FX graph support for Arcee models
- Create lazy loading module structure for Arcee

* feat: update YARN scaling and RoPE validation for Arcee model

* feat: add auto_docstring checkpoint config to Arcee model classes

* docs: add pre-trained model weights reference to Arcee configuration files

* refactor: move RoPE utilities to dedicated modeling_rope_utils module

* Add comprehensive test suite for Arcee model

- Add test_modeling_arcee.py following standard transformers test patterns
- Include tests for all model variants (CausalLM, SequenceClassification, QuestionAnswering, TokenClassification)
- Add specific test for ReLU² activation in ArceeMLP
- Add RoPE scaling tests including YARN support
- Follow CausalLMModelTest pattern used by similar models

* Add documentation for Arcee model

- Add comprehensive model documentation with usage examples
- Include all model variants in autodoc
- Add to table of contents in proper alphabetical order
- Fixes documentation coverage for Arcee model classes

* Make style/fixup

* fix copyright year

* Sync modular conversion

* revert in legacy supported models in src/transformers/utils/fx

* cleaned redundant code in modular_arcee.py

* cleaned testing

* removed pretraining tp

* fix styles

* integration testing

---------

Co-authored-by: Pranav <veldurthipranav@gmail.com>
Co-authored-by: Pranav <56645758+pranav4501@users.noreply.github.com>",1605,0,1605,1,2,1,0.5
23c89a67321ddd85a6e291ed30c421b0bb351b9e,vasqu,2025-06-24 12:42:10+00:00,"[`Attention`] Small fix on output attentions (#38948)

small fix",1,1,2,5,41,32,0.78
4f650040a68c915c4e9fa70c4a7a62714e471d65,dggaytan,2025-06-24 12:24:56+00:00,"Removing extra space in large command for speech-pretraining example (#38705)

Removing extra space in Large command",2,2,4,1,1,1,1.0
d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd,zucchini-nlp,2025-06-24 08:53:52+00:00,"[qwen] refactor attentions for vision/audio (#38930)

* refactor attentions in vision/audio

* remove fa2 import

* make config the only args

* pass along kwargs from modality encoders

* style",409,846,1255,20,298,253,0.85
2e4c045540c3bd1eed226babd20af3941f956c58,Vaibhavs10,2025-06-24 08:39:18+00:00,"🔴 Update default `dtype` for pipelines to `auto` (#38882)

* check typing

* Fallback to fp32 if auto not supported.

* up.

* feedback from review.

* make style.",61,33,94,2,8,6,0.75
